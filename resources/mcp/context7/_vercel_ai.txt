"vercel ai","TITLE: Public Exports for Example AI Provider
DESCRIPTION: This file defines the public API for the example AI provider, re-exporting the `createExample` function, the default `example` instance, and the `ExampleProvider` and `ExampleProviderSettings` types from `example-provider.ts` to make them accessible to consumers.
SOURCE: https://github.com/vercel/ai/blob/v5/content/providers/02-openai-compatible-providers/01-custom-providers.mdx#_snippet_5

LANGUAGE: ts
CODE:
```
export { createExample, example } from './example-provider';
export type {
  ExampleProvider,
  ExampleProviderSettings,
} from './example-provider';
```

----------------------------------------

TITLE: Start Development Server
DESCRIPTION: Initiates the development server for the Express.js application, making the AI SDK example accessible locally.
SOURCE: https://github.com/vercel/ai/blob/v5/examples/express/README.md#_snippet_2

LANGUAGE: sh
CODE:
```
pnpm dev
```

----------------------------------------

TITLE: Generate Text with AI SDK and OpenAI o1 Model
DESCRIPTION: This example shows how to generate text using the OpenAI 'o1' model with the AI SDK. It highlights the unified interface of the SDK, allowing easy model switching by changing just one line of code. The code demonstrates a standard text generation call.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/00-guides/23-o1.mdx#_snippet_1

LANGUAGE: ts
CODE:
```
import { generateText } from 'ai';
import { openai } from '@ai-sdk/openai';

const { text } = await generateText({
  model: openai('o1'),
  prompt: 'Explain the concept of quantum entanglement.',
});
```

----------------------------------------

TITLE: Configure OpenAI API Key
DESCRIPTION: Sets up the OpenAI API key in the .env file, which is essential for authenticating requests to the OpenAI API. This file can also contain other provider-specific settings.
SOURCE: https://github.com/vercel/ai/blob/v5/examples/hono/README.md#_snippet_0

LANGUAGE: sh
CODE:
```
OPENAI_API_KEY=""YOUR_OPENAI_API_KEY""
```

----------------------------------------

TITLE: Bootstrap Next.js AI Chat App with create-next-app
DESCRIPTION: These commands initialize a new Next.js project using the provided example from the Vercel AI SDK repository. They set up a ChatGPT-like AI-powered streaming chat bot with OpenAI integration, allowing users to quickly get started with the example application. Choose the command corresponding to your preferred package manager (npm, Yarn, or pnpm).
SOURCE: https://github.com/vercel/ai/blob/main/examples/next-openai-pages/README.md#_snippet_0

LANGUAGE: Bash
CODE:
```
npx create-next-app --example https://github.com/vercel/ai/tree/main/examples/next-openai-pages next-openai-app
```

LANGUAGE: Bash
CODE:
```
yarn create next-app --example https://github.com/vercel/ai/tree/main/examples/next-openai-pages next-openai-app
```

LANGUAGE: Bash
CODE:
```
pnpm create next-app --example https://github.com/vercel/ai/tree/main/examples/next-openai-pages next-openai-app
```

----------------------------------------

TITLE: Build Interactive Chat UI with AI SDK React `useChat` Hook in Next.js
DESCRIPTION: This TSX example demonstrates building a frontend chat interface in Next.js using the `@ai-sdk/react` `useChat` hook. It manages the chat state, user input, and submission, displaying messages from both the user and the AI. This hook simplifies connecting the UI to a backend AI API for real-time chat experiences.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/00-guides/23-o1.mdx#_snippet_6

LANGUAGE: tsx
CODE:
```
'use client';

import { useChat } from '@ai-sdk/react';

export default function Page() {
  const { messages, input, handleInputChange, handleSubmit, error } = useChat();

  return (
    <>
      {messages.map(message => (
        <div key={message.id}>
          {message.role === 'user' ? 'User: ' : 'AI: '}
          {message.content}
        </div>
      ))}
      <form onSubmit={handleSubmit}>
        <input name=""prompt"" value={input} onChange={handleInputChange} />
        <button type=""submit"">Submit</button>
      </form>
    </>
  );
}
```

----------------------------------------

TITLE: Call DeepSeek R1 with AI SDK
DESCRIPTION: Demonstrates how to make a basic call to DeepSeek R1 using the AI SDK's `generateText` function, specifying the model and prompt.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/00-guides/25-r1.mdx#_snippet_0

LANGUAGE: TypeScript
CODE:
```
import { deepseek } from '@ai-sdk/deepseek';
import { generateText } from 'ai';

const { reasoning, text } = await generateText({
  model: deepseek('deepseek-reasoner'),
  prompt: 'Explain quantum entanglement.',
});
```

----------------------------------------

TITLE: Configure OpenAI API Key
DESCRIPTION: Sets up the environment variable for the OpenAI API key, which is essential for the AI SDK to authenticate with the OpenAI service.
SOURCE: https://github.com/vercel/ai/blob/v5/examples/express/README.md#_snippet_0

LANGUAGE: sh
CODE:
```
OPENAI_API_KEY=""YOUR_OPENAI_API_KEY""
```

----------------------------------------

TITLE: Create Next.js API Route for AI SDK Chat Streaming in TypeScript
DESCRIPTION: This TypeScript snippet provides a Next.js API route (`/api/chat`) for handling chat interactions. It receives user messages, converts them for the AI model, and streams text responses using `@ai-sdk/openai`. The route is configured to allow responses up to 5 minutes, facilitating real-time, interactive AI conversations.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/00-guides/23-o1.mdx#_snippet_5

LANGUAGE: tsx
CODE:
```
import { openai } from '@ai-sdk/openai';
import { convertToModelMessages, streamText, UIMessage } from 'ai';

// Allow responses up to 5 minutes
export const maxDuration = 300;

export async function POST(req: Request) {
  const { messages }: { messages: UIMessage[] } = await req.json();

  const result = streamText({
    model: openai('o1-mini'),
    messages: convertToModelMessages(messages)
  });

  return result.toUIMessageStreamResponse();
}
```

----------------------------------------

TITLE: Test API Endpoint with Curl
DESCRIPTION: Demonstrates how to send a POST request to the local Express.js server endpoint using Curl to verify its functionality and responsiveness.
SOURCE: https://github.com/vercel/ai/blob/v5/examples/express/README.md#_snippet_3

LANGUAGE: sh
CODE:
```
curl -X POST http://localhost:8080
```

----------------------------------------

TITLE: Implement Example AI Provider Factory Function
DESCRIPTION: The `createExample` function initializes and returns an `ExampleProvider` instance. It handles base URL configuration, API key loading, header generation, and the creation of various AI model types (chat, completion, embedding, image) using OpenAI-compatible classes from the AI SDK. A default instance is also exported.
SOURCE: https://github.com/vercel/ai/blob/v5/content/providers/02-openai-compatible-providers/01-custom-providers.mdx#_snippet_4

LANGUAGE: ts
CODE:
```
export function createExample(
  options: ExampleProviderSettings = {},
): ExampleProvider {
  const baseURL = withoutTrailingSlash(
    options.baseURL ?? 'https://api.example.com/v1',
  );
  const getHeaders = () => ({
    Authorization: `Bearer ${loadApiKey({
      apiKey: options.apiKey,
      environmentVariableName: 'EXAMPLE_API_KEY',
      description: 'Example API key',
    })}`,
    ...options.headers,
  });

  interface CommonModelConfig {
    provider: string;
    url: ({ path }: { path: string }) => string;
    headers: () => Record<string, string>;
    fetch?: FetchFunction;
  }

  const getCommonModelConfig = (modelType: string): CommonModelConfig => ({
    provider: `example.${modelType}`,
    url: ({ path }) => {
      const url = new URL(`${baseURL}${path}`);
      if (options.queryParams) {
        url.search = new URLSearchParams(options.queryParams).toString();
      }
      return url.toString();
    },
    headers: getHeaders,
    fetch: options.fetch,
  });

  const createChatModel = (
    modelId: ExampleChatModelId,
    settings: ExampleChatSettings = {},
  ) => {
    return new OpenAICompatibleChatLanguageModel(
      modelId,
      settings,
      getCommonModelConfig('chat'),
    );
  };

  const createCompletionModel = (
    modelId: ExampleCompletionModelId,
    settings: ExampleCompletionSettings = {},
  ) =>
    new OpenAICompatibleCompletionLanguageModel(
      modelId,
      settings,
      getCommonModelConfig('completion'),
    );

  const createTextEmbeddingModel = (
    modelId: ExampleEmbeddingModelId,
    settings: ExampleEmbeddingSettings = {},
  ) =>
    new OpenAICompatibleEmbeddingModel(
      modelId,
      settings,
      getCommonModelConfig('embedding'),
    );

  const createImageModel = (
    modelId: ExampleImageModelId,
    settings: ExampleImageSettings = {},
  ) =>
    new OpenAICompatibleImageModel(
      modelId,
      settings,
      getCommonModelConfig('image'),
    );

  const provider = (
    modelId: ExampleChatModelId,
    settings?: ExampleChatSettings,
  ) => createChatModel(modelId, settings);

  provider.completionModel = createCompletionModel;
  provider.chatModel = createChatModel;
  provider.textEmbeddingModel = createTextEmbeddingModel;
  provider.imageModel = createImageModel;

  return provider;
}

// Export default instance
export const example = createExample();
```

----------------------------------------

TITLE: Run AI SDK Core Example
DESCRIPTION: Navigate to the `examples/ai-core` directory and execute a specific AI SDK Core example using `tsx`.
SOURCE: https://github.com/vercel/ai/blob/v5/CONTRIBUTING.md#_snippet_4

LANGUAGE: Shell
CODE:
```
pnpm tsx src/stream-text/openai.ts
```

----------------------------------------

TITLE: Run Other Framework Examples
DESCRIPTION: Navigate to an example folder for other frameworks (not AI SDK Core) and run the development server to view the example in a browser.
SOURCE: https://github.com/vercel/ai/blob/v5/CONTRIBUTING.md#_snippet_5

LANGUAGE: Shell
CODE:
```
pnpm dev
```

----------------------------------------

TITLE: Example Kasada API Endpoint URL
DESCRIPTION: This snippet provides an example of the Kasada API URL structure. This specific URL format, including placeholders for hostname and version, needs to be configured within the `kasada-server.ts` and `kasada-client.ts` files, based on the details provided in the user's Kasada dashboard.
SOURCE: https://github.com/vercel/ai/blob/v5/examples/next-openai-kasada-bot-protection/README.md#_snippet_1

LANGUAGE: text
CODE:
```
https://${kasadaAPIHostname}/149e9513-01fa-4fb0-aad4-566afd725d1b/2d206a39-8ed7-437e-a3be-862e0f06eea3/api/${kasadaAPIVersion}/classification
```

----------------------------------------

TITLE: Local Development Setup for AI SDK Next.js FastAPI Example
DESCRIPTION: A sequence of commands to prepare and run the AI SDK, Next.js, and FastAPI example locally. This involves setting up a Python virtual environment, installing both Python and JavaScript dependencies, and finally launching the development server.
SOURCE: https://github.com/vercel/ai/blob/main/examples/next-fastapi/README.md#_snippet_1

LANGUAGE: Bash
CODE:
```
virtualenv venv
source venv/bin/activate
pip install -r requirements.txt
pnpm install
pnpm dev
```

----------------------------------------

TITLE: generateSpeech API Reference
DESCRIPTION: Detailed API documentation for the `generateSpeech` function, outlining all available parameters, their types, descriptions, and the structure of the returned audio and metadata.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/07-reference/01-ai-sdk-core/12-generate-speech.mdx#_snippet_2

LANGUAGE: APIDOC
CODE:
```
generateSpeech(
  model: SpeechModelV2,
  text: string,
  voice?: string,
  outputFormat?: string,
  instructions?: string,
  speed?: number,
  language?: string,
  providerOptions?: Record<string, Record<string, JSONValue>>,
  maxRetries?: number,
  abortSignal?: AbortSignal,
  headers?: Record<string, string>
) -> { audio: GeneratedAudioFile, warnings: SpeechWarning[], responses: Array<SpeechModelResponseMetadata> }

Parameters:
  - model: SpeechModelV2 - The speech model to use.
  - text: string - The text to generate the speech from.
  - voice?: string - The voice to use for the speech.
  - outputFormat?: string - The output format to use for the speech e.g. ""mp3"", ""wav"", etc.
  - instructions?: string - Instructions for the speech generation.
  - speed?: number - The speed of the speech generation.
  - language?: string - The language for speech generation. This should be an ISO 639-1 language code (e.g. ""en"", ""es"", ""fr"") or ""auto"" for automatic language detection. Provider support varies.
  - providerOptions?: Record<string, Record<string, JSONValue>> - Additional provider-specific options.
  - maxRetries?: number - Maximum number of retries. Default: 2.
  - abortSignal?: AbortSignal - An optional abort signal to cancel the call.
  - headers?: Record<string, string> - Additional HTTP headers for the request.

Returns:
  - audio: GeneratedAudioFile - The generated audio.
    Properties of GeneratedAudioFile:
      - base64: string - Audio as a base64 encoded string.
      - uint8Array: Uint8Array - Audio as a Uint8Array.
      - mimeType: string - MIME type of the audio (e.g. ""audio/mpeg"").
      - format: string - Format of the audio (e.g. ""mp3"").
  - warnings: SpeechWarning[] - Warnings from the model provider (e.g. unsupported settings).
  - responses: Array<SpeechModelResponseMetadata> - Response metadata from the provider. There may be multiple responses if we made multiple calls to the model.
    Properties of SpeechModelResponseMetadata:
      - timestamp: Date - Timestamp for the start of the generated response.
      - modelId: string - The ID of the response model that was used to generate the response.
      - body?: unknown - Optional response body.
      - headers?: Record<string, string> - Response headers.
```

----------------------------------------

TITLE: Generate Structured JSON Data with AI SDK Core
DESCRIPTION: This TypeScript example demonstrates how to use `generateObject` from AI SDK Core to produce type-safe JSON output. It leverages Zod for schema definition, ensuring the generated data, such as a recipe, strictly conforms to the specified structure.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/00-guides/24-o3.mdx#_snippet_2

LANGUAGE: ts
CODE:
```
import { generateObject } from 'ai';
import { openai } from '@ai-sdk/openai';
import { z } from 'zod';

const { object } = await generateObject({
  model: openai('o3-mini'),
  schema: z.object({
    recipe: z.object({
      name: z.string(),
      ingredients: z.array(z.object({ name: z.string(), amount: z.string() })),
      steps: z.array(z.string()),
    }),
  }),
  prompt: 'Generate a lasagna recipe.',
});
```

----------------------------------------

TITLE: Generate Text with Computer Tool (One-Shot)
DESCRIPTION: This example demonstrates how to use the defined `computerTool` with the AI SDK's `generateText` function for a single interaction. It sends a prompt to the model, enabling it to utilize the `computerTool` for actions like moving the cursor and taking a screenshot, and then logs the final generated text.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/00-guides/05-computer-use.mdx#_snippet_2

LANGUAGE: typescript
CODE:
```
const result = await generateText({
  model: anthropic('claude-3-5-sonnet-20241022'),
  prompt: 'Move the cursor to the center of the screen and take a screenshot',
  tools: { computer: computerTool },
});

console.log(result.text);
```

----------------------------------------

TITLE: Example AI Provider Imports and Configuration Interface
DESCRIPTION: This section defines the necessary imports from the AI SDK for the provider implementation and the `ExampleProviderSettings` interface. The interface specifies configuration options for the example AI provider, including API key, base URL, custom headers, query parameters, and an optional custom fetch function.
SOURCE: https://github.com/vercel/ai/blob/v5/content/providers/02-openai-compatible-providers/01-custom-providers.mdx#_snippet_2

LANGUAGE: ts
CODE:
```
import { LanguageModelV1, EmbeddingModelV2 } from '@ai-sdk/provider';
import {
  OpenAICompatibleChatLanguageModel,
  OpenAICompatibleCompletionLanguageModel,
  OpenAICompatibleEmbeddingModel,
  OpenAICompatibleImageModel,
} from '@ai-sdk/openai-compatible';
import {
  FetchFunction,
  loadApiKey,
  withoutTrailingSlash,
} from '@ai-sdk/provider-utils';
// Import your model id and settings here.

export interface ExampleProviderSettings {
  /**
Example API key.
*/
  apiKey?: string;
  /**
Base URL for the API calls.
*/
  baseURL?: string;
  /**
Custom headers to include in the requests.
*/
  headers?: Record<string, string>;
  /**
Optional custom url query parameters to include in request urls.
*/
  queryParams?: Record<string, string>;
  /**
Custom fetch implementation. You can use it as a middleware to intercept requests,
or to provide a custom fetch implementation for e.g. testing.
*/
  fetch?: FetchFunction;
}
```

----------------------------------------

TITLE: Start SvelteKit OpenAI Development Server
DESCRIPTION: Starts the development server for the `sveltekit-openai` project. The `-F` flag ensures that pnpm focuses on the specified workspace, allowing you to run the example application locally.
SOURCE: https://github.com/vercel/ai/blob/v5/examples/sveltekit-openai/README.md#_snippet_2

LANGUAGE: bash
CODE:
```
pnpm -F sveltekit-openai dev
```

----------------------------------------

TITLE: Configure OpenAI API Key
DESCRIPTION: Sets up the OpenAI API key in the .env file, which is required for accessing OpenAI services. This is a crucial first step for using AI SDK with OpenAI.
SOURCE: https://github.com/vercel/ai/blob/main/examples/mcp/README.md#_snippet_0

LANGUAGE: sh
CODE:
```
OPENAI_API_KEY=""YOUR_OPENAI_API_KEY""
```

----------------------------------------

TITLE: Install Dependencies and Build Project
DESCRIPTION: Executes commands to install necessary project dependencies using pnpm and then builds the AI SDK repository, preparing it for execution.
SOURCE: https://github.com/vercel/ai/blob/v5/examples/express/README.md#_snippet_1

LANGUAGE: sh
CODE:
```
pnpm install
pnpm build
```

----------------------------------------

TITLE: Define and Use Custom Tools with AI SDK `generateText` in TypeScript
DESCRIPTION: This TypeScript example illustrates how to integrate custom tools with the AI SDK's `generateText` function. It defines a `getWeather` tool with Zod-validated parameters, enabling the LLM to invoke external logic (simulated weather data retrieval) based on the user's prompt. This extends the LLM's capabilities to perform specific tasks.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/00-guides/23-o1.mdx#_snippet_4

LANGUAGE: typescript
CODE:
```
import { generateText, tool } from 'ai';
import { openai } from '@ai-sdk/openai';
import { z } from 'zod';

const { text } = await generateText({
  model: openai('o1'),
  prompt: 'What is the weather like today?',
  tools: {
    getWeather: tool({
      description: 'Get the weather in a location',
      parameters: z.object({
        location: z.string().describe('The location to get the weather for')
      }),
      execute: async ({ location }) => ({
        location,
        temperature: 72 + Math.floor(Math.random() * 21) - 10
      })
    })
  }
});
```

----------------------------------------

TITLE: Configure OpenAI API Key in .env
DESCRIPTION: Sets the OpenAI API key as an environment variable, essential for authenticating requests to the OpenAI service. Replace 'YOUR_OPENAI_API_KEY' with your actual key.
SOURCE: https://github.com/vercel/ai/blob/v5/examples/node-http-server/README.md#_snippet_0

LANGUAGE: sh
CODE:
```
OPENAI_API_KEY=""YOUR_OPENAI_API_KEY""
```

----------------------------------------

TITLE: Bootstrap Next.js Project with AI SDK Example
DESCRIPTION: Commands to initialize a new Next.js application pre-configured with the AI SDK and FastAPI example. Choose your preferred package manager (npx, yarn, or pnpm) to set up the project structure and initial files.
SOURCE: https://github.com/vercel/ai/blob/main/examples/next-fastapi/README.md#_snippet_0

LANGUAGE: Bash
CODE:
```
npx create-next-app --example https://github.com/vercel/ai/tree/main/examples/next-fastapi next-fastapi-app
```

LANGUAGE: Bash
CODE:
```
yarn create next-app --example https://github.com/vercel/ai/tree/main/examples/next-fastapi next-fastapi-app
```

LANGUAGE: Bash
CODE:
```
pnpm create next-app --example https://github.com/vercel/ai/tree/main/examples/next-fastapi next-fastapi-app
```

----------------------------------------

TITLE: API Signature for simulateReadableStream
DESCRIPTION: Detailed API documentation for the `simulateReadableStream` function, including its parameters, return type, and type parameters.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/07-reference/01-ai-sdk-core/75-simulate-readable-stream.mdx#_snippet_2

LANGUAGE: APIDOC
CODE:
```
simulateReadableStream(options: object): ReadableStream<T>
  Parameters:
    chunks: T[] (required)
      description: Array of values to be emitted by the stream
    initialDelayInMs: number | null (optional, default: 0)
      description: Initial delay in milliseconds before emitting the first value. Defaults to 0. Set to null to skip the initial delay entirely.
    chunkDelayInMs: number | null (optional, default: 0)
      description: Delay in milliseconds between emitting each value. Defaults to 0. Set to null to skip delays between chunks.
  Returns: ReadableStream<T>
    description: A ReadableStream that:
      - Emits each value from the provided 'chunks' array sequentially
      - Waits for 'initialDelayInMs' before emitting the first value (if not null)
      - Waits for 'chunkDelayInMs' between emitting subsequent values (if not null)
      - Closes automatically after all chunks have been emitted
  Type Parameters:
    T: The type of values contained in the chunks array and emitted by the stream
```

----------------------------------------

TITLE: Generate Text with AI SDK and OpenAI o1-mini
DESCRIPTION: This snippet demonstrates how to use the AI SDK to generate text using the OpenAI 'o1-mini' model. It imports `generateText` from 'ai' and `openai` from '@ai-sdk/openai', then calls `generateText` with the model and a prompt. This is the basic setup for interacting with LLMs via the AI SDK.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/00-guides/23-o1.mdx#_snippet_0

LANGUAGE: ts
CODE:
```
import { generateText } from 'ai';
import { openai } from '@ai-sdk/openai';

const { text } = await generateText({
  model: openai('o1-mini'),
  prompt: 'Explain the concept of quantum entanglement.',
});
```

----------------------------------------

TITLE: createStreamableUI API Reference
DESCRIPTION: Detailed API documentation for the `createStreamableUI` function, outlining its parameters, return value, and available methods for managing the UI stream. This function facilitates dynamic UI updates from server to client.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/07-reference/03-ai-sdk-rsc/03-create-streamable-ui.mdx#_snippet_1

LANGUAGE: APIDOC
CODE:
```
createStreamableUI function:
  Parameters:
    initialValue: ReactNode (optional)
      description: The initial value of the streamable UI.
  Returns:
    value: ReactNode
      description: The value of the streamable UI. This can be returned from a Server Action and received by the client.
  Methods:
    update(ReactNode): void
      description: Updates the current UI node. It takes a new UI node and replaces the old one.
    append(ReactNode): void
      description: Appends a new UI node to the end of the old one. Once appended a new UI node, the previous UI node cannot be updated anymore.
    done(ReactNode | null): void
      description: Marks the UI node as finalized and closes the stream. Once called, the UI node cannot be updated or appended anymore. This method is always required to be called, otherwise the response will be stuck in a loading state.
    error(Error): void
      description: Signals that there is an error in the UI stream. It will be thrown on the client side and caught by the nearest error boundary component.
```

----------------------------------------

TITLE: Start Development Server
DESCRIPTION: Initiates the Hono server in development mode. This command makes the application accessible locally, typically for testing and development purposes.
SOURCE: https://github.com/vercel/ai/blob/v5/examples/hono/README.md#_snippet_2

LANGUAGE: sh
CODE:
```
pnpm dev
```

----------------------------------------

TITLE: Define a Custom Metadata Extractor for API Responses
DESCRIPTION: This comprehensive example defines a 'MetadataExtractor' object, showcasing how to process both complete (non-streaming) and streaming API responses. It demonstrates extracting standard usage data, experimental features, and custom metrics, providing full flexibility to capture provider-specific information.
SOURCE: https://github.com/vercel/ai/blob/v5/content/providers/02-openai-compatible-providers/index.mdx#_snippet_8

LANGUAGE: typescript
CODE:
```
const myMetadataExtractor: MetadataExtractor = {
  // Process complete, non-streaming responses
  extractMetadata: ({ parsedBody }) => {
    // You have access to the complete raw response
    // Extract any fields the provider includes
    return {
      myProvider: {
        standardUsage: parsedBody.usage,
        experimentalFeatures: parsedBody.beta_features,
        customMetrics: {
          processingTime: parsedBody.server_timing?.total_ms,
          modelVersion: parsedBody.model_version,
          // ... any other provider-specific data
        },
      },
    };
  },

  // Process streaming responses
  createStreamExtractor: () => {
    let accumulatedData = {
      timing: [],
      customFields: {},
    };

    return {
      // Process each chunk's raw data
      processChunk: parsedChunk => {
        if (parsedChunk.server_timing) {
          accumulatedData.timing.push(parsedChunk.server_timing);
        }
        if (parsedChunk.custom_data) {
          Object.assign(accumulatedData.customFields, parsedChunk.custom_data);
        }
      },
      // Build final metadata from accumulated data
      buildMetadata: () => ({
        myProvider: {
          streamTiming: accumulatedData.timing,
          customData: accumulatedData.customFields,
        },
      }),
    };
  },
};
```

----------------------------------------

TITLE: Implement Chat UI with useChat Hook (Next.js)
DESCRIPTION: Implements a client-side React component for a chat interface using the AI SDK's `useChat` hook. It handles message display, user input, form submission, and integrates with the backend chat API.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/00-guides/25-r1.mdx#_snippet_5

LANGUAGE: tsx
CODE:
```
'use client';

import { useChat } from '@ai-sdk/react';

export default function Page() {
  const { messages, input, handleInputChange, handleSubmit, error } = useChat();

  return (
    <>
      {messages.map(message => (
        <div key={message.id}>
          {message.role === 'user' ? 'User: ' : 'AI: '}
          {message.reasoning && <pre>{message.reasoning}</pre>}
          {message.content}
        </div>
      ))}
      <form onSubmit={handleSubmit}>
        <input name=""prompt"" value={input} onChange={handleInputChange} />
        <button type=""submit"">Submit</button>
      </form>
    </>
  );
}
```

----------------------------------------

TITLE: Start Development Server with pnpm
DESCRIPTION: Initiates the local development server, typically configured to watch for file changes and provide hot-reloading. This command makes the application accessible for testing.
SOURCE: https://github.com/vercel/ai/blob/v5/examples/node-http-server/README.md#_snippet_2

LANGUAGE: sh
CODE:
```
pnpm dev
```

----------------------------------------

TITLE: Vercel AI SDK Tool Definition and Model Generation Parameters API
DESCRIPTION: Detailed API documentation for configuring tools and model generation parameters in the Vercel AI SDK. This includes defining input schemas for tools, specifying asynchronous execution functions, and setting various sampling and penalty parameters to control the AI model's output characteristics.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/02-stream-text.mdx#_snippet_4

LANGUAGE: APIDOC
CODE:
```
Property: schema
  Type: ZodSchema | JSONSchema
  Optional: false
  Description: The schema of the input that the tool expects. The language model will use this to generate the input. It is also used to validate the output of the language model. Use descriptions to make the input understandable for the language model. You can either pass in a Zod schema or a JSON schema (using the `jsonSchema` function).

Property: execute
  Type: async (parameters: T, options: ToolExecutionOptions) => RESULT
  Optional: true
  Description: An async function that is called with the arguments from the tool call and produces a result. If not provided, the tool will not be executed automatically.
  Properties:
    Type: ToolExecutionOptions
    Parameters:
      Parameter: toolCallId
        Type: string
        Description: The ID of the tool call. You can use it e.g. when sending tool-call related information with stream data.
      Parameter: messages
        Type: ModelMessage[]
        Description: Messages that were sent to the language model to initiate the response that contained the tool call. The messages do not include the system prompt nor the assistant response that contained the tool call.
      Parameter: abortSignal
        Type: AbortSignal
        Description: An optional abort signal that indicates that the overall operation should be aborted.

Property: toolChoice
  Type: ""auto"" | ""none"" | ""required"" | { ""type"": ""tool"", ""toolName"": string }
  Optional: true
  Description: The tool choice setting. It specifies how tools are selected for execution. The default is ""auto"". ""none"" disables tool execution. ""required"" requires tools to be executed. { ""type"": ""tool"", ""toolName"": string } specifies a specific tool to execute.

Property: maxOutputTokens
  Type: number
  Optional: true
  Description: Maximum number of tokens to generate.

Property: temperature
  Type: number
  Optional: true
  Description: Temperature setting. The value is passed through to the provider. The range depends on the provider and model. It is recommended to set either `temperature` or `topP`, but not both.

Property: topP
  Type: number
  Optional: true
  Description: Nucleus sampling. The value is passed through to the provider. The range depends on the provider and model. It is recommended to set either `temperature` or `topP`, but not both.

Property: topK
  Type: number
  Optional: true
  Description: Only sample from the top K options for each subsequent token. Used to remove ""long tail"" low probability responses. Recommended for advanced use cases only. You usually only need to use temperature.

Property: presencePenalty
  Type: number
  Optional: true
  Description: Presence penalty setting. It affects the likelihood of the model to repeat information that is already in the prompt. The value is passed through to the provider. The range depends on the provider and model.

Property: frequencyPenalty
  Type: number
  Optional: true
  Description: Frequency penalty setting. It affects the likelihood of the model to repeatedly use the same words or phrases. The value is passed through to the provider. The range depends on the provider and model.

Property: stopSequences
  Type: string[]
  Optional: true
  Description: Sequences that will stop the generation of the text. If the model generates any of these sequences, it will stop generating further text.

Property: seed
  Type: number
  Optional: true
  Description: The seed (integer) to use for random sampling. If set and supported by the model, calls will generate deterministic results.

Property: maxRetries
  Type: number
  Optional: true
  Description: Maximum number of retries. Set to 0 to disable retries. Default: 2.

Property: abortSignal
  Type: AbortSignal
  Optional: true
  Description: An optional abort signal that can be used to cancel the call.

Property: headers
  Type: Record<string, string>
  Optional: true
  Description: Custom headers to be sent with the request.
```

----------------------------------------

TITLE: Generate Text with AI SDK Core and OpenAI (Node.js)
DESCRIPTION: This TypeScript snippet demonstrates how to use the `generateText` function from the AI SDK Core with the OpenAI provider. It shows how to specify a model, provide a system prompt for context, and a user prompt for the query, then logs the generated text to the console.
SOURCE: https://github.com/vercel/ai/blob/v5/packages/ai/README.md#_snippet_2

LANGUAGE: typescript
CODE:
```
import { generateText } from 'ai';
import { openai } from '@ai-sdk/openai'; // Ensure OPENAI_API_KEY environment variable is set

const { text } = await generateText({
  model: openai('gpt-4o'),
  system: 'You are a friendly assistant!',
  prompt: 'Why is the sky blue?',
});

console.log(text);
```

----------------------------------------

TITLE: Next.js API Route for AI Chat Endpoint
DESCRIPTION: This TypeScript code defines a Next.js API route handler (app/api/chat/route.ts) for processing chat requests. It uses @ai-sdk/openai to stream text responses from GPT-4.5-preview and converts UI messages for the model, supporting responses up to 30 seconds.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/00-guides/22-gpt-4-5.mdx#_snippet_4

LANGUAGE: typescript
CODE:
```
import { openai } from '@ai-sdk/openai';
import { convertToModelMessages, streamText, UIMessage } from 'ai';

// Allow responses up to 30 seconds
export const maxDuration = 30;

export async function POST(req: Request) {
  const { messages }: { messages: UIMessage[] } = await req.json();

  const result = streamText({
    model: openai('gpt-4.5-preview'),
    messages: convertToModelMessages(messages),
  });

  return result.toUIMessageStreamResponse();
}
```

----------------------------------------

TITLE: AI SDK Text Stream Protocol Implementation Example
DESCRIPTION: This example demonstrates a complete implementation of the AI SDK text stream protocol, showing both the frontend usage with `useCompletion` in a Next.js React component and the corresponding backend API route using `streamText` with an OpenAI model.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/04-ai-sdk-ui/50-stream-protocol.mdx#_snippet_0

LANGUAGE: tsx
CODE:
```
'use client';

import { useCompletion } from '@ai-sdk/react';

export default function Page() {
  const { completion, input, handleInputChange, handleSubmit } = useCompletion({
    streamProtocol: 'text',
  });

  return (
    <form onSubmit={handleSubmit}>
      <input name=""prompt"" value={input} onChange={handleInputChange} />
      <button type=""submit"">Submit</button>
      <div>{completion}</div>
    </form>
  );
}
```

LANGUAGE: ts
CODE:
```
import { streamText } from 'ai';
import { openai } from '@ai-sdk/openai';

// Allow streaming responses up to 30 seconds
export const maxDuration = 30;

export async function POST(req: Request) {
  const { prompt }: { prompt: string } = await req.json();

  const result = streamText({
    model: openai('gpt-4o'),
    prompt,
  });

  return result.toTextStreamResponse();
}
```

----------------------------------------

TITLE: Use DeepSeek R1 via Groq with AI SDK
DESCRIPTION: Illustrates how to use DeepSeek R1 through the Groq provider with the AI SDK. It also demonstrates the use of `extractReasoningMiddleware` for reasoning token extraction.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/00-guides/25-r1.mdx#_snippet_2

LANGUAGE: TypeScript
CODE:
```
import { groq } from '@ai-sdk/groq';
import {
  generateText,
  wrapLanguageModel,
  extractReasoningMiddleware,
} from 'ai';

// middleware to extract reasoning tokens
const enhancedModel = wrapLanguageModel({
  model: groq('deepseek-r1-distill-llama-70b'),
  middleware: extractReasoningMiddleware({ tagName: 'think' }),
});

const { reasoning, text } = await generateText({
  model: enhancedModel,
  prompt: 'Explain quantum entanglement.',
});
```

----------------------------------------

TITLE: Create Next.js API Route for AI Chat Endpoint
DESCRIPTION: This TSX code defines a Next.js API route (`app/api/chat/route.ts`) that handles incoming chat messages. It uses `@ai-sdk/openai` to stream text responses and `convertToModelMessages` to adapt UI messages for the model, enabling server-side AI processing.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/00-guides/24-o3.mdx#_snippet_5

LANGUAGE: tsx
CODE:
```
import { openai } from '@ai-sdk/openai';
import { convertToModelMessages, streamText, UIMessage } from 'ai';

// Allow responses up to 5 minutes
export const maxDuration = 300;

export async function POST(req: Request) {
  const { messages }: { messages: UIMessage[] } = await req.json();

  const result = streamText({
    model: openai('o3-mini'),
    messages: convertToModelMessages(messages),
  });

  return result.toUIMessageStreamResponse();
}
```

----------------------------------------

TITLE: Configure OpenAI API Key
DESCRIPTION: Sets the OpenAI API key as an environment variable. This key is crucial for the AI SDK to authenticate and interact with the OpenAI service. Replace 'YOUR_OPENAI_API_KEY' with your actual API key.
SOURCE: https://github.com/vercel/ai/blob/v5/examples/fastify/README.md#_snippet_0

LANGUAGE: sh
CODE:
```
OPENAI_API_KEY=""YOUR_OPENAI_API_KEY""
```

----------------------------------------

TITLE: experimental_useObject Hook API Parameters
DESCRIPTION: Detailed documentation of the parameters accepted by the `experimental_useObject` hook. It includes types, descriptions, and optionality for each parameter, such as API endpoint, schema definition, and various callback functions.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/07-reference/02-ai-sdk-ui/03-use-object.mdx#_snippet_2

LANGUAGE: APIDOC
CODE:
```
experimental_useObject Parameters:
  api: string
    The API endpoint that is called to generate objects. It should stream JSON that matches the schema as chunked text. It can be a relative path (starting with `/`) or an absolute URL.
  schema: Zod Schema | JSON Schema
    A schema that defines the shape of the complete object. You can either pass in a Zod schema or a JSON schema (using the `jsonSchema` function).
  id?: string
    A unique identifier. If not provided, a random one will be generated. When provided, the `useObject` hook with the same `id` will have shared states across components.
  initialValue?: DeepPartial<RESULT>
    An value for the initial object. Optional.
  fetch?: FetchFunction
    A custom fetch function to be used for the API call. Defaults to the global fetch function. Optional.
  headers?: Record<string, string> | Headers
    A headers object to be passed to the API endpoint. Optional.
  credentials?: RequestCredentials
    The credentials mode to be used for the fetch request. Possible values are: ""omit"", ""same-origin"", ""include"". Optional.
  onError?: (error: Error) => void
    Callback function to be called when an error is encountered. Optional.
  onFinish?: (result: OnFinishResult) => void
    Called when the streaming response has finished.
    OnFinishResult:
      object: T | undefined
        The generated object (typed according to the schema). Can be undefined if the final object does not match the schema.
      error: unknown | undefined
        Optional error object. This is e.g. a TypeValidationError when the final object does not match the schema.
```

----------------------------------------

TITLE: Create OpenAI Compatible Provider Instance
DESCRIPTION: Demonstrates how to import `createOpenAICompatible` and initialize a provider instance with a name, API key, and base URL.
SOURCE: https://github.com/vercel/ai/blob/v5/content/providers/02-openai-compatible-providers/index.mdx#_snippet_1

LANGUAGE: typescript
CODE:
```
import { createOpenAICompatible } from '@ai-sdk/openai-compatible';

const provider = createOpenAICompatible({
  name: 'provider-name',
  apiKey: process.env.PROVIDER_API_KEY,
  baseURL: 'https://api.provider.com/v1',
});
```

----------------------------------------

TITLE: Generate Text with Claude 3.7 Sonnet using AI SDK (Amazon Bedrock)
DESCRIPTION: This example shows how to switch providers and use Claude 3.7 Sonnet through Amazon Bedrock with the AI SDK. It imports the Bedrock provider and uses the `generateText` function with the appropriate model identifier to get a text response.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/00-guides/20-sonnet-3-7.mdx#_snippet_1

LANGUAGE: ts
CODE:
```
import { bedrock } from '@ai-sdk/amazon-bedrock';
import { generateText } from 'ai';

const { reasoning, text } = await generateText({
  model: bedrock('anthropic.claude-3-7-sonnet-20250219-v1:0'),
  prompt: 'How many people will live in the world in 2040?',
});
```

----------------------------------------

TITLE: createIdGenerator API Reference
DESCRIPTION: Detailed API documentation for the `createIdGenerator` function, outlining its parameters, their types and descriptions, the function's return type, and important usage notes.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/07-reference/01-ai-sdk-core/91-create-id-generator.mdx#_snippet_1

LANGUAGE: APIDOC
CODE:
```
createIdGenerator(options?: object)

Parameters:
  options: object - Optional configuration object with the following properties:
    options.alphabet: string - The characters to use for generating the random part of the ID. Defaults to alphanumeric characters (0-9, A-Z, a-z).
    options.prefix: string - A string to prepend to all generated IDs. Defaults to none.
    options.separator: string - The character(s) to use between the prefix and the random part. Defaults to ""-"".
    options.size: number - The default length of the random part of the ID. Defaults to 16.

Returns:
  Function - A function that generates IDs based on the configured options.

Notes:
- The generator uses non-secure random generation and should not be used for security-critical purposes.
- The separator character must not be part of the alphabet to ensure reliable prefix checking.
```

----------------------------------------

TITLE: ReplicateStream Function API Documentation
DESCRIPTION: Detailed API documentation for the `ReplicateStream` function, outlining its expected parameters, their types, and descriptions. It also specifies the structure of the `callbacks` object and the function's return type.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/07-reference/04-stream-helpers/18-replicate-stream.mdx#_snippet_1

LANGUAGE: APIDOC
CODE:
```
ReplicateStream(
  pre: Prediction,
  callbacks?: AIStreamCallbacksAndOptions,
  options?: { headers?: Record<string, string> }
) => Promise<ReadableStream>

Parameters:
  pre: Prediction
    Object returned by the Replicate JavaScript SDK.
  callbacks?: AIStreamCallbacksAndOptions
    An object containing callback functions to handle the start, each token, and completion of the AI response. In the absence of this parameter, default behavior is implemented.
    Properties:
      onStart?: () => Promise<void>
        An optional function that is called at the start of the stream processing.
      onCompletion?: (completion: string) => Promise<void>
        An optional function that is called for every completion. It's passed the completion as a string.
      onFinal?: (completion: string) => Promise<void>
        An optional function that is called once when the stream is closed with the final completion message.
      onToken?: (token: string) => Promise<void>
        An optional function that is called for each token in the stream. It's passed the token as a string.
  options?: { headers?: Record<string, string> }
    An optional parameter for passing additional headers.

Returns:
  A ReadableStream wrapped in a promise.
```

----------------------------------------

TITLE: Generate Text with OpenAI Responses API using AI SDK
DESCRIPTION: This snippet demonstrates how to perform basic text generation using the AI SDK's `generateText` function with OpenAI's Responses API (gpt-4o model). It shows importing necessary modules and making a simple prompt call to get a text response.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/00-guides/19-openai-responses.mdx#_snippet_0

LANGUAGE: ts
CODE:
```
import { generateText } from 'ai';
import { openai } from '@ai-sdk/openai';

const { text } = await generateText({
  model: openai.responses('gpt-4o'),
  prompt: 'Explain the concept of quantum entanglement.',
});
```

----------------------------------------

TITLE: Create AI SDK Chat Route Handler (Next.js)
DESCRIPTION: Defines a Next.js API route handler for processing chat messages. It uses the AI SDK to stream text responses from the DeepSeek model, converting UI messages to model messages and enabling reasoning token forwarding.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/00-guides/25-r1.mdx#_snippet_4

LANGUAGE: tsx
CODE:
```
import { deepseek } from '@ai-sdk/deepseek';
import { convertToModelMessages, streamText, UIMessage } from 'ai';

export async function POST(req: Request) {
  const { messages }: { messages: UIMessage[] } = await req.json();

  const result = streamText({
    model: deepseek('deepseek-reasoner'),
    messages: convertToModelMessages(messages),
  });

  return result.toUIMessageStreamResponse({
    sendReasoning: true,
  });
}
```

----------------------------------------

TITLE: Configure OpenAI API Key
DESCRIPTION: Creates a `.env.local` file in the `examples/sveltekit-openai` directory to store the OpenAI API key. This key is essential for the application to authenticate with the OpenAI API.
SOURCE: https://github.com/vercel/ai/blob/v5/examples/sveltekit-openai/README.md#_snippet_1

LANGUAGE: bash
CODE:
```
OPENAI_API_KEY=<your key>
```

----------------------------------------

TITLE: Generate Structured JSON Data with AI SDK and OpenAI o1
DESCRIPTION: This example illustrates how to generate structured JSON data using the AI SDK's `generateObject` function. It leverages Zod for schema definition, ensuring type-safe output that conforms to a specified structure. This functionality is useful for extracting information, classifying data, or generating synthetic data, and is currently supported only by the 'o1' model.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/00-guides/23-o1.mdx#_snippet_3

LANGUAGE: ts
CODE:
```
import { generateObject } from 'ai';
import { openai } from '@ai-sdk/openai';
import { z } from 'zod';

const { object } = await generateObject({
  model: openai('o1'),
  schema: z.object({
    recipe: z.object({
      name: z.string(),
      ingredients: z.array(z.object({ name: z.string(), amount: z.string() })),
      steps: z.array(z.string()),
    }),
  }),
  prompt: 'Generate a lasagna recipe.',
});
```

----------------------------------------

TITLE: Integrate basic web search with AI SDK
DESCRIPTION: Demonstrates how to use the built-in `webSearchPreview` tool with `generateText` to ground AI responses by accessing the internet. It shows a basic prompt and how to log the generated text and source information.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/00-guides/19-openai-responses.mdx#_snippet_3

LANGUAGE: ts
CODE:
```
import { openai } from '@ai-sdk/openai';
import { generateText } from 'ai';

const result = await generateText({
  model: openai.responses('gpt-4o-mini'),
  prompt: 'What happened in San Francisco last week?',
  tools: {
    web_search_preview: openai.tools.webSearchPreview(),
  },
});

console.log(result.text);
console.log(result.sources);
```

----------------------------------------

TITLE: Create AI Chat API Endpoint with Next.js and AI SDK
DESCRIPTION: This server-side API route handles incoming chat messages, streams text responses using DeepInfra's Llama 3.1 model, and converts them to a UI message stream. It's designed for Next.js App Router and uses `@ai-sdk/deepinfra` and `ai` packages.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/00-guides/21-llama-3_1.mdx#_snippet_6

LANGUAGE: tsx
CODE:
```
import { deepinfra } from '@ai-sdk/deepinfra';
import { convertToModelMessages, streamText } from 'ai';

// Allow streaming responses up to 30 seconds
export const maxDuration = 30;

export async function POST(req: Request) {
  const { messages } = await req.json();

  const result = streamText({
    model: deepinfra('meta-llama/Meta-Llama-3.1-70B-Instruct'),
    messages: convertToModelMessages(messages),
  });

  return result.toUIMessageStreamResponse();
}
```

----------------------------------------

TITLE: createProviderRegistry API Parameters
DESCRIPTION: Detailed documentation for the parameters accepted by the `createProviderRegistry` function, including the `providers` record (mapping string IDs to `Provider` instances) and an optional `options` object for configuration.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/07-reference/01-ai-sdk-core/40-provider-registry.mdx#_snippet_6

LANGUAGE: APIDOC
CODE:
```
createProviderRegistry(
  providers: Record<string, Provider>,
  options?: object
)

Parameters:
  providers: Record<string, Provider>
    description: The unique identifier for the provider. It should be unique within the registry.
    properties:
      Provider:
        languageModel: (id: string) => LanguageModel
          description: A function that returns a language model by its id.
        textEmbeddingModel: (id: string) => EmbeddingModel<string>
          description: A function that returns a text embedding model by its id.
        imageModel: (id: string) => ImageModel
          description: A function that returns an image model by its id.

  options: object
    description: Optional configuration for the registry.
    properties:
      Options:
        separator: string
          description: Custom separator between provider and model IDs. Defaults to "":"".
```

----------------------------------------

TITLE: Run Streamable HTTP Transport Client
DESCRIPTION: Executes the client for the streamable HTTP transport. This client interacts with the previously started HTTP server to demonstrate the example.
SOURCE: https://github.com/vercel/ai/blob/main/examples/mcp/README.md#_snippet_3

LANGUAGE: sh
CODE:
```
pnpm http:client
```

----------------------------------------

TITLE: Create Next.js API Route for Chat Endpoint
DESCRIPTION: This TypeScript code defines a Next.js API route (`/api/chat`) that handles incoming chat messages. It uses the AI SDK's `streamText` function with the Anthropic model to generate responses and streams them back to the client, optionally including reasoning tokens for advanced UI display.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/00-guides/18-claude-4.mdx#_snippet_3

LANGUAGE: TypeScript
CODE:
```
import { anthropic, AnthropicProviderOptions } from '@ai-sdk/anthropic';
import { streamText } from 'ai';

export async function POST(req: Request) {
  const { messages } = await req.json();

  const result = streamText({
    model: anthropic('claude-4-sonnet-20250514'),
    messages,
    headers: {
      'anthropic-beta': 'interleaved-thinking-2025-05-14',
    },
    providerOptions: {
      anthropic: {
        thinking: { type: 'enabled', budgetTokens: 15000 },
      } satisfies AnthropicProviderOptions,
    },
  });

  return result.toDataStreamResponse({
    sendReasoning: true,
  });
}
```

----------------------------------------

TITLE: API Reference: `experimental_prepareStep` Callback Parameters
DESCRIPTION: Lists the parameters provided to the `experimental_prepareStep` callback function, allowing access to context like the model, step number, and previous steps for dynamic configuration.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/15-tools-and-tool-calling.mdx#_snippet_9

LANGUAGE: APIDOC
CODE:
```
experimental_prepareStep: (params: object) => Promise<object | void>
  params:
    - model: The model that was passed into generateText.
    - maxSteps: The maximum number of steps that was passed into generateText.
    - stepNumber: The number of the step that is being executed.
    - steps: The steps that have been executed so far.
```

----------------------------------------

TITLE: Use DeepSeek R1 via Fireworks with AI SDK
DESCRIPTION: Shows how to integrate DeepSeek R1 through the Fireworks provider using the AI SDK. It includes wrapping the language model with `extractReasoningMiddleware` to capture reasoning tokens.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/00-guides/25-r1.mdx#_snippet_1

LANGUAGE: TypeScript
CODE:
```
import { fireworks } from '@ai-sdk/fireworks';
import {
  generateText,
  wrapLanguageModel,
  extractReasoningMiddleware,
} from 'ai';

// middleware to extract reasoning tokens
const enhancedModel = wrapLanguageModel({
  model: fireworks('accounts/fireworks/models/deepseek-r1'),
  middleware: extractReasoningMiddleware({ tagName: 'think' }),
});

const { reasoning, text } = await generateText({
  model: enhancedModel,
  prompt: 'Explain quantum entanglement.',
});
```

----------------------------------------

TITLE: Stream Text with OpenAI Responses API and Reasoning Summaries
DESCRIPTION: This TypeScript example demonstrates how to stream text responses from the OpenAI Responses API using `streamText` and capture reasoning summaries. It shows how to configure `reasoningSummary` to 'detailed' and process both text deltas and reasoning parts from the stream.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/03-openai.mdx#_snippet_30

LANGUAGE: TypeScript
CODE:
```
import { openai } from '@ai-sdk/openai';
import { streamText } from 'ai';

const result = streamText({
  model: openai.responses('o4-mini'),
  prompt: 'Tell me about the Mission burrito debate in San Francisco.',
  providerOptions: {
    openai: {
      reasoningSummary: 'detailed',
    },
  },
});

for await (const part of result.fullStream) {
  if (part.type === 'reasoning') {
    console.log(`Reasoning: ${part.textDelta}`);
  } else if (part.type === 'text-delta') {
    process.stdout.write(part.textDelta);
  }
}
```

----------------------------------------

TITLE: Bootstrap Next.js AI Chat Example Project
DESCRIPTION: Commands to initialize a new Next.js project using the provided AI SDK example template. Choose your preferred package manager (npx, yarn, or pnpm) to set up the project.
SOURCE: https://github.com/vercel/ai/blob/main/examples/next-openai-upstash-rate-limits/README.md#_snippet_0

LANGUAGE: bash
CODE:
```
npx create-next-app --example https://github.com/vercel/ai/tree/main/examples/next-openai-rate-limits next-openai-rate-limits-app
```

LANGUAGE: bash
CODE:
```
yarn create next-app --example https://github.com/vercel/ai/tree/main/examples/next-openai-rate-limits next-openai-rate-limits-app
```

LANGUAGE: bash
CODE:
```
pnpm create next-app --example https://github.com/vercel/ai/tree/main/examples/next-openai-rate-limits next-openai-rate-limits-app
```

----------------------------------------

TITLE: OpenAI Reasoning Models API Overview
DESCRIPTION: Provides an overview of OpenAI's reasoning models, their availability through different APIs, current limitations (text generation only, specific function support), and configurable settings like `reasoningEffort` and response metadata such as `reasoningTokens`.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/03-openai.mdx#_snippet_11

LANGUAGE: APIDOC
CODE:
```
OpenAI Reasoning Models:
  Models: o1, o3, o4 series (o4-mini, o3, o3-mini, o1, o1-mini, o1-preview)
  Availability:
    - Chat API: o4-mini, o3, o3-mini, o1, o1-mini, o1-preview
    - Responses API: o4-mini, o3, o3-mini, o1, o1-mini, o1-preview, codex-mini-latest, computer-use-preview
  Limitations:
    - Generate text only
    - Supported only with generateText and streamText
    - Longer latency (especially o1-preview)
  Settings (via providerOptions.openai):
    - reasoningEffort: string ('low', 'medium', 'high') - determines reasoning amount
  Response Metadata (via providerMetadata.openai):
    - reasoningTokens: number - number of reasoning tokens generated
  Notes:
    - System messages converted to developer messages (or removed with warning for unsupported models like o1-preview)
    - maxOutputTokens mapped to max_completion_tokens
```

----------------------------------------

TITLE: Start Step Part Format and Example
DESCRIPTION: Defines the structure and provides an example of the part indicating the beginning of a processing step, including the ID of the message it belongs to.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/04-ai-sdk-ui/50-stream-protocol.mdx#_snippet_8

LANGUAGE: APIDOC
CODE:
```
Format: f:{messageId:string}
Example: f:{""messageId"":""step_123""}
```

----------------------------------------

TITLE: Control Reasoning Effort for OpenAI o3-mini
DESCRIPTION: This example shows how to adjust the reasoning effort of the OpenAI o3-mini model using the `reasoningEffort` parameter within `providerOptions`. Setting it to 'low' can optimize for faster responses, while 'medium' or 'high' can enhance reasoning power.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/00-guides/24-o3.mdx#_snippet_1

LANGUAGE: ts
CODE:
```
import { generateText } from 'ai';
import { openai } from '@ai-sdk/openai';

// Reduce reasoning effort for faster responses
const { text } = await generateText({
  model: openai('o3-mini'),
  prompt: 'Explain quantum entanglement briefly.',
  providerOptions: {
    openai: { reasoningEffort: 'low' },
  },
});
```

----------------------------------------

TITLE: Generate Text with OpenAI Responses API and PDF File Input
DESCRIPTION: This TypeScript example shows how to send a PDF file as part of the message content to the OpenAI Responses API using `generateText`. It demonstrates specifying the file type, data, media type, and an optional filename for PDF input.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/03-openai.mdx#_snippet_32

LANGUAGE: TypeScript
CODE:
```
const result = await generateText({
  model: openai.responses('gpt-4o'),
  messages: [
    {
      role: 'user',
      content: [
        {
          type: 'text',
          text: 'What is an embedding model?',
        },
        {
          type: 'file',
          data: fs.readFileSync('./data/ai.pdf'),
          mediaType: 'application/pdf',
          filename: 'ai.pdf',
        },
      ],
    },
  ],
});
```

----------------------------------------

TITLE: Implement Tool Calling with AI SDK and OpenAI
DESCRIPTION: This TypeScript code illustrates how to enable tool calling with AI SDK, allowing models like `o3-mini` to interact with external systems. It defines a `getWeather` tool with a Zod schema for parameters and an `execute` function to simulate fetching weather data, enhancing the model's capabilities.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/00-guides/24-o3.mdx#_snippet_3

LANGUAGE: ts
CODE:
```
import { generateText, tool } from 'ai';
import { openai } from '@ai-sdk/openai';
import { z } from 'zod';

const { text } = await generateText({
  model: openai('o3-mini'),
  prompt: 'What is the weather like today in San Francisco?',
  tools: {
    getWeather: tool({
      description: 'Get the weather in a location',
      parameters: z.object({
        location: z.string().describe('The location to get the weather for'),
      }),
      execute: async ({ location }) => ({
        location,
        temperature: 72 + Math.floor(Math.random() * 21) - 10,
      }),
    }),
  },
});
```

----------------------------------------

TITLE: Integrate Tools with AI SDK for External Interactions
DESCRIPTION: This example illustrates how to use the `generateText` function with a custom tool (`getWeather`) to enable LLMs to interact with external systems. It defines a tool with a description, Zod-validated parameters, and an asynchronous execution function to fetch simulated weather data, enhancing the model's ability to provide real-time information.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/00-guides/21-llama-3_1.mdx#_snippet_4

LANGUAGE: TypeScript
CODE:
```
import { generateText, tool } from 'ai';
import { deepinfra } from '@ai-sdk/deepinfra';
import { z } from 'zod';

const { text } = await generateText({
  model: deepinfra('meta-llama/Meta-Llama-3.1-70B-Instruct'),
  prompt: 'What is the weather like today?',
  tools: {
    getWeather: tool({
      description: 'Get the weather in a location',
      parameters: z.object({
        location: z.string().describe('The location to get the weather for'),
      }),
      execute: async ({ location }) => ({
        location,
        temperature: 72 + Math.floor(Math.random() * 21) - 10,
      }),
    }),
  },
});
```

----------------------------------------

TITLE: Transcribe Audio with Groq API and Options
DESCRIPTION: Demonstrates how to transcribe an audio file using the Groq transcription API. This example includes reading the audio from a file and providing provider-specific options like language to improve accuracy and latency.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/09-groq.mdx#_snippet_11

LANGUAGE: typescript
CODE:
```
import { experimental_transcribe as transcribe } from 'ai';
import { groq } from '@ai-sdk/groq';
import { readFile } from 'fs/promises';

const result = await transcribe({
  model: groq.transcription('whisper-large-v3'),
  audio: await readFile('audio.mp3'),
  providerOptions: { groq: { language: 'en' } },
});
```

----------------------------------------

TITLE: Start Streamable HTTP Transport Server
DESCRIPTION: Initiates the server for the streamable HTTP transport. This server handles stateful communication for the AI SDK example.
SOURCE: https://github.com/vercel/ai/blob/main/examples/mcp/README.md#_snippet_2

LANGUAGE: sh
CODE:
```
pnpm http:server
```

----------------------------------------

TITLE: MCPClient API Reference
DESCRIPTION: Defines the methods available on the `MCPClient` object, including `tools` for retrieving server-side tools with optional schema definitions, and `close` for terminating the connection and cleaning up resources. It details the signature, description, and parameters for each method.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/07-reference/01-ai-sdk-core/21-create-mcp-client.mdx#_snippet_2

LANGUAGE: APIDOC
CODE:
```
MCPClient:
  methods:
    tools(options?: { schemas?: TOOL_SCHEMAS }): Promise<McpToolSet<TOOL_SCHEMAS>>
      description: Gets the tools available from the MCP server.
      parameters:
        options:
          schemas: TOOL_SCHEMAS (optional)
            description: Schema definitions for compile-time type checking. When not provided, schemas are inferred from the server.
    close(): Promise<void>
      description: Closes the connection to the MCP server and cleans up resources.
```

----------------------------------------

TITLE: Implement Chat UI with useChat Hook in Next.js
DESCRIPTION: This client-side React component uses the `useChat` hook from `@ai-sdk/react` to manage chat state, input, and submission. It displays messages and provides an input field for user interaction, connecting to the backend API route.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/00-guides/21-llama-3_1.mdx#_snippet_7

LANGUAGE: tsx
CODE:
```
'use client';

import { useChat } from '@ai-sdk/react';

export default function Page() {
  const { messages, input, handleInputChange, handleSubmit } = useChat();

  return (
    <>
      {messages.map(message => (
        <div key={message.id}>
          {message.role === 'user' ? 'User: ' : 'AI: '}
          {message.content}
        </div>
      ))}
      <form onSubmit={handleSubmit}>
        <input name=""prompt"" value={input} onChange={handleInputChange} />
        <button type=""submit"">Submit</button>
      </form>
    </>
  );
}
```

----------------------------------------

TITLE: Generate Text with Spark AI Provider Example
DESCRIPTION: This example demonstrates the full workflow of using the Spark provider with the AI SDK. It shows how to initialize the provider with an API key, select a specific Spark model (e.g., 'lite'), and generate text based on a given prompt using the `generateText` utility.
SOURCE: https://github.com/vercel/ai/blob/v5/content/providers/03-community-providers/92-spark.mdx#_snippet_2

LANGUAGE: ts
CODE:
```
import { createSparkProvider } from './index.mjs';
import { generateText } from 'ai';
const spark = createSparkProvider({
  apiKey: ''
});
const { text } = await generateText({
  model: spark('lite'),
  prompt: 'Write a vegetarian lasagna recipe for 4 people.'
});
```

----------------------------------------

TITLE: API: TextStreamPart - Tool Call Streaming Start
DESCRIPTION: Defines the structure for the start of a tool call streaming event. This part signals that a tool call is beginning to stream.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/02-stream-text.mdx#_snippet_8

LANGUAGE: APIDOC
CODE:
```
Type: TextStreamPart
  Parameters:
    - name: type
      type: ""'tool-call-streaming-start'""
      description: Indicates the start of a tool call streaming. Only available when streaming tool calls.
    - name: toolCallId
      type: string
      description: The id of the tool call.
    - name: toolName
      type: string
      description: The name of the tool, which typically would be the name of the function.
```

----------------------------------------

TITLE: Implement Tool Calling with OpenAI Responses API and AI SDK
DESCRIPTION: This snippet demonstrates how to integrate tool calling capabilities with the AI SDK and OpenAI Responses API. It defines a `getWeather` tool with a description, parameters, and an `execute` function, allowing the model to interact with external systems (simulated here) to fetch information and enhance its responses.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/00-guides/19-openai-responses.mdx#_snippet_2

LANGUAGE: ts
CODE:
```
import { generateText, tool } from 'ai';
import { openai } from '@ai-sdk/openai';

const { text } = await generateText({
  model: openai.responses('gpt-4o'),
  prompt: 'What is the weather like today in San Francisco?',
  tools: {
    getWeather: tool({
      description: 'Get the weather in a location',
      parameters: z.object({
        location: z.string().describe('The location to get the weather for'),
      }),
      execute: async ({ location }) => ({
        location,
        temperature: 72 + Math.floor(Math.random() * 21) - 10,
      }),
    }),
  },
});
```

----------------------------------------

TITLE: Configure Multi-Step Agentic Generations with AI SDK
DESCRIPTION: This example extends the `streamText` function by adding a `maxSteps` parameter. This configuration enables the AI model to perform multiple sequential actions and send tool results back automatically, facilitating agentic behavior without requiring immediate user intervention. The `maxSteps` value can be adjusted based on the specific use case.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/00-guides/05-computer-use.mdx#_snippet_4

LANGUAGE: typescript
CODE:
```
const stream = streamText({
  model: anthropic('claude-3-5-sonnet-20241022'),
  prompt: 'Open the browser and navigate to vercel.com',
  tools: { computer: computerTool },
  maxSteps: 10,
});
```

----------------------------------------

TITLE: Generate Structured JSON Data with AI SDK and Zod
DESCRIPTION: This example illustrates how to generate structured JSON objects using the AI SDK's `generateObject` function. It integrates with Zod to define a type-safe schema for the output, such as a recipe, ensuring the model's response conforms to a predefined structure.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/00-guides/22-gpt-4-5.mdx#_snippet_1

LANGUAGE: typescript
CODE:
```
import { generateObject } from 'ai';
import { openai } from '@ai-sdk/openai';
import { z } from 'zod';

const { object } = await generateObject({
  model: openai('gpt-4.5-preview'),
  schema: z.object({
    recipe: z.object({
      name: z.string(),
      ingredients: z.array(z.object({ name: z.string(), amount: z.string() })),
      steps: z.array(z.string())
    })
  }),
  prompt: 'Generate a lasagna recipe.',
});
```

----------------------------------------

TITLE: Generate Text with Requesty and AI SDK
DESCRIPTION: An example demonstrating how to use Requesty with the AI SDK's `generateText` function. It shows how to specify a Requesty model and provide a prompt to get a text response.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/5-requesty.mdx#_snippet_4

LANGUAGE: javascript
CODE:
```
import { requesty } from '@requesty/ai-sdk';
import { generateText } from 'ai';

const { text } = await generateText({
  model: requesty('openai/gpt-4o'),
  prompt: 'Write a vegetarian lasagna recipe for 4 people.',
});

console.log(text);
```

----------------------------------------

TITLE: Tool Call Streaming Start Part Format and Example
DESCRIPTION: Defines the structure and provides an example of the part indicating the beginning of a streaming tool call. This part must be sent before any subsequent tool call delta for the same tool call.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/04-ai-sdk-ui/50-stream-protocol.mdx#_snippet_4

LANGUAGE: APIDOC
CODE:
```
Format: b:{toolCallId:string; toolName:string}
Example: b:{""toolCallId"":""call-456"",""toolName"":""streaming-tool""}
```

----------------------------------------

TITLE: Create Chat API Route with AI SDK `streamText` (Next.js App Router)
DESCRIPTION: This TypeScript snippet demonstrates how to set up a Next.js API route to handle incoming chat requests. It uses `streamText` from the AI SDK Core to generate a streaming text response from an OpenAI model, converting it into a UI message stream suitable for client-side consumption.
SOURCE: https://github.com/vercel/ai/blob/v5/packages/ai/README.md#_snippet_5

LANGUAGE: typescript
CODE:
```
import { streamText } from 'ai';
import { openai } from '@ai-sdk/openai';

export async function POST(req: Request) {
  const { messages } = await req.json();

  const result = streamText({
    model: openai('gpt-4o'),
    system: 'You are a helpful assistant.',
    messages,
  });

  return result.toUIMessageStreamResponse();
}
```

----------------------------------------

TITLE: Generate Text with AI SDK Gateway Provider Example
DESCRIPTION: An example showcasing the usage of the AI SDK Gateway provider to generate text. It imports both `gateway` and `generateText`, then uses the gateway to specify an AI model (e.g., 'xai/grok-3-beta') for a text generation prompt.
SOURCE: https://github.com/vercel/ai/blob/v5/packages/gateway/README.md#_snippet_2

LANGUAGE: ts
CODE:
```
import { gateway } from '@ai-sdk/gateway';
import { generateText } from 'ai';

const { text } = await generateText({
  model: gateway('xai/grok-3-beta'),
  prompt:
    'Tell me about the history of the San Francisco Mission-style burrito.',
});
```

----------------------------------------

TITLE: Enable Extended and Interleaved Thinking for Claude 4
DESCRIPTION: This example shows how to enable advanced reasoning capabilities for Claude 4 models. It configures extended thinking with a token budget via `providerOptions` and activates interleaved thinking by setting the `anthropic-beta` header, allowing the model to alternate between reasoning and tool use.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/00-guides/18-claude-4.mdx#_snippet_1

LANGUAGE: ts
CODE:
```
import { anthropic, AnthropicProviderOptions } from '@ai-sdk/anthropic';
import { generateText } from 'ai';

const { text, reasoning, reasoningDetails } = await generateText({
  model: anthropic('claude-4-sonnet-20250514'),
  prompt: 'How will quantum computing impact cryptography by 2050?',
  providerOptions: {
    anthropic: {
      thinking: { type: 'enabled', budgetTokens: 15000 },
    } satisfies AnthropicProviderOptions,
  },
  headers: {
    'anthropic-beta': 'interleaved-thinking-2025-05-14',
  },
});

console.log(text); // text response
console.log(reasoning); // reasoning text
console.log(reasoningDetails); // reasoning details including redacted reasoning
```

----------------------------------------

TITLE: Configure OpenAI API Key Environment Variable
DESCRIPTION: This command sets the `OPENAI_API_KEY` environment variable, which is essential for authenticating requests to the OpenAI API when using the AI SDK.
SOURCE: https://github.com/vercel/ai/blob/v5/examples/nest/README.md#_snippet_0

LANGUAGE: sh
CODE:
```
OPENAI_API_KEY=""YOUR_OPENAI_API_KEY""
```

----------------------------------------

TITLE: GoogleGenerativeAIStream API Signature
DESCRIPTION: Detailed API documentation for the `GoogleGenerativeAIStream` function, outlining its parameters, their types, descriptions, and nested callback options, along with its return type.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/07-reference/04-stream-helpers/14-google-generative-ai-stream.mdx#_snippet_1

LANGUAGE: APIDOC
CODE:
```
GoogleGenerativeAIStream(
  response: { stream: AsyncIterable<GenerateContentResponse> },
  callbacks?: AIStreamCallbacksAndOptions
) => ReadableStream

Parameters:
  response: { stream: AsyncIterable<GenerateContentResponse> }
    The response object returned by the Google Generative AI API.

  callbacks?: AIStreamCallbacksAndOptions
    An object containing callback functions to handle the start, each token, and completion of the AI response. In the absence of this parameter, default behavior is implemented.
    Properties of AIStreamCallbacksAndOptions:
      onStart?: () => Promise<void>
        An optional function that is called at the start of the stream processing.
      onCompletion?: (completion: string) => Promise<void>
        An optional function that is called for every completion. It's passed the completion as a string.
      onFinal?: (completion: string) => Promise<void>
        An optional function that is called once when the stream is closed with the final completion message.
      onToken?: (token: string) => Promise<void>
        An optional function that is called for each token in the stream. It's passed the token as a string.

Returns:
  ReadableStream
```

----------------------------------------

TITLE: valibotSchema Function API Reference
DESCRIPTION: Detailed API documentation for the `valibotSchema` helper function, outlining its parameters and return type for integration with the AI SDK.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/07-reference/01-ai-sdk-core/27-valibot-schema.mdx#_snippet_1

LANGUAGE: APIDOC
CODE:
```
valibotSchema(valibotSchema: GenericSchema<unknown, T>): Schema object
  valibotSchema:
    type: GenericSchema<unknown, T>
    description: The Valibot schema definition.
  Returns: A Schema object that is compatible with the AI SDK, containing both the JSON schema representation and validation functionality.
```

----------------------------------------

TITLE: Basic Text Generation with SambaNova
DESCRIPTION: Demonstrates how to perform basic text generation using the SambaNova provider and the `generateText` function from the AI SDK, including API key setup and model instantiation.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/96-sambanova.mdx#_snippet_5

LANGUAGE: TypeScript
CODE:
```
import { createSambaNova } from 'sambanova-ai-provider';
import { generateText } from 'ai';

const sambanova = createSambaNova({
  apiKey: 'YOUR_API_KEY',
});

const model = sambanova('Meta-Llama-3.1-70B-Instruct');

const { text } = await generateText({
  model,
  prompt: 'Hello, nice to meet you.',
});

console.log(text);
```

----------------------------------------

TITLE: Install Dependencies and Build AI SDK Project
DESCRIPTION: These commands are necessary to prepare the AI SDK repository for development or execution. `pnpm install` fetches all required project dependencies, while `pnpm build` compiles the project source code.
SOURCE: https://github.com/vercel/ai/blob/v5/examples/ai-core/README.md#_snippet_1

LANGUAGE: sh
CODE:
```
pnpm install
pnpm build
```

----------------------------------------

TITLE: Example using `render` with direct OpenAI provider
DESCRIPTION: This TypeScript/React Server Component example demonstrates the use of the `render` function from `@ai-sdk/rsc` with a direct `OpenAI` provider instance. It shows how to define a tool (`get_city_weather`) that returns a React Server Component (`<Weather />`) after yielding a `<Spinner />` during an asynchronous operation, illustrating the previous API structure.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/08-migration-guides/39-migration-guide-3-1.mdx#_snippet_3

LANGUAGE: tsx
CODE:
```
import { render } from '@ai-sdk/rsc';
import OpenAI from 'openai';
import { z } from 'zod';
import { Spinner, Weather } from '@/components';
import { getWeather } from '@/utils';

const openai = new OpenAI();

async function submitMessage(userInput = 'What is the weather in SF?') {
  'use server';

  return render({
    provider: openai,
    model: 'gpt-4.1',
    messages: [
      { role: 'system', content: 'You are a helpful assistant' },
      { role: 'user', content: userInput },
    ],
    text: ({ content }) => <p>{content}</p>,
    tools: {
      get_city_weather: {
        description: 'Get the current weather for a city',
        parameters: z
          .object({
            city: z.string().describe('the city'),
          })
          .required(),
        render: async function* ({ city }) {
          yield <Spinner />;
          const weather = await getWeather(city);
          return <Weather info={weather} />;
        },
      },
    },
  });
}
```

----------------------------------------

TITLE: Bootstrap Next.js AI Chat Example Project
DESCRIPTION: Commands to initialize a new Next.js project using the Vercel AI SDK and OpenAI example template, supporting npm, Yarn, and pnpm package managers.
SOURCE: https://github.com/vercel/ai/blob/main/examples/next-openai-telemetry/app/README.md#_snippet_0

LANGUAGE: bash
CODE:
```
npx create-next-app --example https://github.com/vercel/ai/tree/main/examples/next-openai next-openai-app
```

LANGUAGE: bash
CODE:
```
yarn create next-app --example https://github.com/vercel/ai/tree/main/examples/next-openai next-openai-app
```

LANGUAGE: bash
CODE:
```
pnpm create next-app --example https://github.com/vercel/ai/tree/main/examples/next-openai next-openai-app
```

----------------------------------------

TITLE: Generate Text with OpenAI Responses API and Access Reasoning Summaries
DESCRIPTION: This TypeScript example illustrates how to generate a single text response using `generateText` from the OpenAI Responses API and access the `reasoning` field from the result. It configures `reasoningSummary` to 'auto' for automatic summary level.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/03-openai.mdx#_snippet_31

LANGUAGE: TypeScript
CODE:
```
import { openai } from '@ai-sdk/openai';
import { generateText } from 'ai';

const result = await generateText({
  model: openai.responses('o3-mini'),
  prompt: 'Tell me about the Mission burrito debate in San Francisco.',
  providerOptions: {
    openai: {
      reasoningSummary: 'auto',
    },
  },
});
console.log('Reasoning:', result.reasoning);
```

----------------------------------------

TITLE: Install and Build Project Dependencies
DESCRIPTION: Executes commands to install all necessary project dependencies and then builds the application. This prepares the project for execution by resolving packages and compiling source code.
SOURCE: https://github.com/vercel/ai/blob/v5/examples/hono/README.md#_snippet_1

LANGUAGE: sh
CODE:
```
pnpm install
pnpm build
```

----------------------------------------

TITLE: Bootstrap Next.js AI Chat Example Project
DESCRIPTION: Commands to initialize a new Next.js project based on the AI SDK OpenAI example using different package managers (npx, yarn, or pnpm). This sets up the project structure and dependencies.
SOURCE: https://github.com/vercel/ai/blob/main/examples/next-openai/README.md#_snippet_0

LANGUAGE: bash
CODE:
```
npx create-next-app --example https://github.com/vercel/ai/tree/main/examples/next-openai next-openai-app
```

LANGUAGE: bash
CODE:
```
yarn create next-app --example https://github.com/vercel/ai/tree/main/examples/next-openai next-openai-app
```

LANGUAGE: bash
CODE:
```
pnpm create next-app --example https://github.com/vercel/ai/tree/main/examples/next-openai next-openai-app
```

----------------------------------------

TITLE: Start Development Server
DESCRIPTION: Launches the Fastify development server. Once running, the application will be accessible locally, typically on port 8080, ready to handle requests.
SOURCE: https://github.com/vercel/ai/blob/v5/examples/fastify/README.md#_snippet_2

LANGUAGE: sh
CODE:
```
pnpm dev
```

----------------------------------------

TITLE: Generate Text with OpenAI's Native Web Search Tool
DESCRIPTION: This snippet demonstrates how to use OpenAI's Responses API with the built-in `web_search_preview` tool to perform web searches and retrieve results. It shows how to import the OpenAI provider and `generateText` function, then call `generateText` with a prompt and the `web_search_preview` tool enabled.
SOURCE: https://github.com/vercel/ai/blob/v5/content/cookbook/05-node/56-web-search-agent.mdx#_snippet_0

LANGUAGE: typescript
CODE:
```
import { openai } from '@ai-sdk/openai';
import { generateText } from 'ai';

const { text, sources } = await generateText({
  model: openai.responses('gpt-4o-mini'),
  prompt: 'What happened in San Francisco last week?',
  tools: {
    web_search_preview: openai.tools.webSearchPreview(),
  },
});

console.log(text);
console.log(sources);
```

----------------------------------------

TITLE: transcribe() API Reference
DESCRIPTION: Detailed API documentation for the `transcribe()` function, outlining its input parameters with their types and descriptions, and the structure of the object returned upon successful transcription, including nested types.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/07-reference/01-ai-sdk-core/11-transcribe.mdx#_snippet_2

LANGUAGE: APIDOC
CODE:
```
transcribe() API Signature

Parameters:
  model: TranscriptionModelV2
    Description: The transcription model to use.
  audio: DataContent (string | Uint8Array | ArrayBuffer | Buffer) | URL
    Description: The audio file to generate the transcript from.
  providerOptions?: Record<string, Record<string, JSONValue>>
    Description: Additional provider-specific options.
  maxRetries?: number
    Description: Maximum number of retries. Default: 2.
  abortSignal?: AbortSignal
    Description: An optional abort signal to cancel the call.
  headers?: Record<string, string>
    Description: Additional HTTP headers for the request.

Returns:
  text: string
    Description: The complete transcribed text from the audio input.
  segments: Array<{ text: string; startSecond: number; endSecond: number }>
    Description: An array of transcript segments, each containing a portion of the transcribed text along with its start and end times in seconds.
  language?: string
    Description: The language of the transcript in ISO-639-1 format e.g. ""en"" for English.
  durationInSeconds?: number
    Description: The duration of the transcript in seconds.
  warnings: TranscriptionWarning[]
    Description: Warnings from the model provider (e.g. unsupported settings).
  responses: Array<TranscriptionModelResponseMetadata>
    Description: Response metadata from the provider. There may be multiple responses if we made multiple calls to the model.
    Properties of TranscriptionModelResponseMetadata:
      timestamp: Date
        Description: Timestamp for the start of the generated response.
      modelId: string
        Description: The ID of the response model that was used to generate the response.
      headers?: Record<string, string>
        Description: Response headers.
```

----------------------------------------

TITLE: Execute an AI Core Example Script with pnpm tsx
DESCRIPTION: This command demonstrates how to run a specific AI Core example script located within the `examples/ai-core` directory. It uses `pnpm tsx` to directly execute TypeScript files, facilitating quick testing and iteration.
SOURCE: https://github.com/vercel/ai/blob/v5/examples/ai-core/README.md#_snippet_2

LANGUAGE: sh
CODE:
```
pnpm tsx src/path/to/example.ts
```

----------------------------------------

TITLE: Generate Structured Text with OpenAI Responses API using experimental_output
DESCRIPTION: This TypeScript example shows how to enforce structured outputs using `generateText` with the `experimental_output` option in the OpenAI Responses API. It uses a Zod schema to define the expected structure for ingredients and steps within the text output.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/03-openai.mdx#_snippet_34

LANGUAGE: TypeScript
CODE:
```
// Using generateText
const result = await generateText({
  model: openai.responses('gpt-4.1'),
  prompt: 'How do I make a pizza?',
  experimental_output: Output.object({
    schema: z.object({
      ingredients: z.array(z.string()),
      steps: z.array(z.string()),
    }),
  }),
});
```

----------------------------------------

TITLE: Test Fastify Endpoint with Curl
DESCRIPTION: Sends a POST request to the local Fastify server's root endpoint using the curl command-line tool. This verifies that the server is running and responding to API calls as expected.
SOURCE: https://github.com/vercel/ai/blob/v5/examples/fastify/README.md#_snippet_3

LANGUAGE: sh
CODE:
```
curl -X POST http://localhost:8080
```

----------------------------------------

TITLE: Configure OpenAI API key in .env
DESCRIPTION: Adds the OpenAI API key to the .env file. This key is essential for authenticating your application with the OpenAI service. Remember to replace 'xxxxxxxxx' with your actual API key.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-getting-started/05-nuxt.mdx#_snippet_4

LANGUAGE: dotenv
CODE:
```
OPENAI_API_KEY=xxxxxxxxx
```

----------------------------------------

TITLE: Configure Requesty with Custom Settings
DESCRIPTION: Demonstrates how to initialize the Requesty AI SDK with custom API key, base URL, headers, and extra body parameters using `createRequesty`.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/5-requesty.mdx#_snippet_8

LANGUAGE: javascript
CODE:
```
import { createRequesty } from '@requesty/ai-sdk';

const requesty = createRequesty({
  apiKey: process.env.REQUESTY_API_KEY,
  baseURL: 'https://router.requesty.ai/v1',
  headers: {
    'Custom-Header': 'custom-value',
  },
  extraBody: {
    custom_field: 'value',
  },
});
```

----------------------------------------

TITLE: Test Local API Endpoint with Curl
DESCRIPTION: This `curl` command sends a POST request to the specified local Nest.js server endpoint, allowing you to verify its functionality and response.
SOURCE: https://github.com/vercel/ai/blob/v5/examples/nest/README.md#_snippet_3

LANGUAGE: sh
CODE:
```
curl -X POST http://localhost:8080
```

----------------------------------------

TITLE: createStreamableValue API Signature
DESCRIPTION: Detailed API documentation for the `createStreamableValue` function, outlining its input parameters and the structure of its return value.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/07-reference/03-ai-sdk-rsc/04-create-streamable-value.mdx#_snippet_1

LANGUAGE: APIDOC
CODE:
```
createStreamableValue(value: any): streamable
  Parameters:
    value: any - Any data that RSC supports. Example, JSON.
  Returns:
    value: streamable - This creates a special value that can be returned from Actions to the client. It holds the data inside and can be updated via the update method.
```

----------------------------------------

TITLE: Bootstrap Nuxt AI Chat Example Project
DESCRIPTION: Command to initialize a new Nuxt project based on the Vercel AI SDK example template for an OpenAI chat bot.
SOURCE: https://github.com/vercel/ai/blob/main/examples/nuxt-openai/README.md#_snippet_0

LANGUAGE: bash
CODE:
```
npx create-nuxt -t github:vercel/ai/examples/nuxt-openai nuxt-openai
```

----------------------------------------

TITLE: API Reference: `onStepFinish` Callback Parameters
DESCRIPTION: Lists the parameters provided to the `onStepFinish` callback function, offering detailed information about the completed step, including generated text, tool calls, tool results, finish reason, and usage.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/15-tools-and-tool-calling.mdx#_snippet_11

LANGUAGE: APIDOC
CODE:
```
onStepFinish: (params: object) => void
  params:
    - text: string
    - toolCalls: ToolCall[]
    - toolResults: ToolResult[]
    - finishReason: string
    - usage: object
```

----------------------------------------

TITLE: Example migrating to `streamUI` with AI SDK OpenAI provider
DESCRIPTION: This TypeScript/React Server Component example illustrates the migration to the `streamUI` function from `@ai-sdk/rsc`, utilizing the `@ai-sdk/openai` provider. It showcases how to define a tool (`get_city_weather`) with a `generate` key that yields a `<Spinner />` and then returns a React Server Component (`<Weather />`), demonstrating the new API structure for improved flexibility and integration with AI SDK providers.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/08-migration-guides/39-migration-guide-3-1.mdx#_snippet_4

LANGUAGE: tsx
CODE:
```
import { streamUI } from '@ai-sdk/rsc';
import { openai } from '@ai-sdk/openai';
import { z } from 'zod';
import { Spinner, Weather } from '@/components';
import { getWeather } from '@/utils';

async function submitMessage(userInput = 'What is the weather in SF?') {
  'use server';

  const result = await streamUI({
    model: openai('gpt-4.1'),
    system: 'You are a helpful assistant',
    messages: [{ role: 'user', content: userInput }],
    text: ({ content }) => <p>{content}</p>,
    tools: {
      get_city_weather: {
        description: 'Get the current weather for a city',
        parameters: z
          .object({
            city: z.string().describe('Name of the city'),
          })
          .required(),
        generate: async function* ({ city }) {
          yield <Spinner />;
          const weather = await getWeather(city);
          return <Weather info={weather} />;
        },
      },
    },
  });

  return result.value;
}
```

----------------------------------------

TITLE: Use Tools with Requesty and AI SDK (generateObject)
DESCRIPTION: An example showcasing tool usage with Requesty and the AI SDK's `generateObject` function. It demonstrates defining a Zod schema for structured output and generating an object based on a prompt.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/5-requesty.mdx#_snippet_6

LANGUAGE: javascript
CODE:
```
import { requesty } from '@requesty/ai-sdk';
import { generateObject } from 'ai';
import { z } from 'zod';

const { object } = await generateObject({
  model: requesty('openai/gpt-4o'),
  schema: z.object({
    recipe: z.object({
      name: z.string(),
      ingredients: z.array(z.string()),
      steps: z.array(z.string())
    })
  }),
  prompt: 'Generate a recipe for chocolate chip cookies.',
});

console.log(object.recipe);
```

----------------------------------------

TITLE: Generate Text with Llama 3.1 using Amazon Bedrock and AI SDK
DESCRIPTION: This example shows how to switch the AI SDK provider to Amazon Bedrock for generating text with Llama 3.1. It highlights the flexibility of the AI SDK's unified interface, allowing easy model and provider changes by importing the Bedrock provider and using it in the `generateText` call.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/00-guides/21-llama-3_1.mdx#_snippet_1

LANGUAGE: TypeScript
CODE:
```
import { generateText } from 'ai';
import { bedrock } from '@ai-sdk/amazon-bedrock';

const { text } = await generateText({
  model: bedrock('meta.llama3-1-405b-instruct-v1'),
  prompt: 'What is love?',
});
```

----------------------------------------

TITLE: Example AI Provider Interface Methods
DESCRIPTION: Defines the `ExampleProvider` interface, outlining the methods available for creating different types of AI models (chat, completion, embedding, image). Each method specifies the model ID and optional settings, returning the appropriate AI SDK model type.
SOURCE: https://github.com/vercel/ai/blob/v5/content/providers/02-openai-compatible-providers/01-custom-providers.mdx#_snippet_3

LANGUAGE: APIDOC
CODE:
```
ExampleProvider:
  (modelId: ExampleChatModelId, settings?: ExampleChatSettings): LanguageModelV1
    Creates a model for text generation.
  chatModel(modelId: ExampleChatModelId, settings?: ExampleChatSettings): LanguageModelV1
    Creates a chat model for text generation.
  completionModel(modelId: ExampleCompletionModelId, settings?: ExampleCompletionSettings): LanguageModelV1
    Creates a completion model for text generation.
  textEmbeddingModel(modelId: ExampleEmbeddingModelId, settings?: ExampleEmbeddingSettings): EmbeddingModelV2<string>
    Creates a text embedding model for text generation.
  imageModel(modelId: ExampleImageModelId, settings?: ExampleImageSettings): ImageModelV2
```

----------------------------------------

TITLE: Bootstrap Next.js Project with AI SDK Vertex AI Example
DESCRIPTION: This command uses `create-next-app` to initialize a new Next.js project, pre-configured with the AI SDK and Google Vertex AI Edge example. It fetches the example directly from the Vercel AI GitHub repository, providing a quick way to set up the project locally for development.
SOURCE: https://github.com/vercel/ai/blob/main/examples/next-google-vertex/README.md#_snippet_0

LANGUAGE: bash
CODE:
```
npx create-next-app --example https://github.com/vercel/ai/tree/main/examples/next-google-vertex-edge next-vertex-edge-app
```

----------------------------------------

TITLE: LlamaIndex Adapter API Methods
DESCRIPTION: Defines the core methods provided by the `@ai-sdk/llamaindex` package for converting LlamaIndex streams into AI SDK compatible data streams and responses.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/04-stream-helpers/16-llamaindex-adapter.mdx#_snippet_1

LANGUAGE: APIDOC
CODE:
```
toDataStream(stream: AsyncIterable<EngineResponse>, AIStreamCallbacksAndOptions): AIStream
  Converts LlamaIndex output streams to data stream.

toDataStreamResponse(stream: AsyncIterable<EngineResponse>, options?: {init?: ResponseInit, data?: StreamData, callbacks?: AIStreamCallbacksAndOptions}): Response
  Converts LlamaIndex output streams to data stream response.

mergeIntoDataStream(stream: AsyncIterable<EngineResponse>, options: { dataStream: DataStreamWriter; callbacks?: StreamCallbacks }): void
  Merges LlamaIndex output streams into an existing data stream.
```

----------------------------------------

TITLE: Importing Internal OpenAI-Compatible API Functions
DESCRIPTION: This example illustrates how to access and import internal utility functions from the `@ai-sdk/openai-compatible/internal` package. These functions, such as `convertToOpenAICompatibleChatMessages`, can be useful for advanced provider development and integration.
SOURCE: https://github.com/vercel/ai/blob/v5/content/providers/02-openai-compatible-providers/01-custom-providers.mdx#_snippet_8

LANGUAGE: ts
CODE:
```
import { convertToOpenAICompatibleChatMessages } from '@ai-sdk/openai-compatible/internal';
```

----------------------------------------

TITLE: Call OpenAI o3-mini with AI SDK
DESCRIPTION: This snippet demonstrates how to make a basic text generation call to the OpenAI o3-mini model using the AI SDK. It imports necessary functions from 'ai' and '@ai-sdk/openai' to generate text based on a given prompt.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/00-guides/24-o3.mdx#_snippet_0

LANGUAGE: ts
CODE:
```
import { generateText } from 'ai';
import { openai } from '@ai-sdk/openai';

const { text } = await generateText({
  model: openai('o3-mini'),
  prompt: 'Explain the concept of quantum entanglement.',
});
```

----------------------------------------

TITLE: Generate Structured JSON Data with AI SDK
DESCRIPTION: This example illustrates how to generate structured JSON data using the AI SDK's `generateObject` function. It leverages Zod for schema definition, ensuring type-safe output from the OpenAI Responses API (gpt-4o model) based on a provided prompt, useful for extracting or classifying information.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/00-guides/19-openai-responses.mdx#_snippet_1

LANGUAGE: ts
CODE:
```
import { generateObject } from 'ai';
import { openai } from '@ai-sdk/openai';
import { z } from 'zod';

const { object } = await generateObject({
  model: openai.responses('gpt-4o'),
  schema: z.object({
    recipe: z.object({
      name: z.string(),
      ingredients: z.array(z.object({ name: z.string(), amount: z.string() })),
      steps: z.array(z.string()),
    }),
  }),
  prompt: 'Generate a lasagna recipe.',
});
```

----------------------------------------

TITLE: Install AI SDK and Anthropic Provider
DESCRIPTION: This command installs the necessary packages for using the AI SDK with the Anthropic provider. It adds `ai` (beta) and `@ai-sdk/anthropic` (beta) to your project dependencies, enabling you to start building AI applications with computer interaction capabilities.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/00-guides/05-computer-use.mdx#_snippet_0

LANGUAGE: shell
CODE:
```
pnpm add ai@beta @ai-sdk/anthropic@beta
```

----------------------------------------

TITLE: Convert UIMessage to ModelMessage for Streaming API
DESCRIPTION: This TypeScript example demonstrates how to handle incoming `UIMessage`s in a Next.js API route. It uses `convertToModelMessages` from the `ai` package to transform UI-specific messages into a format suitable for language models, then streams the response using `streamText` with an OpenAI model.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/01-announcing-ai-sdk-5-beta/index.mdx#_snippet_1

LANGUAGE: ts
CODE:
```
import { openai } from '@ai-sdk/openai';
import { convertToModelMessages, streamText, UIMessage } from 'ai';

export async function POST(req: Request) {
  const { messages }: { messages: UIMessage[] } = await req.json();

  const result = streamText({
    model: openai('gpt-4o'),
    messages: convertToModelMessages(messages),
  });

  return result.toUIMessageStreamResponse();
}
```

----------------------------------------

TITLE: customProvider Function API Reference
DESCRIPTION: Detailed API documentation for the `customProvider` function, outlining its parameters and the structure of the returned `Provider` instance with its methods.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/07-reference/01-ai-sdk-core/42-custom-provider.mdx#_snippet_2

LANGUAGE: APIDOC
CODE:
```
customProvider(parameters): Provider instance
  Parameters:
    languageModels: Record<string, LanguageModel> (optional)
      description: A record of language models, where keys are model IDs and values are LanguageModel instances.
    textEmbeddingModels: Record<string, EmbeddingModel<string>> (optional)
      description: A record of text embedding models, where keys are model IDs and values are EmbeddingModel<string> instances.
    imageModels: Record<string, ImageModel> (optional)
      description: A record of image models, where keys are model IDs and values are ImageModelV2 instances.
    fallbackProvider: Provider (optional)
      description: An optional fallback provider to use when a requested model is not found in the custom provider.
  Returns:
    Provider instance with the following methods:
      languageModel(id: string): LanguageModel
        description: A function that returns a language model by its id (format: providerId:modelId)
      textEmbeddingModel(id: string): EmbeddingModel<string>
        description: A function that returns a text embedding model by its id (format: providerId:modelId)
      imageModel(id: string): ImageModel
        description: A function that returns an image model by its id (format: providerId:modelId)
```

----------------------------------------

TITLE: Bootstrap Next.js AI Chat App with create-next-app
DESCRIPTION: Commands to quickly set up a new Next.js project based on the AI SDK, Next.js, and OpenAI telemetry example. These commands use `create-next-app` with different package managers to clone and initialize the project.
SOURCE: https://github.com/vercel/ai/blob/main/examples/next-openai-telemetry/README.md#_snippet_0

LANGUAGE: bash
CODE:
```
npx create-next-app --example https://github.com/vercel/ai/tree/main/examples/next-openai-telemetry next-openai-telemetry-app
```

LANGUAGE: bash
CODE:
```
yarn create next-app --example https://github.com/vercel/ai/tree/main/examples/next-openai-telemetry next-openai-telemetry-app
```

LANGUAGE: bash
CODE:
```
pnpm create next-app --example https://github.com/vercel/ai/tree/main/examples/next-openai-telemetry next-openai-telemetry-app
```

----------------------------------------

TITLE: Log OpenAI API Requests with Custom Fetch in AI SDK
DESCRIPTION: This TypeScript example demonstrates how to provide a custom `fetch` function to the `@ai-sdk/openai` client. The custom function intercepts API requests, logs the URL, headers, and parsed JSON body, and then proceeds with the original fetch call. This is useful for debugging, monitoring, or custom request manipulation.
SOURCE: https://github.com/vercel/ai/blob/v5/content/cookbook/05-node/70-intercept-fetch-requests.mdx#_snippet_0

LANGUAGE: typescript
CODE:
```
import { generateText } from 'ai';
import { createOpenAI } from '@ai-sdk/openai';

const openai = createOpenAI({
  // example fetch wrapper that logs the input to the API call:
  fetch: async (url, options) => {
    console.log('URL', url);
    console.log('Headers', JSON.stringify(options!.headers, null, 2));
    console.log(
      `Body ${JSON.stringify(JSON.parse(options!.body! as string), null, 2)}`,
    );
    return await fetch(url, options);
  },
});

const { text } = await generateText({
  model: openai('gpt-3.5-turbo'),
  prompt: 'Why is the sky blue?',
});
```

----------------------------------------

TITLE: SambaNova Provider Instance Configuration Options
DESCRIPTION: API documentation for the optional settings available when creating a customized SambaNova provider instance, including `baseURL`, `apiKey`, `headers`, and `fetch`.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/96-sambanova.mdx#_snippet_3

LANGUAGE: APIDOC
CODE:
```
createSambaNova(options?: object)
  options:
    baseURL: string
      Use a different URL prefix for API calls. Default: `https://api.sambanova.ai/v1`.
    apiKey: string
      API key sent using the `Authorization` header. Defaults to `SAMBANOVA_API_KEY` environment variable.
    headers: Record<string, string>
      Custom headers to include in requests.
    fetch: (input: RequestInfo, init?: RequestInit) => Promise<Response>
      Custom fetch implementation. Defaults to global `fetch` function. Useful for intercepting requests or providing a custom fetch implementation.
```

----------------------------------------

TITLE: Cohere Language Model Capabilities (APIDOC)
DESCRIPTION: Overview of capabilities for popular Cohere language models, including support for image input, object generation, tool usage, and tool streaming.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/25-cohere.mdx#_snippet_6

LANGUAGE: APIDOC
CODE:
```
Model               | Image Input | Object Generation | Tool Usage | Tool Streaming
--------------------|-------------|-------------------|------------|---------------
`command-a-03-2025` | No          | Yes               | Yes        | Yes
`command-r-plus`    | No          | Yes               | Yes        | Yes
`command-r`         | No          | Yes               | Yes        | Yes
`command-a-03-2025` | No          | Yes               | Yes        | Yes
`command`           | No          | No                | No         | No
`command-light`     | No          | No                | No         | No
```

----------------------------------------

TITLE: Migrate AI SDK `generateText` between OpenAI and DeepInfra
DESCRIPTION: Demonstrates the AI SDK's unified API for seamless migration between different AI models and providers. This snippet shows how to use the `generateText` function with both OpenAI's GPT-4.1 and DeepInfra's Meta Llama 3.1-70B-Instruct, highlighting that only the provider import and model name change, while the core `generateText` usage remains consistent.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/00-guides/21-llama-3_1.mdx#_snippet_10

LANGUAGE: tsx
CODE:
```
import { generateText } from 'ai';
import { openai } from '@ai-sdk/openai';

const { text } = await generateText({
  model: openai('gpt-4.1'),
  prompt: 'What is love?',
});
```

LANGUAGE: tsx
CODE:
```
import { generateText } from 'ai';
import { deepinfra } from '@ai-sdk/deepinfra';

const { text } = await generateText({
  model: deepinfra('meta-llama/Meta-Llama-3.1-70B-Instruct'),
  prompt: 'What is love?',
});
```

----------------------------------------

TITLE: Implement Firecrawl for AI-Driven Web Scraping
DESCRIPTION: This TypeScript example illustrates how to build a web scraping tool using the Firecrawl API. It defines a 'webSearch' tool (which performs URL crawling) that takes a URL, crawls it using Firecrawl, and handles the response, including error checking. The snippet also demonstrates how to integrate this tool with an AI model to prompt for specific web content, such as the latest blog post from a given URL.
SOURCE: https://github.com/vercel/ai/blob/v5/content/cookbook/05-node/56-web-search-agent.mdx#_snippet_4

LANGUAGE: typescript
CODE:
```
import { generateText, tool } from 'ai';
import { openai } from '@ai-sdk/openai';
import { z } from 'zod';
import FirecrawlApp from '@mendable/firecrawl-js';
import 'dotenv/config';

const app = new FirecrawlApp({ apiKey: process.env.FIRECRAWL_API_KEY });

export const webSearch = tool({
  description: 'Search the web for up-to-date information',
  parameters: z.object({
    urlToCrawl: z
      .string()
      .url()
      .min(1)
      .max(100)
      .describe('The URL to crawl (including http:// or https://)'),
  }),
  execute: async ({ urlToCrawl }) => {
    const crawlResponse = await app.crawlUrl(urlToCrawl, {
      limit: 1,
      scrapeOptions: {
        formats: ['markdown', 'html'],
      },
    });
    if (!crawlResponse.success) {
      throw new Error(`Failed to crawl: ${crawlResponse.error}`);
    }
    return crawlResponse.data;
  },
});

const main = async () => {
  const { text } = await generateText({
    model: openai('gpt-4o-mini'), // can be any model that supports tools
    prompt: 'Get the latest blog post from vercel.com/blog',
    tools: {
      webSearch,
    },
    maxSteps: 2,
  });
  console.log(text);
};

main();
```

----------------------------------------

TITLE: Set Requesty API Key Environment Variable
DESCRIPTION: Securely configure your Requesty API key by setting it as an environment variable named `REQUESTY_API_KEY`. Examples are provided for Linux/Mac (bash), Windows Command Prompt, and Windows PowerShell.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/5-requesty.mdx#_snippet_1

LANGUAGE: bash
CODE:
```
# Linux/Mac
export REQUESTY_API_KEY=your_api_key_here
```

LANGUAGE: cmd
CODE:
```
# Windows Command Prompt
set REQUESTY_API_KEY=your_api_key_here
```

LANGUAGE: powershell
CODE:
```
# Windows PowerShell
$env:REQUESTY_API_KEY=""your_api_key_here""
```

----------------------------------------

TITLE: Example Usage of streamToResponse with Node.js HTTP Server
DESCRIPTION: Demonstrates how to use `streamToResponse` to pipe a text stream from an OpenAI model to a Node.js `ServerResponse` object. This example sets up a basic HTTP server, generates a text stream using `@ai-sdk/openai`, and uses `StreamData` to append additional information to the stream before sending it as a response.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/07-reference/04-stream-helpers/05-stream-to-response.mdx#_snippet_1

LANGUAGE: typescript
CODE:
```
import { openai } from '@ai-sdk/openai';
import { StreamData, streamText, streamToResponse } from 'ai';
import { createServer } from 'http';

createServer(async (req, res) => {
  const result = streamText({
    model: openai('gpt-4.1'),
    prompt: 'What is the weather in San Francisco?',
  });

  // use stream data
  const data = new StreamData();

  data.append('initialized call');

  streamToResponse(
    result.toAIStream({
      onFinal() {
        data.append('call completed');
        data.close();
      },
    }),
    res,
    {},
    data,
  );
}).listen(8080);
```

----------------------------------------

TITLE: Generate Speech using Hume AI SDK Provider
DESCRIPTION: An example demonstrating how to use the Hume provider with AI SDK's experimental_generateSpeech function to convert text to speech via the Hume API.
SOURCE: https://github.com/vercel/ai/blob/main/packages/hume/README.md#_snippet_2

LANGUAGE: typescript
CODE:
```
import { hume } from '@ai-sdk/hume';
import { experimental_generateSpeech as generateSpeech } from 'ai';

const result = await generateSpeech({
  model: hume.speech('aurora'),
  text: 'Hello, world!',
});
```

----------------------------------------

TITLE: Stream Text Responses with AI Gateway Provider
DESCRIPTION: Demonstrates how to use the `streamText` function from the AI SDK with the `ai-gateway-provider` and `@ai-sdk/openai` to stream AI-generated text. It includes setup for `createAiGateway` with account details and `createOpenAI` with an API key, followed by an example of making a prompt and iterating over the text stream.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/12-cloudflare-ai-gateway.mdx#_snippet_7

LANGUAGE: typescript
CODE:
```
import { createAiGateway } from 'ai-gateway-provider';
import { createOpenAI } from '@ai-sdk/openai';
import { streamText } from 'ai';

const aigateway = createAiGateway({
  accountId: 'your-cloudflare-account-id',
  gateway: 'your-gateway-name',
  apiKey: 'your-cloudflare-api-key'
});

const openai = createOpenAI({ apiKey: 'openai-api-key' });

const result = await streamText({
  model: aigateway([openai('gpt-4o-mini')]),
  prompt: 'Write a multi-part greeting.'
});

let accumulatedText = '';
for await (const chunk of result.textStream) {
  accumulatedText += chunk;
}

console.log(accumulatedText); // Output: ""Hello world!""
```

----------------------------------------

TITLE: Generate Text with OpenAI GPT-4.5 using AI SDK
DESCRIPTION: This code demonstrates basic text generation using the AI SDK with OpenAI's GPT-4.5 model. It shows how to import `generateText` and `openai` to prompt the model for an explanation, returning the generated text.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/00-guides/22-gpt-4-5.mdx#_snippet_0

LANGUAGE: typescript
CODE:
```
import { generateText } from 'ai';
import { openai } from '@ai-sdk/openai';

const { text } = await generateText({
  model: openai('gpt-4.5-preview'),
  prompt: 'Explain the concept of quantum entanglement.',
});
```

----------------------------------------

TITLE: Simulating AI SDK API Errors for Testing
DESCRIPTION: This example shows how to intentionally throw an error in an API route handler (e.g., `app/api/chat/route.ts`) to simulate server-side errors for testing purposes within the AI SDK environment.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/04-ai-sdk-ui/21-error-handling.mdx#_snippet_3

LANGUAGE: ts
CODE:
```
export async function POST(req: Request) {
  throw new Error('This is a test error');
}
```

----------------------------------------

TITLE: Configure Environment Variables for AI SDK
DESCRIPTION: This snippet outlines how to set up essential environment variables, such as `OPENAI_API_KEY`, in a `.env` file. These variables are crucial for authenticating with various AI providers and enabling the SDK's functionality.
SOURCE: https://github.com/vercel/ai/blob/v5/examples/ai-core/README.md#_snippet_0

LANGUAGE: sh
CODE:
```
OPENAI_API_KEY=""YOUR_OPENAI_API_KEY""
...
```

----------------------------------------

TITLE: Generate Text with OpenAI Provider and AI SDK
DESCRIPTION: An example demonstrating how to use the OpenAI provider with the AI SDK's 'generateText' function to create a text-based response from an OpenAI model.
SOURCE: https://github.com/vercel/ai/blob/main/packages/openai/README.md#_snippet_2

LANGUAGE: ts
CODE:
```
import { openai } from '@ai-sdk/openai';
import { generateText } from 'ai';

const { text } = await generateText({
  model: openai('gpt-4-turbo'),
  prompt: 'Write a vegetarian lasagna recipe for 4 people.',
});
```

----------------------------------------

TITLE: Integrating Tools with AI SDK for Slack Bot
DESCRIPTION: This TypeScript code defines the `generateResponse` function for a Slack bot using the AI SDK. It integrates two tools: `getWeather` to fetch current weather data from Open-Meteo API based on latitude, longitude, and city, and `searchWeb` to search the web using the Exa API for information, optionally restricting to a specific domain. The function processes user messages, invokes tools as needed, and formats the final response for Slack.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/00-guides/03-slackbot.mdx#_snippet_7

LANGUAGE: typescript
CODE:
```
import { openai } from '@ai-sdk/openai';
import { generateText, tool } from 'ai';
import type { ModelMessage } from 'ai';
import { z } from 'zod';
import { exa } from './utils';

export const generateResponse = async (
  messages: ModelMessage[],
  updateStatus?: (status: string) => void,
) => {
  const { text } = await generateText({
    model: openai('gpt-4o'),
    system: `You are a Slack bot assistant. Keep your responses concise and to the point.
    - Do not tag users.
    - Current date is: ${new Date().toISOString().split('T')[0]}
    - Always include sources in your final response if you use web search.`,
    messages,
    maxSteps: 10,
    tools: {
      getWeather: tool({
        description: 'Get the current weather at a location',
        parameters: z.object({
          latitude: z.number(),
          longitude: z.number(),
          city: z.string(),
        }),
        execute: async ({ latitude, longitude, city }) => {
          updateStatus?.(`is getting weather for ${city}...`);

          const response = await fetch(
            `https://api.open-meteo.com/v1/forecast?latitude=${latitude}&longitude=${longitude}&current=temperature_2m,weathercode,relativehumidity_2m&timezone=auto`,
          );

          const weatherData = await response.json();
          return {
            temperature: weatherData.current.temperature_2m,
            weatherCode: weatherData.current.weathercode,
            humidity: weatherData.current.relativehumidity_2m,
            city,
          };
        },
      }),
      searchWeb: tool({
        description: 'Use this to search the web for information',
        parameters: z.object({
          query: z.string(),
          specificDomain: z
            .string()
            .nullable()
            .describe(
              'a domain to search if the user specifies e.g. bbc.com. Should be only the domain name without the protocol',
            ),
        }),
        execute: async ({ query, specificDomain }) => {
          updateStatus?.(`is searching the web for ${query}...`);
          const { results } = await exa.searchAndContents(query, {
            livecrawl: 'always',
            numResults: 3,
            includeDomains: specificDomain ? [specificDomain] : undefined,
          });

          return {
            results: results.map(result => ({
              title: result.title,
              url: result.url,
              snippet: result.text.slice(0, 1000),
            })),
          };
        },
      }),
    },
  });

  // Convert markdown to Slack mrkdwn format
  return text.replace(/\[(.*?)\]\((.*?)\)/g, '<$2|$1>').replace(/\*\*/g, '*');
};
```

----------------------------------------

TITLE: API Methods for @ai-sdk/langchain Adapter
DESCRIPTION: Provides a comprehensive reference for the core methods available in the @ai-sdk/langchain module, detailing their signatures, parameters, and return types for various stream conversion scenarios.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/04-stream-helpers/16-langchain-adapter.mdx#_snippet_1

LANGUAGE: APIDOC
CODE:
```
Methods:
  toDataStream(stream: ReadableStream<LangChainAIMessageChunk> | ReadableStream<string>, AIStreamCallbacksAndOptions): AIStream
    Description: Converts LangChain output streams to data stream.
  toDataStreamResponse(stream: ReadableStream<LangChainAIMessageChunk> | ReadableStream<string>, options?: {init?: ResponseInit, data?: StreamData, callbacks?: AIStreamCallbacksAndOptions}): Response
    Description: Converts LangChain output streams to data stream response.
  mergeIntoDataStream(stream: ReadableStream<LangChainStreamEvent> | ReadableStream<LangChainAIMessageChunk> | ReadableStream<string>, options: { dataStream: DataStreamWriter; callbacks?: StreamCallbacks }): void
    Description: Merges LangChain output streams into an existing data stream.
```

----------------------------------------

TITLE: API Signature for extractReasoningMiddleware
DESCRIPTION: Comprehensive documentation of the `extractReasoningMiddleware` function's API. It details the configurable parameters, their types, descriptions, and default values, along with the structure and behavior of the returned middleware object and relevant type parameters.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/07-reference/01-ai-sdk-core/66-extract-reasoning-middleware.mdx#_snippet_1

LANGUAGE: APIDOC
CODE:
```
extractReasoningMiddleware(options: object): middleware object
  options:
    tagName: string (required)
      description: The name of the XML tag to extract reasoning from (without angle brackets)
    separator: string (optional, default: ""\n"")
      description: The separator to use between reasoning and text sections.
    startWithReasoning: boolean (optional, default: false)
      description: Starts with reasoning tokens. Set to true when the response always starts with reasoning and the initial tag is omitted.

  Returns:
    middleware object that:
      - Processes both streaming and non-streaming responses
      - Extracts content between specified XML tags as reasoning
      - Removes the XML tags and reasoning from the main text
      - Adds a `reasoning` property to the result containing the extracted content
      - Maintains proper separation between text sections using the specified separator

  Type Parameters:
    LanguageModelV2StreamPart
      description: The middleware works with the `LanguageModelV2StreamPart` type for streaming responses.
```

----------------------------------------

TITLE: API Reference: Core Parameters and Message Types
DESCRIPTION: Detailed documentation for the parameters used in the Vercel AI SDK, including message structures, tool calls, and function definitions.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/07-reference/03-ai-sdk-rsc/20-render.mdx#_snippet_1

LANGUAGE: APIDOC
CODE:
```
Parameters:\n  model (string): Model identifier, must be OpenAI SDK compatible.\n  provider (provider client): Currently the only provider available is OpenAI. This needs to match the model name.\n  initial (ReactNode, optional): The initial UI to render.\n  messages (Array<SystemMessage | UserMessage | AssistantMessage | ToolMessage>): A list of messages that represent a conversation.\n    SystemMessage:\n      role ('system'): The role for the system message.\n      content (string): The content of the message.\n    UserMessage:\n      role ('user'): The role for the user message.\n      content (string): The content of the message.\n    AssistantMessage:\n      role ('assistant'): The role for the assistant message.\n      content (string): The content of the message.\n      tool_calls (ToolCall[]): A list of tool calls made by the model.\n        ToolCall:\n          id (string): The id of the tool call.\n          type ('function'): The type of the tool call.\n          function (Function): The function to call.\n            Function:\n              name (string): The name of the function.\n              arguments (string): The arguments of the function.\n    ToolMessage:\n      role ('tool'): The role for the tool message.\n      content (string): The content of the message.\n      toolCallId (string): The id of the tool call.\n  functions (ToolSet, optional): Tools that are accessible to and can be called by the model.\n    Tool:\n      description (string, optional): Information about the purpose of the tool including details on how and when it can be used by the model.\n      parameters (zod schema): The typed schema that describes the parameters of the tool that can also be used to validation and error handling.\n      render (async (parameters) => any, optional): 
```

----------------------------------------

TITLE: Import and Create Requesty Provider Instances
DESCRIPTION: Demonstrates how to import the default `requesty` provider instance or create a custom instance using `createRequesty`. Custom instances allow explicit API key configuration or other advanced settings.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/5-requesty.mdx#_snippet_2

LANGUAGE: typescript
CODE:
```
import { requesty } from '@requesty/ai-sdk';
```

LANGUAGE: typescript
CODE:
```
import { createRequesty } from '@requesty/ai-sdk';

const customRequesty = createRequesty({
  apiKey: 'YOUR_REQUESTY_API_KEY',
});
```

----------------------------------------

TITLE: Next.js API Route for Streaming AI Chat Responses (TypeScript)
DESCRIPTION: This TypeScript route handler (`app/api/chat/route.ts`) defines a POST endpoint to manage chat interactions. It receives user messages, converts them for the model, and streams text responses from an Anthropic Claude 3.7 Sonnet model. The handler also includes `providerOptions` for advanced features like 'thinking' budget and can send reasoning tokens back to the client.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/00-guides/20-sonnet-3-7.mdx#_snippet_4

LANGUAGE: TypeScript
CODE:
```
import { anthropic, AnthropicProviderOptions } from '@ai-sdk/anthropic';
import { streamText, UIMessage, convertToModelMessages } from 'ai';

export async function POST(req: Request) {
  const { messages }: { messages: UIMessage[] } = await req.json();

  const result = streamText({
    model: anthropic('claude-3-7-sonnet-20250219'),
    messages: convertToModelMessages(messages),
    providerOptions: {
      anthropic: {
        thinking: { type: 'enabled', budgetTokens: 12000 },
      } satisfies AnthropicProviderOptions,
    },
  });

  return result.toUIMessageStreamResponse({
    sendReasoning: true,
  });
}
```

----------------------------------------

TITLE: Next.js Frontend Chat UI with AI SDK useChat Hook (TypeScript)
DESCRIPTION: This React component (`app/page.tsx`) demonstrates how to build a chat user interface in a Next.js application using the `useChat` hook from `@ai-sdk/react`. It handles message display, user input, and form submission, automatically interacting with the backend API route. The component also shows how to render different message parts, including text and model reasoning.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/00-guides/20-sonnet-3-7.mdx#_snippet_5

LANGUAGE: TypeScript
CODE:
```
'use client';

import { useChat } from '@ai-sdk/react';

export default function Page() {
  const { messages, input, handleInputChange, handleSubmit, error } = useChat();

  return (
    <>
      {messages.map(message => (
        <div key={message.id}>
          {message.role === 'user' ? 'User: ' : 'AI: '}
          {message.parts.map((part, index) => {
            // text parts:
            if (part.type === 'text') {
              return <div key={index}>{part.text}</div>;
            }
            // reasoning parts:
            if (part.type === 'reasoning') {
              return (
                <pre key={index}>
                  {part.details.map(detail =>
                    detail.type === 'text' ? detail.text : '<redacted>',
                  )}
                </pre>
              );
            }
          })}
        </div>
      ))}
      <form onSubmit={handleSubmit}>
        <input name=""prompt"" value={input} onChange={handleInputChange} />
        <button type=""submit"">Submit</button>
      </form>
    </>
  );
}
```

----------------------------------------

TITLE: Vercel AI SDK Message Types API Reference
DESCRIPTION: Detailed API documentation for various message types used in the Vercel AI SDK, including their roles, content structures, and part definitions like text, file, reasoning, and tool calls. This covers the structure of messages exchanged with AI models.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/03-generate-object.mdx#_snippet_6

LANGUAGE: APIDOC
CODE:
```
AssistantModelMessage:
  role: 'assistant' (string) - The role for the assistant message.
  content: string | Array<TextPart | FilePart | ReasoningPart | ToolCallPart> (array) - The content of the message.
    TextPart:
      type: 'text' (string) - The type of the message part.
      text: string (string) - The text content of the message part.
    ReasoningPart:
      type: 'reasoning' (string) - The type of the message part.
      text: string (string) - The reasoning text.
    FilePart:
      type: 'file' (string) - The type of the message part.
      data: string | Uint8Array | Buffer | ArrayBuffer | URL (string) - The file content of the message part. String are either base64 encoded content, base64 data URLs, or http(s) URLs.
      mediaType: string (string) - The IANA media type of the file.
      filename: string (string, optional) - The name of the file.
    ToolCallPart:
      type: 'tool-call' (string) - The type of the message part.
      toolCallId: string (string) - The id of the tool call.
      toolName: string (string) - The name of the tool, which typically would be the name of the function.
      args: object based on zod schema (object) - Parameters generated by the model to be used by the tool.

ToolModelMessage:
  role: 'tool' (string) - The role for the assistant message.
  content: Array<ToolResultPart> (array) - The content of the message.
    ToolResultPart:
      type: 'tool-result' (string) - The type of the message part.
```

----------------------------------------

TITLE: OpenAI Compatible Provider Configuration Options
DESCRIPTION: Details the optional settings available for customizing the `createOpenAICompatible` function, including `baseURL`, `apiKey`, `headers`, `queryParams`, and `fetch`.
SOURCE: https://github.com/vercel/ai/blob/v5/content/providers/02-openai-compatible-providers/index.mdx#_snippet_2

LANGUAGE: APIDOC
CODE:
```
createOpenAICompatible(options: object): Provider
  options:
    baseURL: string
      Set the URL prefix for API calls.
    apiKey: string
      API key for authenticating requests. If specified, adds an `Authorization` header to request headers with the value `Bearer <apiKey>`. This will be added before any headers potentially specified in the `headers` option.
    headers: Record<string,string>
      Optional custom headers to include in requests. These will be added to request headers after any headers potentially added by use of the `apiKey` option.
    queryParams: Record<string,string>
      Optional custom url query parameters to include in request urls.
    fetch: (input: RequestInfo, init?: RequestInit) => Promise<Response>
      Custom [fetch](https://developer.mozilla.org/en-US/docs/Web/API/fetch) implementation. Defaults to the global `fetch` function. You can use it as a middleware to intercept requests, or to provide a custom fetch implementation for e.g. testing.
```

----------------------------------------

TITLE: Next.js API Route for AI Chat with Caching Middleware
DESCRIPTION: This Next.js API route (`POST /api/chat`) demonstrates how to integrate the previously defined `cacheMiddleware` with an AI model. It wraps an `openai` model with `wrapLanguageModel` to enable caching. The route handles incoming chat messages, streams text responses, and includes an example `tool` definition for fetching weather information, showcasing how to use tools with `streamText`.
SOURCE: https://github.com/vercel/ai/blob/v5/content/cookbook/01-next/122-caching-middleware.mdx#_snippet_2

LANGUAGE: tsx
CODE:
```
import { cacheMiddleware } from '@/ai/middleware';
import { openai } from '@ai-sdk/openai';
import { wrapLanguageModel, streamText, tool } from 'ai';
import { z } from 'zod';

const wrappedModel = wrapLanguageModel({
  model: openai('gpt-4o-mini'),
  middleware: cacheMiddleware,
});

export async function POST(req: Request) {
  const { messages } = await req.json();

  const result = streamText({
    model: wrappedModel,
    messages,
    tools: {
      weather: tool({
        description: 'Get the weather in a location',
        parameters: z.object({
          location: z.string().describe('The location to get the weather for'),
        }),
        execute: async ({ location }) => ({
          location,
          temperature: 72 + Math.floor(Math.random() * 21) - 10,
        }),
      }),
    },
  });
  return result.toUIMessageStreamResponse();
}
```

----------------------------------------

TITLE: Utilize OpenAI Web Search Tool (TypeScript)
DESCRIPTION: Demonstrates how to integrate and force the use of the `web_search_preview` tool with an OpenAI responses model. It shows configuring search context size and user location, and accessing the resulting URL sources.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/03-openai.mdx#_snippet_29

LANGUAGE: TypeScript
CODE:
```
const result = await generateText({
  model: openai.responses('gpt-4o-mini'),
  prompt: 'What happened in San Francisco last week?',
  tools: {
    web_search_preview: openai.tools.webSearchPreview({
      // optional configuration:
      searchContextSize: 'high',
      userLocation: {
        type: 'approximate',
        city: 'San Francisco',
        region: 'California',
      },
    }),
  },
  // Force web search tool:
  toolChoice: { type: 'tool', toolName: 'web_search_preview' },
});

// URL sources
const sources = result.sources;
```

----------------------------------------

TITLE: API Endpoint for Resuming AI Chat Streams
DESCRIPTION: This describes the required GET endpoint for resuming AI chat streams. The `experimental_resume` function on the client makes a GET request to this endpoint, automatically appending the `chatId` query parameter. The backend should implement a GET handler for `/api/chat` that uses this `chatId` to look up the most recent stream ID from its database and resume the stream.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/04-ai-sdk-ui/03-chatbot-message-persistence.mdx#_snippet_11

LANGUAGE: APIDOC
CODE:
```
GET /api/chat?chatId=<your-chat-id>
```

----------------------------------------

TITLE: Implement Multi-Step Text Streaming on Server with AI SDK
DESCRIPTION: This TypeScript code demonstrates how to create a multi-step text streaming API endpoint using `@ai-sdk/openai` and `ai` libraries. It showcases chaining `streamText` calls, forcing tool invocations, and controlling data stream events (`experimental_sendFinish`, `experimental_sendStart`) to build complex conversational workflows.
SOURCE: https://github.com/vercel/ai/blob/v5/content/cookbook/01-next/24-stream-text-multistep.mdx#_snippet_0

LANGUAGE: typescript
CODE:
```
import { openai } from '@ai-sdk/openai';
import { createDataStreamResponse, streamText, tool } from 'ai';
import { z } from 'zod';

export async function POST(req: Request) {
  const { messages } = await req.json();

  return createDataStreamResponse({
    execute: async dataStream => {
      // step 1 example: forced tool call
      const result1 = streamText({
        model: openai('gpt-4o-mini'),
        system: 'Extract the user goal from the conversation.',
        messages,
        toolChoice: 'required', // force the model to call a tool
        tools: {
          extractGoal: tool({
            parameters: z.object({ goal: z.string() }),
            execute: async ({ goal }) => goal, // no-op extract tool
          }),
        },
      });

      // forward the initial result to the client without the finish event:
      result1.mergeIntoDataStream(dataStream, {
        experimental_sendFinish: false, // omit the finish event
      });

      // note: you can use any programming construct here, e.g. if-else, loops, etc.
      // workflow programming is normal programming with this approach.

      // example: continue stream with forced tool call from previous step
      const result2 = streamText({
        // different system prompt, different model, no tools:
        model: openai('gpt-4o'),
        system:
          'You are a helpful assistant with a different system prompt. Repeat the extract user goal in your answer.',
        // continue the workflow stream with the messages from the previous step:
        messages: [
          ...convertToModelMessages(messages),
          ...(await result1.response).messages,
        ],
      });

      // forward the 2nd result to the client (incl. the finish event):
      result2.mergeIntoDataStream(dataStream, {
        experimental_sendStart: false, // omit the start event
      });
    },
  });
}
```

----------------------------------------

TITLE: Vercel AI SDK: generateObject Function Examples
DESCRIPTION: The `generateObject` function is used to generate structured data objects directly. This section provides links to various examples demonstrating its usage across different frameworks and environments.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/10-generating-structured-data.mdx#_snippet_13

LANGUAGE: APIDOC
CODE:
```
Examples for generateObject:
- Learn to generate objects in Node.js: /examples/node/generating-structured-data/generate-object
- Learn to generate objects in Next.js with Route Handlers (AI SDK UI): /examples/next-pages/basics/generating-object
- Learn to generate objects in Next.js with Server Actions (AI SDK RSC): /examples/next-app/basics/generating-object
```

----------------------------------------

TITLE: Generate Structured Object with OpenAI Responses API using Zod Schema
DESCRIPTION: This TypeScript example demonstrates enforcing structured outputs using `generateObject` with the OpenAI Responses API. It defines a Zod schema for a recipe object, ensuring the model's output conforms to the specified structure.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/03-openai.mdx#_snippet_33

LANGUAGE: TypeScript
CODE:
```
// Using generateObject
const result = await generateObject({
  model: openai.responses('gpt-4.1'),
  schema: z.object({
    recipe: z.object({
      name: z.string(),
      ingredients: z.array(
        z.object({
          name: z.string(),
          amount: z.string(),
        }),
      ),
      steps: z.array(z.string()),
    }),
  }),
  prompt: 'Generate a lasagna recipe.',
});
```

----------------------------------------

TITLE: API Reference for embed() Function
DESCRIPTION: Detailed API documentation for the `embed()` function, including its parameters and return types. This section outlines all configurable options for generating embeddings and the structure of the response.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/05-embed.mdx#_snippet_1

LANGUAGE: APIDOC
CODE:
```
function embed(params: object): object
  params:
    model: EmbeddingModel - The embedding model to use. Example: openai.textEmbeddingModel('text-embedding-3-small')
    value: VALUE - The value to embed. The type depends on the model.
    maxRetries?: number - Maximum number of retries. Set to 0 to disable retries. Default: 2.
    abortSignal?: AbortSignal - An optional abort signal that can be used to cancel the call.
    headers?: Record<string, string> - Additional HTTP headers to be sent with the request. Only applicable for HTTP-based providers.
    experimental_telemetry?: TelemetrySettings - Telemetry configuration. Experimental feature.
      TelemetrySettings:
        isEnabled?: boolean - Enable or disable telemetry. Disabled by default while experimental.
        recordInputs?: boolean - Enable or disable input recording. Enabled by default.
        recordOutputs?: boolean - Enable or disable output recording. Enabled by default.
        functionId?: string - Identifier for this function. Used to group telemetry data by function.
        metadata?: Record<string, string | number | boolean | Array<null | undefined | string> | Array<null | undefined | number> | Array<null | undefined | boolean>> - Additional information to include in the telemetry data.
        tracer?: Tracer - A custom tracer to use for the telemetry data.
  returns:
    value: VALUE - The value that was embedded.
    embedding: number[] - The embedding of the value.
    usage: EmbeddingModelUsage - The token usage for generating the embeddings.
      EmbeddingModelUsage:
        tokens: number - The number of tokens used in the embedding.
    response?: Response - Optional response data.
      Response:
        headers?: Record<string, string> - Response headers.
        body?: unknown - The response body.
```

----------------------------------------

TITLE: createDataStreamResponse API Parameters
DESCRIPTION: Detailed documentation of the parameters accepted by the `createDataStreamResponse` function. This includes `status`, `statusText`, `headers`, `execute` (which provides a `DataStreamWriter` instance with methods like `write`, `writeData`, `writeMessageAnnotation`, `writeSource`, `merge`, and `onError`), and a top-level `onError` handler.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/07-reference/02-ai-sdk-ui/41-create-data-stream-response.mdx#_snippet_2

LANGUAGE: APIDOC
CODE:
```
createDataStreamResponse(options: object)
  options:
    status?: number
      description: The status code for the response.
    statusText?: string
      description: The status text for the response.
    headers?: Headers | Record<string, string>
      description: Additional headers for the response.
    execute: (dataStream: DataStreamWriter) => Promise<void> | void
      description: A function that receives a DataStreamWriter instance and can use it to write data to the stream.
      DataStreamWriter:
        write(data: DataStreamText): void
          description: Appends a data part to the stream.
        writeData(value: JSONValue): void
          description: Appends a data part to the stream.
        writeMessageAnnotation(value: JSONValue): void
          description: Appends a message annotation to the stream.
        writeSource(source: Source): void
          description: Appends a source part to the stream.
        merge(stream: ReadableStream<DataStreamText>): void
          description: Merges the contents of another stream to this stream.
        onError?: ((error: unknown) => string) | undefined
          description: Error handler that is used by the data stream writer. This is intended for forwarding when merging streams to prevent duplicated error masking.
    onError?: (error: unknown) => string
      description: A function that handles errors and returns an error message string. By default, it returns ""An error occurred.""
```

----------------------------------------

TITLE: Generate Text using Portkey Chat Model
DESCRIPTION: An example demonstrating how to use the `generateText` function from the AI SDK with a Portkey chat model to get a single text response based on a given prompt.
SOURCE: https://github.com/vercel/ai/blob/v5/content/providers/03-community-providers/10-portkey.mdx#_snippet_3

LANGUAGE: javascript
CODE:
```
import { createPortkey } from '@portkey-ai/vercel-provider';
import { generateText } from 'ai';

const portkey = createPortkey({
  apiKey: 'YOUR_PORTKEY_API_KEY',
  config: portkeyConfig,
});

const { text } = await generateText({
  model: portkey.chatModel(''),
  prompt: 'What is Portkey?',
});

console.log(text);
```

----------------------------------------

TITLE: Install AI SDK and DeepSeek Provider
DESCRIPTION: Command to install the necessary AI SDK packages, including the core SDK, DeepSeek provider, and React hooks for UI integration, using pnpm.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/00-guides/25-r1.mdx#_snippet_3

LANGUAGE: shell
CODE:
```
pnpm install ai@beta @ai-sdk/deepseek@beta @ai-sdk/react@beta
```

----------------------------------------

TITLE: Example: Using MCPClient to Generate Text with AI Tools
DESCRIPTION: Demonstrates how to initialize an `MCPClient` with a `stdio` transport, retrieve available tools from the client, and then use these tools in conjunction with an AI model (e.g., `gpt-4o-mini`) to generate text based on a user query. The example includes a `try...finally` block to ensure the client connection is properly closed, even if errors occur during the process. This snippet illustrates the typical workflow for integrating MCP tools into an AI application.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/07-reference/01-ai-sdk-core/21-create-mcp-client.mdx#_snippet_3

LANGUAGE: typescript
CODE:
```
import { experimental_createMCPClient, generateText } from 'ai';
import { openai } from '@ai-sdk/openai';

try {
  const client = await experimental_createMCPClient({
    transport: {
      type: 'stdio',
      command: 'node server.js',
    },
  });

  const tools = await client.tools();

  const response = await generateText({
    model: openai('gpt-4o-mini'),
    tools,
    messages: [{ role: 'user', content: 'Query the data' }],
  });

  console.log(response);
} finally {
  await client.close();
}
```

----------------------------------------

TITLE: Server-Side API Route for Multi-Step Tool Execution
DESCRIPTION: This Next.js API route (`/api/chat`) implements the server-side logic for AI tool calling. It uses `streamText` from the `ai` module and defines two tools, `getLocation` and `getWeather`, with Zod schemas for input validation. The `execute` functions simulate fetching data, demonstrating how the AI can orchestrate multiple tool calls in sequence.
SOURCE: https://github.com/vercel/ai/blob/v5/content/cookbook/01-next/72-call-tools-multiple-steps.mdx#_snippet_1

LANGUAGE: ts
CODE:
```
import { ToolInvocation, streamText } from 'ai';
import { openai } from '@ai-sdk/openai';
import { z } from 'zod';

interface Message {
  role: 'user' | 'assistant';
  content: string;
  toolInvocations?: ToolInvocation[];
}

function getLocation({ lat, lon }) {
  return { lat: 37.7749, lon: -122.4194 };
}

function getWeather({ lat, lon, unit }) {
  return { value: 25, description: 'Sunny' };
}

export async function POST(req: Request) {
  const { messages }: { messages: Message[] } = await req.json();

  const result = streamText({
    model: openai('gpt-4o'),
    system: 'You are a helpful assistant.',
    messages,
    tools: {
      getLocation: {
        description: 'Get the location of the user',
        parameters: z.object({}),
        execute: async () => {
          const { lat, lon } = getLocation();
          return `Your location is at latitude ${lat} and longitude ${lon}`;
        },
      },
      getWeather: {
        description: 'Get the weather for a location',
        parameters: z.object({
          lat: z.number().describe('The latitude of the location'),
          lon: z.number().describe('The longitude of the location'),
          unit: z
            .enum(['C', 'F'])
            .describe('The unit to display the temperature in'),
        }),
        execute: async ({ lat, lon, unit }) => {
          const { value, description } = getWeather({ lat, lon, unit });
          return `It is currently ${value}°${unit} and ${description}!`;
        },
      },
    },
  });

  return result.toUIMessageStreamResponse();
}
```

----------------------------------------

TITLE: Language Model V2 Stream Sequence Example
DESCRIPTION: Illustrates a typical sequence of events in the Language Model V2 streaming system, showing the progression from stream start, content delivery, tool calls, response metadata, to the final finish event.
SOURCE: https://github.com/vercel/ai/blob/v5/content/providers/03-community-providers/01-custom-providers.mdx#_snippet_13

LANGUAGE: typescript
CODE:
```
{
  type: 'stream-start',
  warnings: []
}
{
  type: 'text',
  text: 'Hello'
}
{
  type: 'text',
  text: ' world'
}
{
  type: 'tool-call',
  toolCallId: '1',
  toolName: 'search',
  args: {...}
}
{
  type: 'response-metadata',
  modelId: 'gpt-4.1',
  ...
}
{
  type: 'finish',
  usage: {
    inputTokens: 10,
    outputTokens: 20
  },
  finishReason: 'stop'
}
```

----------------------------------------

TITLE: `generateImage` API Parameters
DESCRIPTION: Detailed documentation of the parameters accepted by the `generateImage` function, including type, description, and optionality.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/07-reference/01-ai-sdk-core/10-generate-image.mdx#_snippet_2

LANGUAGE: APIDOC
CODE:
```
generateImage(params: object): Promise<{ images: string[] }>
  params:
    model: ImageModelV2 (required) - The image model to use.
    prompt: string (required) - The input prompt to generate the image from.
    n: number (optional) - Number of images to generate.
    size: string (optional) - Size of the images to generate. Format: {width}x{height}.
    aspectRatio: string (optional) - Aspect ratio of the images to generate. Format: {width}:{height}.
    seed: number (optional) - Seed for the image generation.
    providerOptions: ProviderOptions (optional) - Additional provider-specific options.
    maxRetries: number (optional) - Maximum number of retries. Default: 2.
    abortSignal: AbortSignal (optional) - An optional abort signal to cancel the call.
    headers: Record<string, string> (optional) - Additional HTTP headers for the request.
```

----------------------------------------

TITLE: Expose OpenAI API key in Nuxt runtime config
DESCRIPTION: Modifies the nuxt.config.ts file to expose the OPENAI_API_KEY from environment variables to the application's runtime configuration. This makes the API key securely accessible within your Nuxt.js application, particularly on the server-side.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-getting-started/05-nuxt.mdx#_snippet_5

LANGUAGE: TypeScript
CODE:
```
export default defineNuxtConfig({
  // rest of your nuxt config
  runtimeConfig: {
    openaiApiKey: process.env.OPENAI_API_KEY,
  },
});
```

----------------------------------------

TITLE: Implement Tool Calling with AI SDK and GPT-4.5
DESCRIPTION: This snippet demonstrates how to enable tool calling with GPT-4.5 using the AI SDK. It defines a `getWeather` tool with a description and parameters, allowing the model to interact with external functions (simulated here) to retrieve dynamic information like weather data.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/00-guides/22-gpt-4-5.mdx#_snippet_2

LANGUAGE: typescript
CODE:
```
import { generateText, tool } from 'ai';
import { openai } from '@ai-sdk/openai';
import { z } from 'zod';

const { text } = await generateText({
  model: openai('gpt-4.5-preview'),
  prompt: 'What is the weather like today in San Francisco?',
  tools: {
    getWeather: tool({
      description: 'Get the weather in a location',
      parameters: z.object({
        location: z.string().describe('The location to get the weather for')
      }),
      execute: async ({ location }) => ({
        location,
        temperature: 72 + Math.floor(Math.random() * 21) - 10
      })
    })
  }
});
```

----------------------------------------

TITLE: Example Response Headers from generateText Bedrock API Call (JavaScript)
DESCRIPTION: Provides a sample JSON object representing the HTTP response headers returned by the Amazon Bedrock service when invoked via the `generateText` function. It highlights key headers like `connection`, `content-type`, and `x-amzn-requestid`.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/08-amazon-bedrock.mdx#_snippet_28

LANGUAGE: JavaScript
CODE:
```
{
  ""connection"": ""keep-alive"",
  ""content-length"": ""2399"",
  ""content-type"": ""application/json"",
  ""date"": ""Fri, 07 Feb 2025 04:28:30 GMT"",
  ""x-amzn-requestid"": ""c9f3ace4-dd5d-49e5-9807-39aedfa47c8e""
}
```

----------------------------------------

TITLE: Server-side `streamObject` with Array Output (Next.js API Route)
DESCRIPTION: This Next.js API route handles incoming requests to generate an array of notification objects using `streamObject` from the `ai` library. It specifies `output: 'array'` and uses the `notificationSchema` to guide the AI model in producing a structured array of objects.
SOURCE: https://github.com/vercel/ai/blob/v5/content/cookbook/01-next/40-stream-object.mdx#_snippet_6

LANGUAGE: typescript
CODE:
```
import { openai } from '@ai-sdk/openai';
import { streamObject } from 'ai';
import { notificationSchema } from './schema';

export const maxDuration = 30;

export async function POST(req: Request) {
  const context = await req.json();

  const result = streamObject({
    model: openai('gpt-4.1'),
    output: 'array',
    schema: notificationSchema,
    prompt:
      `Generate 3 notifications for a messages app in this context:` + context,
  });

  return result.toTextStreamResponse();
}
```

----------------------------------------

TITLE: Install AI SDK and OpenAI Provider for Next.js Chatbot
DESCRIPTION: This command provides the necessary `pnpm` installation steps for setting up a Next.js application with AI SDK. It includes the core `ai` package, the `@ai-sdk/openai` provider, and `@ai-sdk/react` for UI integration.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/00-guides/24-o3.mdx#_snippet_4

LANGUAGE: bash
CODE:
```
pnpm install ai@beta @ai-sdk/openai@beta @ai-sdk/react@beta
```

----------------------------------------

TITLE: API Reference for defaultSettingsMiddleware
DESCRIPTION: Detailed documentation for the `defaultSettingsMiddleware` function, including its parameters and the structure of the returned middleware object.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/07-reference/01-ai-sdk-core/68-default-settings-middleware.mdx#_snippet_2

LANGUAGE: APIDOC
CODE:
```
defaultSettingsMiddleware(config: object)
  Parameters:
    config: object
      settings: object
        description: An object containing default parameter values to apply to language model calls. These can include any valid LanguageModelV2CallOptions properties and optional provider metadata.
  Returns: MiddlewareObject
    description: A middleware object that:
      - Merges the default settings with the parameters provided in each model call
      - Ensures that explicitly provided parameters take precedence over defaults
      - Merges provider metadata objects
```

----------------------------------------

TITLE: Generate Text with OpenAI Compatible Provider
DESCRIPTION: An end-to-end example demonstrating how to initialize an OpenAI compatible provider and use it with `generateText` to produce text based on a prompt.
SOURCE: https://github.com/vercel/ai/blob/v5/content/providers/02-openai-compatible-providers/index.mdx#_snippet_4

LANGUAGE: typescript
CODE:
```
import { createOpenAICompatible } from '@ai-sdk/openai-compatible';
import { generateText } from 'ai';

const provider = createOpenAICompatible({
  name: 'provider-name',
  apiKey: process.env.PROVIDER_API_KEY,
  baseURL: 'https://api.provider.com/v1',
});

const { text } = await generateText({
  model: provider('model-id'),
  prompt: 'Write a vegetarian lasagna recipe for 4 people.',
});
```

----------------------------------------

TITLE: getMutableAIState API Reference
DESCRIPTION: Detailed API documentation for the `getMutableAIState` function, outlining its parameters, return type, and available methods for manipulating the AI state.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/07-reference/03-ai-sdk-rsc/07-get-mutable-ai-state.mdx#_snippet_1

LANGUAGE: APIDOC
CODE:
```
getMutableAIState:
  Parameters:
    key: string (optional) - Returns the value of the specified key in the AI state, if it's an object.
  Returns: The mutable AI state.
  Methods:
    update(newState: any): void - Updates the AI state with the new state.
    done(newState: any): void - Updates the AI state with the new state, marks it as finalized and closes the stream.
```

----------------------------------------

TITLE: Define AI Model Message Parts and Tool Interfaces
DESCRIPTION: This API documentation defines the various message parts and tool structures used for communication with AI models. It includes specifications for handling file content, tool call requests, tool execution results, and the overall definition of callable tools with their parameters.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/02-stream-text.mdx#_snippet_3

LANGUAGE: APIDOC
CODE:
```
FilePart:
  type: ""'file'""
  parameters:
    - name: 'type'
      type: ""'file'""
      description: 'The type of the message part.'
    - name: 'data'
      type: 'string | Uint8Array | Buffer | ArrayBuffer | URL'
      description: 'The file content of the message part. String are either base64 encoded content, base64 data URLs, or http(s) URLs.'
    - name: 'mediaType'
      type: 'string'
      description: 'The IANA media type of the file.'
    - name: 'filename'
      type: 'string'
      isOptional: true
      description: 'The name of the file.'

ToolCallPart:
  type: 'ToolCallPart'
  parameters:
    - name: 'type'
      type: ""'tool-call'""
      description: 'The type of the message part.'
    - name: 'toolCallId'
      type: 'string'
      description: 'The id of the tool call.'
    - name: 'toolName'
      type: 'string'
      description: 'The name of the tool, which typically would be the name of the function.'
    - name: 'args'
      type: 'object based on zod schema'
      description: 'Parameters generated by the model to be used by the tool.'

ToolModelMessage:
  type: 'ToolModelMessage'
  parameters:
    - name: 'role'
      type: ""'tool'""
      description: 'The role for the assistant message.'
    - name: 'content'
      type: 'Array<ToolResultPart>'
      description: 'The content of the message.'
      properties:
        - type: 'ToolResultPart'
          parameters:
            - name: 'type'
              type: ""'tool-result'""
              description: 'The type of the message part.'
            - name: 'toolCallId'
              type: 'string'
              description: 'The id of the tool call the result corresponds to.'
            - name: 'toolName'
              type: 'string'
              description: 'The name of the tool the result corresponds to.'
            - name: 'result'
              type: 'unknown'
              description: 'The result returned by the tool after execution.'
            - name: 'isError'
              type: 'boolean'
              isOptional: true
              description: 'Whether the result is an error or an error message.'

ToolSet:
  name: 'tools'
  type: 'ToolSet'
  description: 'Tools that are accessible to and can be called by the model. The model needs to support calling tools.'
  properties:
    - type: 'Tool'
      parameters:
        - name: 'description'
          isOptional: true
          type: 'string'
          description: 'Information about the purpose of the tool including details on how and when it can be used by the model.'
        - name: 'parameters'
          type: 'Zod Schema | JSON Schema'
          description: ''
```

----------------------------------------

TITLE: appendResponseMessages API Reference
DESCRIPTION: Detailed API documentation for the `appendResponseMessages` function, outlining its parameters, their types and descriptions, and the function's return type and purpose.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/07-reference/02-ai-sdk-ui/32-append-response-messages.mdx#_snippet_1

LANGUAGE: APIDOC
CODE:
```
appendResponseMessages(messages: Message[], responseMessages: ResponseMessage[]): Message[]
  messages: Message[]
    An existing array of UI messages for useChat (usually from state).
  responseMessages: ResponseMessage[]
    The new array of AI messages returned from the AI service to be appended. For example, ""assistant"" messages get added as new items, while tool-call results (role: ""tool"") are merged with the previous assistant message.

Returns: Message[]
  A new array of UI messages with the appended AI response messages (and updated tool-call results for the preceding assistant message).
```

----------------------------------------

TITLE: API Signature for pipeDataStreamToResponse
DESCRIPTION: Detailed API documentation for the `pipeDataStreamToResponse` function, outlining its parameters, their types, descriptions, and nested properties, including the methods available on the `DataStreamWriter` instance.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/07-reference/02-ai-sdk-ui/42-pipe-data-stream-to-response.mdx#_snippet_2

LANGUAGE: APIDOC
CODE:
```
pipeDataStreamToResponse(response: ServerResponse, options: object): void

Parameters for pipeDataStreamToResponse:
  response (ServerResponse): The Node.js ServerResponse object to pipe the data to.
  options (object): Configuration options for piping the data.
    status (number, optional): The status code for the response.
    statusText (string, optional): The status text for the response.
    headers (Headers | Record<string, string>, optional): Additional headers for the response.
    execute ((dataStream: DataStreamWriter) => Promise<void> | void): A function that receives a DataStreamWriter instance and can use it to write data to the stream.
      DataStreamWriter methods:
        writeData(value: JSONValue): void - Appends a data part to the stream.
        writeMessageAnnotation(value: JSONValue): void - Appends a message annotation to the stream.
        merge(stream: ReadableStream<DataStreamText>): void - Merges the contents of another stream to this stream.
        onError ((error: unknown) => string) | undefined: Error handler that is used by the data stream writer. This is intended for forwarding when merging streams to prevent duplicated error masking.
    onError ((error: unknown) => string): A function that handles errors and returns an error message string. By default, it returns ""An error occurred.""
```

----------------------------------------

TITLE: Configure Max Duration in vercel.json for Vercel Functions
DESCRIPTION: For frameworks other than Next.js App Router, this configuration in `vercel.json` allows setting a specific maximum duration (e.g., 30 seconds) for individual Vercel API routes. This ensures that long-running functions, like chat API endpoints, do not time out.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/09-troubleshooting/06-timeout-on-vercel.mdx#_snippet_1

LANGUAGE: json
CODE:
```
{
  ""functions"": {
    ""api/chat/route.ts"": {
      ""maxDuration"": 30
    }
  }
}
```

----------------------------------------

TITLE: createDataStream API Signature
DESCRIPTION: Detailed API documentation for the `createDataStream` function, outlining its parameters, their types, descriptions, and the structure of the `DataStreamWriter` instance. It also specifies the function's return type.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/07-reference/02-ai-sdk-ui/40-create-data-stream.mdx#_snippet_2

LANGUAGE: APIDOC
CODE:
```
createDataStream(
  execute: (dataStream: DataStreamWriter) => Promise<void> | void,
    description: A function that receives a DataStreamWriter instance and can use it to write data to the stream.
    DataStreamWriter properties:
      write(data: DataStreamText): void
        description: Appends a data part to the stream.
      writeData(value: JSONValue): void
        description: Appends a data part to the stream.
      writeMessageAnnotation(value: JSONValue): void
        description: Appends a message annotation to the stream.
      writeSource(source: Source): void
        description: Appends a source part to the stream.
      merge(stream: ReadableStream<DataStreamText>): void
        description: Merges the contents of another stream to this stream.
      onError((error: unknown) => string) | undefined
        description: Error handler that is used by the data stream writer. This is intended for forwarding when merging streams to prevent duplicated error masking.
  onError: (error: unknown) => string
    description: A function that handles errors and returns an error message string. By default, it returns ""An error occurred.""
)

Returns:
  ReadableStream<DataStreamText>
    description: A readable stream that emits formatted data stream parts.
```

----------------------------------------

TITLE: Install Dependencies and Build Project
DESCRIPTION: Executes pnpm commands to install all necessary project dependencies and then builds the application. These steps are prerequisites for running the Fastify server.
SOURCE: https://github.com/vercel/ai/blob/v5/examples/fastify/README.md#_snippet_1

LANGUAGE: sh
CODE:
```
pnpm install
pnpm build
```

----------------------------------------

TITLE: Vercel AI SDK Function Parameters API Reference
DESCRIPTION: Detailed API reference for the parameters used in Vercel AI SDK functions, including their types, descriptions, and nested structures for complex data types like message arrays and their content parts.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/04-stream-object.mdx#_snippet_6

LANGUAGE: APIDOC
CODE:
```
Parameter: model
  Type: LanguageModel
  Description: The language model to use. Example: openai('gpt-4.1')
Parameter: output
  Type: 'object' | 'array' | 'enum' | 'no-schema' | undefined
  Description: The type of output to generate. Defaults to 'object'.
Parameter: mode
  Type: 'auto' | 'json' | 'tool'
  Description: The mode to use for object generation. Not every model supports all modes. Defaults to 'auto' for 'object' output and to 'json' for 'no-schema' output. Must be 'json' for 'no-schema' output.
Parameter: schema
  Type: Zod Schema | JSON Schema
  Description: The schema that describes the shape of the object to generate. It is sent to the model to generate the object and used to validate the output. You can either pass in a Zod schema or a JSON schema (using the `jsonSchema` function). In 'array' mode, the schema is used to describe an array element. Not available with 'no-schema' or 'enum' output.
Parameter: schemaName
  Type: string | undefined
  Description: Optional name of the output that should be generated. Used by some providers for additional LLM guidance, e.g. via tool or schema name. Not available with 'no-schema' or 'enum' output.
Parameter: schemaDescription
  Type: string | undefined
  Description: Optional description of the output that should be generated. Used by some providers for additional LLM guidance, e.g. via tool or schema name. Not available with 'no-schema' or 'enum' output.
Parameter: system
  Type: string
  Description: The system prompt to use that specifies the behavior of the model.
Parameter: prompt
  Type: string
  Description: The input prompt to generate the text from.
Parameter: messages
  Type: Array<SystemModelMessage | UserModelMessage | AssistantModelMessage | ToolModelMessage> | Array<UIMessage>
  Description: A list of messages that represent a conversation. Automatically converts UI messages from the useChat hook.
  Nested Types:
    Type: SystemModelMessage
      Parameters:
        Parameter: role
          Type: 'system'
          Description: The role for the system message.
        Parameter: content
          Type: string
          Description: The content of the message.
    Type: UserModelMessage
      Parameters:
        Parameter: role
          Type: 'user'
          Description: The role for the user message.
        Parameter: content
          Type: string | Array<TextPart | ImagePart | FilePart>
          Description: The content of the message.
          Nested Types:
            Type: TextPart
              Parameters:
                Parameter: type
                  Type: 'text'
                  Description: The type of the message part.
                Parameter: text
                  Type: 'string'
                  Description: The text content of the message part.
            Type: ImagePart
              Parameters:
                Parameter: type
                  Type: 'image'
                  Description: The type of the message part.
                Parameter: image
                  Type: 'string | Uint8Array | Buffer | ArrayBuffer | URL'
                  Description: The image content of the message part. String are either base64 encoded content, base64 data URLs, or http(s) URLs.
                Parameter: mediaType
                  Type: 'string'
                  Description: The IANA media type of the image. Optional.
                  (Optional)
            Type: FilePart
              Parameters:
                Parameter: type
                  Type: 'file'
                  Description: The type of the message part.
                Parameter: data
                  Type: (truncated)
                  Description: (truncated)
```

----------------------------------------

TITLE: Example Output of Intercepted Fetch Request
DESCRIPTION: Shows the console output when intercepting fetch requests, detailing the URL, headers, and request body sent to the SambaNova API.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/96-sambanova.mdx#_snippet_8

LANGUAGE: Bash
CODE:
```
URL https://api.sambanova.ai/v1/chat/completions
Headers {
  ""Content-Type"": ""application/json"",
  ""Authorization"": ""Bearer YOUR_API_KEY""
}
Body {
  ""model"": ""Meta-Llama-3.1-70B-Instruct"",
  ""temperature"": 0,
  ""messages"": [
    {
      ""role"": ""user"",
      ""content"": ""Hello, nice to meet you.""
    }
  ]
}
```

----------------------------------------

TITLE: HuggingFaceStream API Signature Reference
DESCRIPTION: Comprehensive API documentation for the `HuggingFaceStream` function, detailing its parameters, their types, descriptions, and the expected return type. It also outlines the structure of the `AIStreamCallbacksAndOptions` object for custom stream handling.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/07-reference/04-stream-helpers/15-hugging-face-stream.mdx#_snippet_1

LANGUAGE: APIDOC
CODE:
```
HuggingFaceStream(
  iter: AsyncGenerator<any>,
  callbacks?: AIStreamCallbacksAndOptions
) => ReadableStream

Parameters:
  iter: AsyncGenerator<any>
    Description: This parameter should be the generator function returned by the hf.textGenerationStream method in the Hugging Face Inference SDK.

  callbacks: AIStreamCallbacksAndOptions (Optional)
    Description: An object containing callback functions to handle the start, each token, and completion of the AI response. In the absence of this parameter, default behavior is implemented.
    Properties (AIStreamCallbacksAndOptions):
      onStart: () => Promise<void>
        Description: An optional function that is called at the start of the stream processing.
      onCompletion: (completion: string) => Promise<void>
        Description: An optional function that is called for every completion. It's passed the completion as a string.
      onFinal: (completion: string) => Promise<void>
        Description: An optional function that is called once when the stream is closed with the final completion message.
      onToken: (token: string) => Promise<void>
        Description: An optional function that is called for each token in the stream. It's passed the token as a string.

Returns:
  ReadableStream
```

----------------------------------------

TITLE: createAI Function API Signature
DESCRIPTION: This section details the API signature of the `createAI` function, including its parameters and return value. It describes how to configure the client-server context provider for managing UI and AI states within an application.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/07-reference/03-ai-sdk-rsc/02-create-ai.mdx#_snippet_1

LANGUAGE: APIDOC
CODE:
```
createAI(parameters): <AI/> context provider
  Parameters:
    actions: Record<string, Action>
      Description: Server side actions that can be called from the client.
    initialAIState: any
      Description: Initial AI state to be used in the client.
    initialUIState: any
      Description: Initial UI state to be used in the client.
    onGetUIState: () => UIState
      Description: is called during SSR to compare and update UI state.
    onSetAIState: (Event) => void
      Description: is triggered whenever an update() or done() is called by the mutable AI state in your action, so you can safely store your AI state in the database.
      Event properties:
        state: AIState
          Description: The resulting AI state after the update.
        done: boolean
          Description: Whether the AI state updates have been finalized or not.
```

----------------------------------------

TITLE: Install Project Dependencies with pnpm
DESCRIPTION: Installs all required project dependencies using pnpm. This command should be run from the root directory of the AI SDK repository to ensure all necessary packages are available.
SOURCE: https://github.com/vercel/ai/blob/v5/examples/node-http-server/README.md#_snippet_1

LANGUAGE: sh
CODE:
```
pnpm install
```

----------------------------------------

TITLE: Create AI Chat API Route with Vercel AI SDK
DESCRIPTION: This TypeScript code defines an API route (`server/api/chat.ts`) for a chat application. It initializes an OpenAI provider using `@ai-sdk/openai`, retrieves the API key from runtime configuration, and streams text from the `gpt-4o` model. The route converts `UIMessage` arrays to `ModelMessage` arrays for the model's context and streams the LLM response back to the client.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-getting-started/05-nuxt.mdx#_snippet_6

LANGUAGE: typescript
CODE:
```
import { streamText, UIMessage, convertToModelMessages } from 'ai';
import { createOpenAI } from '@ai-sdk/openai';

export default defineLazyEventHandler(async () => {
  const apiKey = useRuntimeConfig().openaiApiKey;
  if (!apiKey) throw new Error('Missing OpenAI API key');
  const openai = createOpenAI({
    apiKey: apiKey,
  });

  return defineEventHandler(async (event: any) => {
    const { messages }: { messages: UIMessage[] } = await readBody(event);

    const result = streamText({
      model: openai('gpt-4o'),
      messages: convertToModelMessages(messages),
    });

    return result.toUIMessageStreamResponse();
  });
});
```

----------------------------------------

TITLE: Example AI Provider Package Dependencies
DESCRIPTION: Lists the core dependencies required for the example AI provider, including `@ai-sdk/openai-compatible`, `@ai-sdk/provider`, and `@ai-sdk/provider-utils`. These packages provide the foundational components for building AI model integrations and ensuring compatibility.
SOURCE: https://github.com/vercel/ai/blob/v5/content/providers/02-openai-compatible-providers/01-custom-providers.mdx#_snippet_6

LANGUAGE: js
CODE:
```
{
  ""name"": ""@company-name/example"",
  ""version"": ""0.0.1"",
  ""dependencies"": {
    ""@ai-sdk/openai-compatible"": ""^0.0.7"",
    ""@ai-sdk/provider"": ""^1.0.2"",
    ""@ai-sdk/provider-utils"": ""^2.0.4""
  }
}
```

----------------------------------------

TITLE: Generate Text with Llama 3.1 using DeepInfra and AI SDK
DESCRIPTION: This snippet demonstrates how to use the AI SDK to generate text with the Llama 3.1 model via the DeepInfra provider. It imports necessary modules, initializes the DeepInfra model, and calls `generateText` with a prompt to get a text response.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/00-guides/21-llama-3_1.mdx#_snippet_0

LANGUAGE: TypeScript
CODE:
```
import { deepinfra } from '@ai-sdk/deepinfra';
import { generateText } from 'ai';

const { text } = await generateText({
  model: deepinfra('meta-llama/Meta-Llama-3.1-405B-Instruct'),
  prompt: 'What is love?',
});
```

----------------------------------------

TITLE: Transcribe Audio with Gladia Provider
DESCRIPTION: Demonstrates an end-to-end example of using the AI SDK with the Gladia provider to transcribe an audio file. It imports the `gladia` model and the `experimental_transcribe` function, then calls the function with an audio URL to get the transcribed text.
SOURCE: https://github.com/vercel/ai/blob/main/packages/gladia/README.md#_snippet_2

LANGUAGE: ts
CODE:
```
import { gladia } from '@ai-sdk/gladia';
import { experimental_transcribe as transcribe } from 'ai';

const { text } = await transcribe({
  model: gladia.transcription(),
  audio: new URL(
    'https://github.com/vercel/ai/raw/refs/heads/main/examples/ai-core/data/galileo.mp3',
  ),
});
```

----------------------------------------

TITLE: Handle New Assistant Threads and Set Suggested Prompts
DESCRIPTION: This function is triggered when a new assistant thread is started. It posts a welcoming message to the new thread, informing the user about the bot's origin. Additionally, it configures and sets suggested prompts within the Slack interface to guide users on how to interact with the assistant.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/00-guides/03-slackbot.mdx#_snippet_4

LANGUAGE: TypeScript
CODE:
```
import type { AssistantThreadStartedEvent } from '@slack/web-api';
import { client } from './slack-utils';

export async function assistantThreadMessage(
  event: AssistantThreadStartedEvent,
) {
  const { channel_id, thread_ts } = event.assistant_thread;
  console.log(`Thread started: ${channel_id} ${thread_ts}`);
  console.log(JSON.stringify(event));

  await client.chat.postMessage({
    channel: channel_id,
    thread_ts: thread_ts,
    text: ""Hello, I'm an AI assistant built with the AI SDK by Vercel!"",
  });

  await client.assistant.threads.setSuggestedPrompts({
    channel_id: channel_id,
    thread_ts: thread_ts,
    prompts: [
      {
        title: 'Get the weather',
        message: 'What is the current weather in London?',
      },
      {
        title: 'Get the news',
        message: 'What is the latest Premier League news from the BBC?',
      },
    ],
  });
}
```

----------------------------------------

TITLE: Initialize Mem0 Client
DESCRIPTION: Demonstrates how to create a Mem0 client instance using `createMem0`, configuring API keys and optional global settings for enhanced AI interaction.
SOURCE: https://github.com/vercel/ai/blob/v5/content/providers/03-community-providers/70-mem0.mdx#_snippet_1

LANGUAGE: typescript
CODE:
```
import { createMem0 } from '@mem0/vercel-ai-provider';

const mem0 = createMem0({
  provider: 'openai',
  mem0ApiKey: 'm0-xxx',
  apiKey: 'provider-api-key',
  config: {
    compatibility: 'strict',
  },
  // Optional Mem0 Global Config
  mem0Config: {
    user_id: 'mem0-user-id',
    org_id: 'mem0-org-id',
    project_id: 'mem0-project-id',
  },
});
```

----------------------------------------

TITLE: Generate Text with Chrome AI
DESCRIPTION: An example demonstrating how to use the `generateText` function from the AI SDK with the Chrome AI provider to get a single text response from the model.
SOURCE: https://github.com/vercel/ai/blob/v5/content/providers/03-community-providers/04-chrome-ai.mdx#_snippet_4

LANGUAGE: javascript
CODE:
```
import { generateText } from 'ai';
import { chromeai } from 'chrome-ai';

const { text } = await generateText({
  model: chromeai(),
  prompt: 'Who are you?',
});

console.log(text); //  I am a large language model, trained by Google.
```

----------------------------------------

TITLE: Instantiate OpenAI Language Model with Additional Settings
DESCRIPTION: Illustrates how to create an OpenAI language model instance while providing additional configuration options specific to the chosen API.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/03-openai.mdx#_snippet_5

LANGUAGE: TypeScript
CODE:
```
const model = openai('gpt-4.1', {
  // additional settings
});
```

----------------------------------------

TITLE: Generate Text Using OpenAI Language Model
DESCRIPTION: An example demonstrating the use of an OpenAI language model with the `generateText` function from the AI SDK to produce text based on a given prompt.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/03-openai.mdx#_snippet_6

LANGUAGE: TypeScript
CODE:
```
import { openai } from '@ai-sdk/openai';
import { generateText } from 'ai';

const { text } = await generateText({
  model: openai('gpt-4.1'),
  prompt: 'Write a vegetarian lasagna recipe for 4 people.',
});
```

----------------------------------------

TITLE: Tool Call Part Format and Example
DESCRIPTION: Defines the structure and provides an example of a complete tool call. When streamed, this part is sent after the streaming of the tool call is finished.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/04-ai-sdk-ui/50-stream-protocol.mdx#_snippet_6

LANGUAGE: APIDOC
CODE:
```
Format: 9:{toolCallId:string; toolName:string; args:object}
Example: 9:{""toolCallId"":""call-123"",""toolName"":""my-tool"",""args"":{""some"":""argument""}}
```

----------------------------------------

TITLE: Generate Text with Dify.ai Provider
DESCRIPTION: An example demonstrating how to use the Dify provider with `generateText` for Dify.ai. It includes setting up the provider with an application ID and API key, sending a user message, and logging the generated text along with conversation and message IDs from the provider metadata.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/97-dify.mdx#_snippet_2

LANGUAGE: typescript
CODE:
```
import { generateText } from 'ai';
import { difyProvider } from 'dify-ai-provider';

const dify = difyProvider('dify-application-id', {
  responseMode: 'blocking',
  apiKey: 'dify-api-key',
});

const { text, providerMetadata } = await generateText({
  model: dify,
  messages: [{ role: 'user', content: 'Hello, how are you today?' }],
  headers: { 'user-id': 'test-user' },
});

const { conversationId, messageId } = providerMetadata.difyWorkflowData;
console.log(text);
console.log('conversationId', conversationId);
console.log('messageId', messageId);
```

----------------------------------------

TITLE: AI SDK `StreamTextResult` Object Reference
DESCRIPTION: API documentation for the `StreamTextResult` object, which is returned by the `streamText` function. It provides methods to convert the streamed result into various response formats suitable for UI or data streams.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-getting-started/05-nuxt.mdx#_snippet_9

LANGUAGE: APIDOC
CODE:
```
StreamTextResult:
  toUIMessageStreamResponse(): Response (converts result to a streamed UI message response)
  toDataStreamResponse(): Response (converts result to a streamed data response)
```

----------------------------------------

TITLE: Attach Dynamic Message Metadata on Server-Side
DESCRIPTION: This server-side example shows how to dynamically attach metadata to messages using the `toUIMessageStreamResponse()` utility. It illustrates how to update metadata at different stages of the streaming process (start, finish-step, finish) to include information such as the model ID, response duration, and total token usage.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/01-announcing-ai-sdk-5-beta/index.mdx#_snippet_5

LANGUAGE: TypeScript
CODE:
```
import { openai } from '@ai-sdk/openai';
import { convertToModelMessages, streamText, UIMessage } from 'ai';
import { ExampleMetadata } from './example-metadata-schema';

export async function POST(req: Request) {
  const { messages }: { messages: UIMessage[] } = await req.json();

  const startTime = Date.now();
  const result = streamText({
    model: openai('gpt-4o'),
    prompt: convertToModelMessages(messages),
  });

  return result.toUIMessageStreamResponse({
    messageMetadata: ({ part }): ExampleMetadata | undefined => {
      // send custom information to the client on start:
      if (part.type === 'start') {
        return {
          model: 'gpt-4o', // initial model id
        };
      }

      // send additional model information on finish-step:
      if (part.type === 'finish-step') {
        return {
          model: part.response.modelId, // update with the actual model id
          duration: Date.now() - startTime,
        };
      }

      // when the message is finished, send additional information:
      if (part.type === 'finish') {
        return {
          totalTokens: part.totalUsage.totalTokens,
        };
      }
    },
  });
}
```

----------------------------------------

TITLE: API: Web Source Parameters
DESCRIPTION: Defines the parameters available for configuring web search sources within the xai model's search functionality.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/01-xai.mdx#_snippet_12

LANGUAGE: APIDOC
CODE:
```
country: string (ISO alpha-2 country code)
allowedWebsites: string[] (Max 5 allowed websites)
excludedWebsites: string[] (Max 5 excluded websites)
safeSearch: boolean (Enable safe search, default: true)
```

----------------------------------------

TITLE: Route Handler with AI SDK Core and OpenAI Provider
DESCRIPTION: Refactored Next.js Route Handler showcasing the use of the new AI SDK Core's unified API with the `@ai-sdk/openai` provider. This example demonstrates how to use the `streamText` function for model interaction and return the response using `toUIMessageStreamResponse()` for simplified streaming UI development.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/08-migration-guides/39-migration-guide-3-1.mdx#_snippet_2

LANGUAGE: tsx
CODE:
```
import { streamText } from 'ai';
import { openai } from '@ai-sdk/openai';

export async function POST(req: Request) {
  const { messages } = await req.json();

  const result = await streamText({
    model: openai('gpt-4.1'),
    messages,
  });

  return result.toUIMessageStreamResponse();
}
```

----------------------------------------

TITLE: Start SSE Transport Server (Legacy)
DESCRIPTION: Launches the server for the legacy Server-Sent Events (SSE) transport. This server provides a streaming interface for older implementations.
SOURCE: https://github.com/vercel/ai/blob/main/examples/mcp/README.md#_snippet_6

LANGUAGE: sh
CODE:
```
pnpm sse:server
```

----------------------------------------

TITLE: embedMany API Parameters Reference
DESCRIPTION: Detailed documentation of the parameters accepted by the `embedMany` function, including their types, descriptions, and optionality. It also covers nested properties for `experimental_telemetry`.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/06-embed-many.mdx#_snippet_2

LANGUAGE: APIDOC
CODE:
```
embedMany Parameters:
- model: EmbeddingModel (The embedding model to use. Example: openai.textEmbeddingModel('text-embedding-3-small'))
- values: Array<VALUE> (The values to embed. The type depends on the model.)
- maxRetries: number (Optional, Maximum number of retries. Set to 0 to disable retries. Default: 2.)
- abortSignal: AbortSignal (Optional, An optional abort signal that can be used to cancel the call.)
- headers: Record<string, string> (Optional, Additional HTTP headers to be sent with the request. Only applicable for HTTP-based providers.)
- experimental_telemetry: TelemetrySettings (Optional, Telemetry configuration. Experimental feature.)
  - isEnabled: boolean (Optional, Enable or disable telemetry. Disabled by default while experimental.)
  - recordInputs: boolean (Optional, Enable or disable input recording. Enabled by default.)
  - recordOutputs: boolean (Optional, Enable or disable output recording. Enabled by default.)
  - functionId: string (Optional, Identifier for this function. Used to group telemetry data by function.)
  - metadata: Record<string, string | number | boolean | Array<null | undefined | string> | Array<null | undefined | number> | Array<null | undefined | boolean>> (Optional, Additional information to include in the telemetry data.)
  - tracer: Tracer (Optional, A custom tracer to use for the telemetry data.)
```

----------------------------------------

TITLE: Create environment file
DESCRIPTION: Creates an empty .env file in the project root, which will be used to store sensitive environment variables such as API keys.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-getting-started/05-nuxt.mdx#_snippet_3

LANGUAGE: Shell
CODE:
```
touch .env
```

----------------------------------------

TITLE: Start Nest.js Development Server
DESCRIPTION: This command initiates the Nest.js application in development mode, typically enabling features like hot-reloading for efficient development and testing.
SOURCE: https://github.com/vercel/ai/blob/v5/examples/nest/README.md#_snippet_2

LANGUAGE: sh
CODE:
```
pnpm run start:dev
```

----------------------------------------

TITLE: Example Response Headers from streamText Bedrock API Call (JavaScript)
DESCRIPTION: Presents a sample JSON object illustrating the HTTP response headers received from the Amazon Bedrock service when streaming text via the `streamText` function. It includes headers relevant to streamed content like `content-type` and `transfer-encoding`.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/08-amazon-bedrock.mdx#_snippet_30

LANGUAGE: JavaScript
CODE:
```
{
  ""connection"": ""keep-alive"",
  ""content-type"": ""application/vnd.amazon.eventstream"",
  ""date"": ""Fri, 07 Feb 2025 04:33:37 GMT"",
  ""transfer-encoding"": ""chunked"",
  ""x-amzn-requestid"": ""a976e3fc-0e45-4241-9954-b9bdd80ab407""
}
```

----------------------------------------

TITLE: Import createOpenAICompatible Method
DESCRIPTION: Shows how to import the `createOpenAICompatible` function from the `@ai-sdk/openai-compatible` module to create a provider instance.
SOURCE: https://github.com/vercel/ai/blob/main/packages/openai-compatible/README.md#_snippet_1

LANGUAGE: ts
CODE:
```
import { createOpenAICompatible } from '@ai-sdk/openai-compatible';
```

----------------------------------------

TITLE: embedMany API Returns Reference
DESCRIPTION: Documentation of the return object structure for the `embedMany` function, detailing the `values`, `embeddings`, and `usage` properties, including nested properties for `EmbeddingModelUsage`.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/06-embed-many.mdx#_snippet_3

LANGUAGE: APIDOC
CODE:
```
embedMany Returns:
- values: Array<VALUE> (The values that were embedded.)
- embeddings: number[][] (The embeddings. They are in the same order as the values.)
- usage: EmbeddingModelUsage (The token usage for generating the embeddings.)
  - tokens: number (The total number of input tokens.)
  - body: unknown (Optional, The response body.)
```

----------------------------------------

TITLE: GoogleGenerativeAIProvider Configuration Options
DESCRIPTION: Documentation for the optional settings available when creating a custom Google Generative AI provider instance, detailing parameters like `baseURL`, `apiKey`, `headers`, and `fetch`.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/15-google-generative-ai.mdx#_snippet_3

LANGUAGE: APIDOC
CODE:
```
createGoogleGenerativeAI(options?: object): GoogleGenerativeAIProvider
  options:
    baseURL: string
      Description: Use a different URL prefix for API calls, e.g. to use proxy servers. The default prefix is https://generativelanguage.googleapis.com/v1beta.
    apiKey: string
      Description: API key that is being sent using the x-goog-api-key header. It defaults to the GOOGLE_GENERATIVE_AI_API_KEY environment variable.
    headers: Record<string,string>
      Description: Custom headers to include in the requests.
    fetch: (input: RequestInfo, init?: RequestInit) => Promise<Response>
      Description: Custom fetch implementation. Defaults to the global fetch function. You can use it as a middleware to intercept requests, or to provide a custom fetch implementation for e.g. testing.
```

----------------------------------------

TITLE: AI SDK UI `useChat` Hook Reference
DESCRIPTION: API documentation for the `useChat` hook from the `@ai-sdk/vue` package. This hook simplifies the implementation of chat interfaces by providing state management for messages and utility functions for sending messages, typically interacting with a `/api/chat` endpoint by default.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-getting-started/05-nuxt.mdx#_snippet_10

LANGUAGE: APIDOC
CODE:
```
useChat(options?: object):
  messages: Array<{ id: string; role: string; parts: Array<{ type: string; text?: string }> }> (current chat messages)
  sendMessage(message: { text: string }): void (function to send a message to the chat API)
```

----------------------------------------

TITLE: Implement Weather Tool in Chatbot API Route (TypeScript)
DESCRIPTION: This TypeScript code snippet updates the `server/api/chat.ts` file to integrate a 'weather' tool. It uses `@ai-sdk/openai` for model interaction and `zod` for input schema validation, enabling the chatbot to simulate fetching weather data for a specified location.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-getting-started/05-nuxt.mdx#_snippet_13

LANGUAGE: typescript
CODE:
```
import { streamText, UIMessage, convertToModelMessages, tool } from 'ai';
import { createOpenAI } from '@ai-sdk/openai';
import { z } from 'zod';

export default defineLazyEventHandler(async () => {
  const apiKey = useRuntimeConfig().openaiApiKey;
  if (!apiKey) throw new Error('Missing OpenAI API key');
  const openai = createOpenAI({
    apiKey: apiKey,
  });

  return defineEventHandler(async (event: any) => {
    const { messages }: { messages: UIMessage[] } = await readBody(event);

    const result = streamText({
      model: openai('gpt-4o'),
      messages: convertToModelMessages(messages),
      tools: {
        weather: tool({
          description: 'Get the weather in a location (fahrenheit)',
          inputSchema: z.object({
            location: z
              .string()
              .describe('The location to get the weather for'),
          }),
          execute: async ({ location }) => {
            const temperature = Math.round(Math.random() * (90 - 32) + 32);
            return {
              location,
              temperature,
            };
          },
        }),
      },
    });

    return result.toUIMessageStreamResponse();
  });
});
```

----------------------------------------

TITLE: Bootstrap Next.js AI Chat Example
DESCRIPTION: Commands to initialize a new Next.js application using the provided AI SDK, OpenAI, and Sentry example template. These commands use different package managers to create the project directory and copy the example files.
SOURCE: https://github.com/vercel/ai/blob/main/examples/next-openai-telemetry-sentry/README.md#_snippet_0

LANGUAGE: bash
CODE:
```
npx create-next-app --example https://github.com/vercel/ai/tree/main/examples/next-openai-telemetry-sentry next-openai-telemetry-sentry-app
```

LANGUAGE: bash
CODE:
```
yarn create next-app --example https://github.com/vercel/ai/tree/main/examples/next-openai-telemetry-sentry next-openai-telemetry-sentry-app
```

LANGUAGE: bash
CODE:
```
pnpm create next-app --example https://github.com/vercel/ai/tree/main/examples/next-openai-telemetry-sentry next-openai-telemetry-sentry-app
```

----------------------------------------

TITLE: Generate Text with Image Prompt and Tool Call (TypeScript)
DESCRIPTION: This TypeScript example demonstrates how to use the AI SDK's `generateText` function to send a multimodal prompt, including both text and an image URL, to an OpenAI model. It also shows how to define and integrate a tool (`logFood`) with Zod for parameter validation, allowing the model to call this tool based on the image content.
SOURCE: https://github.com/vercel/ai/blob/v5/content/cookbook/05-node/52-call-tools-with-image-prompt.mdx#_snippet_0

LANGUAGE: typescript
CODE:
```
import { generateText, tool } from 'ai';
import { openai } from '@ai-sdk/openai';
import { z } from 'zod';

const result = await generateText({
  model: openai('gpt-4.1'),
  messages: [
    {
      role: 'user',
      content: [
        { type: 'text', text: 'can you log this meal for me?' },
        {
          type: 'image',
          image: new URL(
            'https://upload.wikimedia.org/wikipedia/commons/thumb/e/e4/Cheeseburger_%2817237580619%29.jpg/640px-Cheeseburger_%2817237580619%29.jpg',
          ),
        },
      ],
    },
  ],
  tools: {
    logFood: tool({
      description: 'Log a food item',
      parameters: z.object({
        name: z.string(),
        calories: z.number(),
      }),
      execute({ name, calories }) {
        storeInDatabase({ name, calories });
      },
    }),
  },
});
```

----------------------------------------

TITLE: Generate Text with xAI Grok Model
DESCRIPTION: Example of using an xAI Grok language model with the `generateText` function to generate a recipe.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/01-xai.mdx#_snippet_5

LANGUAGE: TypeScript
CODE:
```
import { xai } from '@ai-sdk/xai';
import { generateText } from 'ai';

const { text } = await generateText({
  model: xai('grok-3'),
  prompt: 'Write a vegetarian lasagna recipe for 4 people.'
});
```

----------------------------------------

TITLE: Handle Custom Request Body on Server (Next.js API Route)
DESCRIPTION: This server-side example shows a Next.js API route (`app/api/chat/route.ts`) designed to receive and process a custom request body, specifically expecting only the text of the last message. It illustrates how to load message history from storage, use `@ai-sdk/openai` to stream a response, and save the conversation.
SOURCE: https://github.com/vercel/ai/blob/v5/content/cookbook/01-next/80-send-custom-body-from-use-chat.mdx#_snippet_1

LANGUAGE: tsx
CODE:
```
import { openai } from '@ai-sdk/openai'
import { streamText } from 'ai'

// Allow streaming responses up to 30 seconds
export const maxDuration = 30

export async function POST(req: Request) {
  // we receive only the text from the last message
  const text = await req.json()

  // e.g. load message history from storage
  const history = await loadHistory()

  // Call the language model
  const result = streamText({
    model: openai('gpt-4.1'),
    messages: [...history, { role: 'user', content: text }]
    onFinish({ text }) {
      // e.g. save the message and the response to storage
    }
  })

  // Respond with the stream
  return result.toUIMessageStreamResponse()
}
```

----------------------------------------

TITLE: useCompletion Hook API Parameters Reference
DESCRIPTION: Detailed documentation of the parameters available for configuring the `useCompletion` hook. It covers options for specifying the API endpoint, managing unique identifiers for shared states, setting initial input/completion values, defining callback functions for various stages (response, finish, error), and customizing HTTP request details like headers, body, credentials, stream protocol, and fetch behavior.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/07-reference/02-ai-sdk-ui/02-use-completion.mdx#_snippet_1

LANGUAGE: APIDOC
CODE:
```
useCompletion Hook Parameters:
  api: string = '/api/completion'
    description: The API endpoint that is called to generate text. It can be a relative path (starting with `/`) or an absolute URL.
  id: string
    description: An unique identifier for the completion. If not provided, a random one will be generated. When provided, the `useCompletion` hook with the same `id` will have shared states across components. This is useful when you have multiple components showing the same chat stream.
  initialInput: string
    description: An optional string for the initial prompt input.
  initialCompletion: string
    description: An optional string for the initial completion result.
  onResponse: (response: Response) => void
    description: An optional callback function that is called with the response from the API endpoint. Useful for throwing customized errors or logging.
  onFinish: (prompt: string, completion: string) => void
    description: An optional callback function that is called when the completion stream ends.
  onError: (error: Error) => void
    description: An optional callback that will be called when the chat stream encounters an error.
  headers: Record<string, string> | Headers
    description: An optional object of headers to be passed to the API endpoint.
  body: any
    description: An optional, additional body object to be passed to the API endpoint.
  credentials: 'omit' | 'same-origin' | 'include'
    description: An optional literal that sets the mode of credentials to be used on the request. Defaults to same-origin.
  streamProtocol: 'text' | 'data' (optional)
    description: An optional literal that sets the type of stream to be used. Defaults to `data`. If set to `text`, the stream will be treated as a text stream.
  fetch: FetchFunction (optional)
    description: Optional. A custom fetch function to be used for the API call. Defaults to the global fetch function.
  experimental_throttle: number (optional)
    description: React only. Custom throttle wait time in milliseconds for the completion and data updates. When specified, throttles how often the UI updates during streaming. Default is undefined, which disables throttling.
```

----------------------------------------

TITLE: Complete Azure OpenAI Image Generation Example
DESCRIPTION: A complete example demonstrating how to import necessary modules, initialize an Azure OpenAI image model, and generate an image using `experimental_generateImage`.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/04-azure.mdx#_snippet_25

LANGUAGE: ts
CODE:
```
import { azure } from '@ai-sdk/azure';
import { experimental_generateImage as generateImage } from 'ai';

const { image } = await generateImage({
  model: azure.imageModel('your-dalle-deployment-name'),
  prompt: 'A photorealistic image of a cat astronaut floating in space',
  size: '1024x1024', // '1024x1024', '1792x1024', or '1024x1792' for DALL-E 3
});

// image contains the URL or base64 data of the generated image
console.log(image);
```

----------------------------------------

TITLE: Enable Live Search with xAI Grok Model
DESCRIPTION: Example of enabling Live Search functionality for an xAI Grok model, including search parameters and citation retrieval.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/01-xai.mdx#_snippet_8

LANGUAGE: TypeScript
CODE:
```
import { xai } from '@ai-sdk/xai';
import { generateText } from 'ai';

const { text, sources } = await generateText({
  model: xai('grok-3-latest'),
  prompt: 'What are the latest developments in AI?',
  providerOptions: {
    xai: {
      searchParameters: {
        mode: 'auto', // 'auto', 'on', or 'off'
        returnCitations: true,
        maxSearchResults: 5
      }
    }
  }
});

console.log(text);
console.log('Sources:', sources);
```

----------------------------------------

TITLE: DeepSeek Provider Configuration Options
DESCRIPTION: Details the optional settings available for customizing the DeepSeek provider instance, including `baseURL`, `apiKey`, `headers`, and `fetch`.
SOURCE: https://github.com/vercel/ai/blob/v5/content/providers/01-ai-sdk-providers/30-deepseek.mdx#_snippet_3

LANGUAGE: APIDOC
CODE:
```
createDeepSeek(options: object)
  baseURL: string
    Description: Use a different URL prefix for API calls.
    Default: https://api.deepseek.com/v1
  apiKey: string
    Description: API key that is being sent using the Authorization header. It defaults to the DEEPSEEK_API_KEY environment variable.
  headers: Record<string, string>
    Description: Custom headers to include in the requests.
  fetch: (input: RequestInfo, init?: RequestInit) => Promise<Response>
    Description: Custom fetch implementation.
```

----------------------------------------

TITLE: Mem0 Memory Management API Reference
DESCRIPTION: Detailed API documentation for functions to manage and retrieve memories within the Mem0 AI SDK, outlining their purpose, parameters, and return types.
SOURCE: https://github.com/vercel/ai/blob/v5/content/providers/03-community-providers/70-mem0.mdx#_snippet_4

LANGUAGE: APIDOC
CODE:
```
addMemories(messages: LanguageModelV1Prompt, options: { user_id: string; mem0ApiKey?: string; org_id?: string; project_id?: string; }): Promise<void>
  Description: Adds user memories to enhance contextual responses.
  Parameters:
    messages: LanguageModelV1Prompt - The prompt messages to associate with memories.
    options: Object - Configuration options.
      user_id: string - The ID of the user.
      mem0ApiKey?: string - Optional Mem0 API key.
      org_id?: string - Optional organization ID.
      project_id?: string - Optional project ID.

retrieveMemories(prompt: string, options: { user_id: string; mem0ApiKey?: string; org_id?: string; project_id?: string; }): Promise<string>
  Description: Retrieves memory context for prompts.
  Parameters:
    prompt: string - The prompt for which to retrieve memories.
    options: Object - Configuration options.
      user_id: string - The ID of the user.
      mem0ApiKey?: string - Optional Mem0 API key.
      org_id?: string - Optional organization ID.
      project_id?: string - Optional project ID.
  Returns: string - A response in string format with a system prompt ingested with the retrieved memories.

getMemories(prompt: string, options: { user_id: string; mem0ApiKey?: string; org_id?: string; project_id?: string; }): Promise<Array<Object>>
  Description: Gets memories from your profile in array format.
  Parameters:
    prompt: string - The prompt for which to get memories.
    options: Object - Configuration options.
      user_id: string - The ID of the user.
      mem0ApiKey?: string - Optional Mem0 API key.
      org_id?: string - Optional organization ID.
      project_id?: string - Optional project ID.
  Returns: Array<Object> - Raw memories in the form of an array of objects.
```

----------------------------------------

TITLE: API Return Properties Reference
DESCRIPTION: Detailed documentation of the properties returned by the API, including their types, descriptions, and nested structures. Each property is a Promise that resolves when the response is finished. This reference covers content, finish reasons, usage statistics, provider metadata, reasoning, and sources.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/02-stream-text.mdx#_snippet_18

LANGUAGE: APIDOC
CODE:
```
content: Promise<Array<ContentPart<TOOLS>>>
	Description: The content that was generated in the last step. Resolved when the response is finished.

finishReason: Promise<'stop' | 'length' | 'content-filter' | 'tool-calls' | 'error' | 'other' | 'unknown'>
	Description: The reason why the generation finished. Resolved when the response is finished.

usage: Promise<LanguageModelUsage>
	Description: The token usage of the last step. Resolved when the response is finished.
	Properties:
		LanguageModelUsage:
			promptTokens: number
				Description: The total number of tokens in the prompt.
			completionTokens: number
				Description: The total number of tokens in the completion.
			totalTokens: number
				Description: The total number of tokens generated.

totalUsage: Promise<LanguageModelUsage>
	Description: The total token usage of the generated response. When there are multiple steps, the usage is the sum of all step usages. Resolved when the response is finished.
	Properties:
		LanguageModelUsage:
			promptTokens: number
				Description: The total number of tokens in the prompt.
			completionTokens: number
				Description: The total number of tokens in the completion.
			totalTokens: number
				Description: The total number of tokens generated.

providerMetadata: Promise<ProviderMetadata | undefined>
	Description: Additional provider-specific metadata from the last step. Metadata is passed through from the provider to the AI SDK and enables provider-specific results that can be fully encapsulated in the provider.

text: Promise<string>
	Description: The full text that has been generated. Resolved when the response is finished.

reasoning: Promise<Array<ReasoningPart>>
	Description: The full reasoning that the model has generated in the last step. Resolved when the response is finished.
	Properties:
		ReasoningPart:
			type: 'reasoning'
				Description: The type of the reasoning part.
			text: string
				Description: The reasoning text.

reasoningText: Promise<string | undefined>
	Description: The reasoning text that the model has generated in the last step. Can be undefined if the model has only generated text. Resolved when the response is finished.

sources: Promise<Array<Source>>
	Description: Sources that have been used as input to generate the response. For multi-step generation, the sources are accumulated from all steps. Resolved when the response is finished.
	Properties:
		Source:
			sourceType: 'url'
				Description: A URL source. This is return by web search RAG models.
			id: string
				Description: The ID of the source.
			url: string
				Description: The URL of the source.
			title: string (Optional)
				Description: The title of the source.
			providerMetadata: SharedV2ProviderMetadata (Optional)
				Description: Additional provider metadata for the source.

files: Promise<Array<GeneratedFile>>
	Description: (truncated)
```

----------------------------------------

TITLE: Vercel AI SDK: streamObject Function Examples
DESCRIPTION: The `streamObject` function allows for streaming structured data objects as they are generated. This section provides links to various examples demonstrating its usage across different frameworks and environments.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/10-generating-structured-data.mdx#_snippet_14

LANGUAGE: APIDOC
CODE:
```
Examples for streamObject:
- Learn to stream objects in Node.js: /examples/node/streaming-structured-data/stream-object
- Learn to stream objects in Next.js with Route Handlers (AI SDK UI): /examples/next-pages/basics/streaming-object-generation
- Learn to stream objects in Next.js with Server Actions (AI SDK RSC): /examples/next-app/basics/streaming-object-generation
```

----------------------------------------

TITLE: Stream Text with Computer Tool
DESCRIPTION: This snippet shows how to use the `computerTool` with the AI SDK's `streamText` function for real-time responses. It allows the model to interact with the computer (e.g., opening a browser) and stream back its thought process or results as they become available, processing each chunk of the streamed text.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/00-guides/05-computer-use.mdx#_snippet_3

LANGUAGE: typescript
CODE:
```
const result = streamText({
  model: anthropic('claude-3-5-sonnet-20241022'),
  prompt: 'Open the browser and navigate to vercel.com',
  tools: { computer: computerTool },
});

for await (const chunk of result.textStream) {
  console.log(chunk);
}
```

----------------------------------------

TITLE: Configure Slack Event Subscription URL
DESCRIPTION: Specifies the endpoint for Slack to send event notifications, pointing to the deployed Vercel application's API. This URL enables Slack to communicate with your bot.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/00-guides/03-slackbot.mdx#_snippet_11

LANGUAGE: bash
CODE:
```
https://your-vercel-url.vercel.app/api/events
```

----------------------------------------

TITLE: InkeepStream API Signature
DESCRIPTION: Detailed API documentation for the InkeepStream function, outlining its parameters, their types, descriptions, and the function's return type. This function transforms a Response object from Inkeep's API into a ReadableStream.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/07-reference/04-stream-helpers/19-inkeep-stream.mdx#_snippet_1

LANGUAGE: APIDOC
CODE:
```
InkeepStream(
  response: Response,
  callbacks?: AIStreamCallbacksAndOptions
) -> ReadableStream

Parameters:
  response: The response object returned by a call made by the Provider SDK.
  callbacks: An object containing callback functions to handle the start, each token, and completion of the AI response. In the absence of this parameter, default behavior is implemented.
    AIStreamCallbacksAndOptions:
      onStart: () => Promise<void> - An optional function that is called at the start of the stream processing.
      onCompletion: (completion: string) => Promise<void> - An optional function that is called for every completion. It's passed the completion as a string.
      onFinal: (completion: string) => Promise<void> - An optional function that is called once when the stream is closed with the final completion message.
      onToken: (token: string) => Promise<void> - An optional function that is called for each token in the stream. It's passed the token as a string.

Returns:
  A ReadableStream.
```

----------------------------------------

TITLE: AI SDK `streamText` Function Reference
DESCRIPTION: API documentation for the `streamText` function from the `ai` package, used for streaming text from a language model. It details the configuration object's properties, including the model provider and message array types, and the `StreamTextResult` object it returns.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-getting-started/05-nuxt.mdx#_snippet_8

LANGUAGE: APIDOC
CODE:
```
streamText(config: object): StreamTextResult
  config:
    model: ModelProvider (e.g., openai('gpt-4o'))
    messages: ModelMessage[] (converted from UIMessage[] using convertToModelMessages)
    settings: object (optional, for further customization)
  Returns: StreamTextResult
```

----------------------------------------

TITLE: Fireworks Provider Instance Configuration Options
DESCRIPTION: Documentation for optional settings available when creating a customized Fireworks provider instance, including `baseURL`, `apiKey`, `headers`, and `fetch`.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/26-fireworks.mdx#_snippet_3

LANGUAGE: APIDOC
CODE:
```
baseURL: string
  Description: Use a different URL prefix for API calls, e.g. to use proxy servers. The default prefix is https://api.fireworks.ai/inference/v1.
apiKey: string
  Description: API key that is being sent using the Authorization header. It defaults to the FIREWORKS_API_KEY environment variable.
headers: Record<string,string>
  Description: Custom headers to include in the requests.
fetch: (input: RequestInfo, init?: RequestInit) => Promise<Response>
  Description: Custom fetch implementation.
```

----------------------------------------

TITLE: Create Custom DeepSeek Provider Instance
DESCRIPTION: Shows how to create a custom DeepSeek provider instance using `createDeepSeek` for advanced configuration, such as setting an API key.
SOURCE: https://github.com/vercel/ai/blob/v5/content/providers/01-ai-sdk-providers/30-deepseek.mdx#_snippet_2

LANGUAGE: ts
CODE:
```
import { createDeepSeek } from '@ai-sdk/deepseek';

const deepseek = createDeepSeek({
  apiKey: process.env.DEEPSEEK_API_KEY ?? ''
});
```

----------------------------------------

TITLE: Configure Self-Hosted Dify Provider
DESCRIPTION: This example illustrates how to configure the Dify provider for a self-hosted Dify instance. It uses `createDifyProvider` to specify a custom base URL, then initializes the provider with an application ID and API key.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/97-dify.mdx#_snippet_3

LANGUAGE: typescript
CODE:
```
import { createDifyProvider } from 'dify-ai-provider';

const difyProvider = createDifyProvider({
  baseURL: 'your-base-url',
});

const dify = difyProvider('dify-application-id', {
  responseMode: 'blocking',
  apiKey: 'dify-api-key',
});
```

----------------------------------------

TITLE: DeepSeek Model Capabilities
DESCRIPTION: A summary of capabilities for various DeepSeek models, including text generation, object generation, image input, tool usage, and tool streaming support.
SOURCE: https://github.com/vercel/ai/blob/v5/content/providers/01-ai-sdk-providers/30-deepseek.mdx#_snippet_7

LANGUAGE: APIDOC
CODE:
```
Model: deepseek-chat
  Text Generation: Yes
  Object Generation: Yes
  Image Input: No
  Tool Usage: Yes
  Tool Streaming: Yes

Model: deepseek-reasoner
  Text Generation: Yes
  Object Generation: No
  Image Input: No
  Tool Usage: No
  Tool Streaming: No
```

----------------------------------------

TITLE: Refine AI Chatbot Behavior with System Instructions in Next.js API Route
DESCRIPTION: This updated TypeScript code for the Next.js API route enhances the chatbot's behavior by adding a `system` instruction to the `streamText` function. This instruction guides the AI model to prioritize information from tool calls and provides a fallback response if no relevant information is found, thereby refining its conversational constraints.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/00-guides/01-rag-chatbot.mdx#_snippet_21

LANGUAGE: tsx
CODE:
```
import { openai } from '@ai-sdk/openai';
import { convertToModelMessages, streamText, UIMessage } from 'ai';

// Allow streaming responses up to 30 seconds
export const maxDuration = 30;

export async function POST(req: Request) {
  const { messages }: { messages: UIMessage[] } = await req.json();

  const result = streamText({
    model: openai('gpt-4o'),
    system: `You are a helpful assistant. Check your knowledge base before answering any questions.
    Only respond to questions using information from tool calls.
    if no relevant information is found in the tool calls, respond, ""Sorry, I don't know.""`,
    messages: convertToModelMessages(messages),
  });

  return result.toUIMessageStreamResponse();
}
```

----------------------------------------

TITLE: DeepInfra Provider Configuration Options
DESCRIPTION: API documentation detailing the optional settings available when creating a custom DeepInfra provider instance. These options allow for fine-grained control over API calls, including base URL, API key, custom headers, and fetch implementation.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/11-deepinfra.mdx#_snippet_3

LANGUAGE: APIDOC
CODE:
```
createDeepInfra(options?: object)
  options:
    baseURL: string
      Description: Use a different URL prefix for API calls, e.g. to use proxy servers.
      Default: https://api.deepinfra.com/v1
      Note: Language models and embeddings use OpenAI-compatible endpoints at {baseURL}/openai, while image models use {baseURL}/inference.
    apiKey: string
      Description: API key that is being sent using the Authorization header.
      Default: DEEPINFRA_API_KEY environment variable.
    headers: Record<string,string>
      Description: Custom headers to include in the requests.
    fetch: (input: RequestInfo, init?: RequestInit) => Promise<Response>
      Description: Custom fetch implementation. Defaults to the global fetch function. Can be used as a middleware to intercept requests, or to provide a custom fetch implementation for e.g. testing.
```

----------------------------------------

TITLE: Extract and Define AI SDK Tools with `tool` Helper
DESCRIPTION: This snippet demonstrates how to organize tools into separate files using the `tool` helper function from the AI SDK. This approach ensures correct type inference for tool definitions, making it easier to manage and scale applications with many tools. The example shows a `weatherTool` with a description and Zod-defined parameters.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/15-tools-and-tool-calling.mdx#_snippet_23

LANGUAGE: ts
CODE:
```
import { tool } from 'ai';
import { z } from 'zod';

// the `tool` helper function ensures correct type inference:
export const weatherTool = tool({
  description: 'Get the weather in a location',
  parameters: z.object({
    location: z.string().describe('The location to get the weather for'),
  }),
  execute: async ({ location }) => ({
    location,
    temperature: 72 + Math.floor(Math.random() * 21) - 10,
  }),
});
```

----------------------------------------

TITLE: Generate Text with Claude 3.7 Sonnet using AI SDK (Anthropic)
DESCRIPTION: This snippet demonstrates how to generate text using Claude 3.7 Sonnet via the AI SDK's Anthropic provider. It imports necessary modules, defines the model, and sends a prompt to get a text response. The output includes the generated text, reasoning, and reasoning details.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/00-guides/20-sonnet-3-7.mdx#_snippet_0

LANGUAGE: ts
CODE:
```
import { anthropic } from '@ai-sdk/anthropic';
import { generateText } from 'ai';

const { text, reasoning, reasoningDetails } = await generateText({
  model: anthropic('claude-3-7-sonnet-20250219'),
  prompt: 'How many people will live in the world in 2040?',
});
console.log(text); // text response
```

----------------------------------------

TITLE: Migrate AI SDK model instance to Responses API
DESCRIPTION: Compares the syntax for instantiating models between the OpenAI Completions API and the new Responses API. It demonstrates changing the model provider instance from `openai(modelId)` to `openai.responses(modelId)`.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/00-guides/19-openai-responses.mdx#_snippet_6

LANGUAGE: ts
CODE:
```
import { generateText } from 'ai';
import { openai } from '@ai-sdk/openai';

// Completions API
const { text } = await generateText({
  model: openai('gpt-4o'),
  prompt: 'Explain the concept of quantum entanglement.',
});

// Responses API
const { text } = await generateText({
  model: openai.responses('gpt-4o'),
  prompt: 'Explain the concept of quantum entanglement.',
});
```

----------------------------------------

TITLE: Example Prompt Format for LLM Documentation Q&A
DESCRIPTION: This snippet demonstrates the recommended format for providing the AI SDK documentation to a large language model (LLM) and then asking questions based on it. It helps ensure the LLM has the necessary context to answer queries about the SDK's features and usage.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/00-introduction/index.mdx#_snippet_0

LANGUAGE: prompt
CODE:
```
Documentation:
{paste documentation here}
---
Based on the above documentation, answer the following:
{your question}
```

----------------------------------------

TITLE: Build Stdio Transport Example
DESCRIPTION: Compiles or prepares the stdio transport example. This step ensures the necessary components are ready for the stdio client to run.
SOURCE: https://github.com/vercel/ai/blob/main/examples/mcp/README.md#_snippet_4

LANGUAGE: sh
CODE:
```
pnpm stdio:build
```

----------------------------------------

TITLE: OpenAI PDF Support API Details
DESCRIPTION: Describes the mechanism for sending PDF files to the OpenAI Chat API, specifying the required message content structure including `type: 'file'`, `data` (file buffer), `mediaType: 'application/pdf'`, and an optional `filename`.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/03-openai.mdx#_snippet_17

LANGUAGE: APIDOC
CODE:
```
OpenAI PDF Support:
  API: OpenAI Chat API
  Method: Pass PDF files as part of message content
  Message Content Type: 'file'
  File Object Structure:
    - type: 'file'
    - data: fs.readFileSync('./path/to/file.pdf') (Buffer)
    - mediaType: 'application/pdf'
    - filename: 'string' (optional)
  Functionality: Model accesses PDF content and responds to questions about it.
```

----------------------------------------

TITLE: Qwen Language Model Capabilities
DESCRIPTION: Overview of capabilities for various Qwen language models, including support for image input, object generation, tool usage, and tool streaming.
SOURCE: https://github.com/vercel/ai/blob/v5/content/providers/03-community-providers/02-qwen.mdx#_snippet_6

LANGUAGE: APIDOC
CODE:
```
Model Capabilities:
  qwen-vl-max:
    Image Input: true
    Object Generation: true
    Tool Usage: true
    Tool Streaming: true
  qwen-plus-latest:
    Image Input: false
    Object Generation: true
    Tool Usage: true
    Tool Streaming: true
  qwen-max:
    Image Input: false
    Object Generation: true
    Tool Usage: true
    Tool Streaming: true
  qwen2.5-72b-instruct:
    Image Input: false
    Object Generation: true
    Tool Usage: true
    Tool Streaming: true
  qwen2.5-14b-instruct-1m:
    Image Input: false
    Object Generation: true
    Tool Usage: true
    Tool Streaming: true
  qwen2.5-vl-72b-instruct:
    Image Input: true
    Object Generation: true
    Tool Usage: true
    Tool Streaming: true
```

----------------------------------------

TITLE: Configure OpenAI Completion Model Provider Options in TypeScript
DESCRIPTION: This example shows how to apply OpenAI-specific settings to a completion model during a `doGenerate` call. These `providerOptions` allow fine-grained control over the model's behavior, such as echoing the prompt, modifying token likelihoods, or adding a suffix. This enhances the flexibility of API interactions beyond standard call settings.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/03-openai.mdx#_snippet_36

LANGUAGE: ts
CODE:
```
const model = openai.completion('gpt-3.5-turbo-instruct');

await model.doGenerate({
  providerOptions: {
    openai: {
      echo: true, // optional, echo the prompt in addition to the completion
      logitBias: {
        // optional likelihood for specific tokens
        '50256': -100,
      },
      suffix: 'some text', // optional suffix that comes after a completion of inserted text
      user: 'test-user', // optional unique user identifier
    },
  },
});
```

----------------------------------------

TITLE: Test Hono Endpoint with cURL
DESCRIPTION: Sends an HTTP POST request to the local Hono server endpoint using cURL. This command is used to verify that the server is running and responding correctly.
SOURCE: https://github.com/vercel/ai/blob/v5/examples/hono/README.md#_snippet_3

LANGUAGE: sh
CODE:
```
curl -i -X POST http://localhost:8080
```

----------------------------------------

TITLE: generateId() API Reference
DESCRIPTION: Detailed API documentation for the `generateId` function, outlining its optional `size` parameter, its default value, deprecation status, and the type and description of its return value.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/07-reference/01-ai-sdk-core/90-generate-id.mdx#_snippet_2

LANGUAGE: APIDOC
CODE:
```
generateId(size?: number): string
  Parameters:
    size: number (optional)
      Description: The length of the generated ID. It defaults to 16. This parameter is deprecated and will be removed in the next major version.
  Returns:
    string
      Description: A string representing the generated ID.
```

----------------------------------------

TITLE: Qwen Provider Configuration Options
DESCRIPTION: Detailed documentation for optional settings available when creating a custom Qwen provider instance, including 'baseURL', 'apiKey', 'headers', and 'fetch'.
SOURCE: https://github.com/vercel/ai/blob/v5/content/providers/03-community-providers/02-qwen.mdx#_snippet_3

LANGUAGE: APIDOC
CODE:
```
QwenProviderOptions:
  baseURL: string
    description: Use a different URL prefix for API calls, e.g. to use proxy servers.
    default: https://dashscope-intl.aliyuncs.com/compatible-mode/v1
  apiKey: string
    description: API key that is being sent using the `Authorization` header.
    default: DASHSCOPE_API_KEY environment variable
  headers: Record<string,string>
    description: Custom headers to include in the requests.
  fetch: (input: RequestInfo, init?: RequestInit) => Promise<Response>
    description: Custom fetch implementation. Defaults to the global `fetch` function. Can be used as a middleware or for custom fetch implementation (e.g., testing).
```

----------------------------------------

TITLE: Customize HTTP Request Options for useCompletion Hook
DESCRIPTION: This example illustrates how to configure the underlying HTTP POST request made by the `useCompletion` hook. It shows how to specify a custom API endpoint (`api`), add custom HTTP headers (`headers`), include additional data in the request body (`body`), and set credentials options (`credentials`). This allows for flexible integration with various backend services and authentication mechanisms.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/04-ai-sdk-ui/05-completion.mdx#_snippet_7

LANGUAGE: tsx
CODE:
```
const { messages, input, handleInputChange, handleSubmit } = useCompletion({
  api: '/api/custom-completion',
  headers: {
    Authorization: 'your_token'
  },
  body: {
    user_id: '123'
  },
  credentials: 'same-origin'
});
```

----------------------------------------

TITLE: Create Server-Side AI Chat API with Parallel Tool Calling
DESCRIPTION: This Next.js API route (`/api/chat`) uses the `ai` and `@ai-sdk/openai` libraries to stream text responses. It defines a `getWeather` tool with Zod schema validation for its parameters, enabling the AI model to call this function in parallel based on user queries.
SOURCE: https://github.com/vercel/ai/blob/v5/content/cookbook/01-next/71-call-tools-in-parallel.mdx#_snippet_1

LANGUAGE: ts
CODE:
```
import { ToolInvocation, streamText } from 'ai';
import { openai } from '@ai-sdk/openai';
import { z } from 'zod';

interface Message {
  role: 'user' | 'assistant';
  content: string;
  toolInvocations?: ToolInvocation[];
}

function getWeather({ city, unit }) {
  return { value: 25, description: 'Sunny' };
}

export async function POST(req: Request) {
  const { messages }: { messages: Message[] } = await req.json();

  const result = streamText({
    model: openai('gpt-4o'),
    system: 'You are a helpful assistant.',
    messages,
    tools: {
      getWeather: {
        description: 'Get the weather for a location',
        parameters: z.object({
          city: z.string().describe('The city to get the weather for'),
          unit: z
            .enum(['C', 'F'])
            .describe('The unit to display the temperature in'),
        }),
        execute: async ({ city, unit }) => {
          const { value, description } = getWeather({ city, unit });
          return `It is currently ${value}°${unit} and ${description} in ${city}!`;
        },
      },
    },
  });

  return result.toUIMessageStreamResponse();
}
```

----------------------------------------

TITLE: Example Google Generative AI Grounding Metadata JSON
DESCRIPTION: This JSON example illustrates the structure and content of the 'groundingMetadata' returned by the Google Generative AI model when search grounding is enabled. It provides a concrete instance of 'webSearchQueries', 'searchEntryPoint', and 'groundingSupports'.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/15-google-generative-ai.mdx#_snippet_13

LANGUAGE: json
CODE:
```
{
  ""groundingMetadata"": {
    ""webSearchQueries"": [""What's the weather in Chicago this weekend?""],
    ""searchEntryPoint"": {
      ""renderedContent"": ""...""
    },
    ""groundingSupports"": [
      {
        ""segment"": {
          ""startIndex"": 0,
          ""endIndex"": 65,
          ""text"": ""Chicago weather changes rapidly, so layers let you adjust easily.""
        },
        ""groundingChunkIndices"": [0],
        ""confidenceScores"": [0.99]
      }
    ]
  }
}
```

----------------------------------------

TITLE: Convert LlamaIndex ChatEngine Stream to Data Stream Response (TypeScript)
DESCRIPTION: Example of a Next.js API route demonstrating how to use `toDataStreamResponse` to convert a LlamaIndex `SimpleChatEngine` stream into a Vercel AI SDK data stream response.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/04-stream-helpers/16-llamaindex-adapter.mdx#_snippet_2

LANGUAGE: tsx
CODE:
```
import { OpenAI, SimpleChatEngine } from 'llamaindex';
import { toDataStreamResponse } from '@ai-sdk/llamaindex';

export async function POST(req: Request) {
  const { prompt } = await req.json();

  const llm = new OpenAI({ model: 'gpt-4o' });
  const chatEngine = new SimpleChatEngine({ llm });

  const stream = await chatEngine.chat({
    message: prompt,
    stream: true,
  });

  return toDataStreamResponse(stream);
}
```

----------------------------------------

TITLE: Message Annotation Part Format and Example
DESCRIPTION: Defines the structure and provides an example of the message annotation part, which is appended to messages as they are received and made available through the `annotations` property.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/04-ai-sdk-ui/50-stream-protocol.mdx#_snippet_2

LANGUAGE: APIDOC
CODE:
```
Format: 8:Array<JSONValue>
Example: 8:[{""id"":""message-123"",""other"":""annotation""}]
```

----------------------------------------

TITLE: Install AI SDK and Anthropic Provider
DESCRIPTION: This command installs the necessary npm packages for using the AI SDK and the Anthropic provider in your project, enabling integration with Anthropic models like Claude Sonnet.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/00-guides/18-claude-4.mdx#_snippet_2

LANGUAGE: Shell
CODE:
```
pnpm install ai @ai-sdk/anthropic
```

----------------------------------------

TITLE: React useObject Hook Usage Example
DESCRIPTION: Demonstrates how to use the `experimental_useObject` hook in a React component. It sets up an API endpoint and a Zod schema for object parsing, allowing a button click to submit input and display the generated object content.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/07-reference/02-ai-sdk-ui/03-use-object.mdx#_snippet_0

LANGUAGE: tsx
CODE:
```
'use client';

import { experimental_useObject as useObject } from '@ai-sdk/react';

export default function Page() {
  const { object, submit } = useObject({
    api: '/api/use-object',
    schema: z.object({ content: z.string() }),
  });

  return (
    <div>
      <button onClick={() => submit('example input')}>Generate</button>
      {object?.content && <p>{object.content}</p>}
    </div>
  );
}
```

----------------------------------------

TITLE: simulateStreamingMiddleware API Signature
DESCRIPTION: Details the API signature for `simulateStreamingMiddleware`, specifying that it takes no parameters and returns a middleware object capable of converting complete language model responses into simulated streams, preserving all relevant data.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/07-reference/01-ai-sdk-core/67-simulate-streaming-middleware.mdx#_snippet_1

LANGUAGE: APIDOC
CODE:
```
simulateStreamingMiddleware(): MiddlewareObject
  Parameters: None
  Returns: MiddlewareObject
    - Takes a complete response from a language model
    - Converts it into a simulated stream of chunks
    - Properly handles various response components including:
      - Text content
      - Reasoning (as string or array of objects)
      - Tool calls
      - Metadata and usage information
      - Warnings
```

----------------------------------------

TITLE: Reinitialize Git Repository
DESCRIPTION: Removes the existing Git repository configuration, reinitializes a new one, stages all current files, and commits them. This sequence is useful for unlinking a repository or starting fresh if a local repository is incorrectly configured.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/06-advanced/10-vercel-deployment-guide.mdx#_snippet_1

LANGUAGE: bash
CODE:
```
rm -rf .git
git init
git add .
git commit -m ""init""
```

----------------------------------------

TITLE: Combine Multiple Anthropic Computer Use Tools in TypeScript
DESCRIPTION: This snippet demonstrates how to initialize and combine `computer`, `bash`, and `textEditor` tools from Anthropic's AI SDK. It shows how to define custom execution logic for `bash` and `textEditor` tools, including handling file operations for the text editor. Finally, it illustrates how to use these combined tools within a `generateText` call to execute a multi-step prompt.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/00-guides/05-computer-use.mdx#_snippet_5

LANGUAGE: ts
CODE:
```
const computerTool = anthropic.tools.computer_20241022({
  ...
});

const bashTool = anthropic.tools.bash_20241022({
  execute: async ({ command, restart }) => execSync(command).toString()
});

const textEditorTool = anthropic.tools.textEditor_20241022({
  execute: async ({
    command,
    path,
    file_text,
    insert_line,
    new_str,
    old_str,
    view_range
  }) => {
    // Handle file operations based on command
    switch(command) {
      return executeTextEditorFunction({
        command,
        path,
        fileText: file_text,
        insertLine: insert_line,
        newStr: new_str,
        oldStr: old_str,
        viewRange: view_range
      });
    }
  }
});


const response = await generateText({
  model: anthropic(""claude-3-5-sonnet-20241022""),
  prompt: ""Create a new file called example.txt, write 'Hello World' to it, and run 'cat example.txt' in the terminal"",
  tools: {
    computer: computerTool,
    bash: bashTool,
    str_replace_editor: textEditorTool
  }
});
```

----------------------------------------

TITLE: Create Environment File
DESCRIPTION: Copies the example environment file (`.env.example`) to create a new `.env` file. This file will be used to store sensitive information like database connection strings and API keys, which are crucial for the application to function.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/00-guides/01-rag-chatbot.mdx#_snippet_4

LANGUAGE: bash
CODE:
```
cp .env.example .env
```

----------------------------------------

TITLE: Create Server-Side Chat API Endpoint with AI SDK
DESCRIPTION: This Next.js API route handles incoming chat messages from the client. It uses the AI SDK's `generateText` function with an OpenAI model (gpt-4) to produce an assistant's response based on the provided conversation history, then returns the updated messages as JSON.
SOURCE: https://github.com/vercel/ai/blob/v5/content/cookbook/01-next/11-generate-text-with-chat-prompt.mdx#_snippet_1

LANGUAGE: typescript
CODE:
```
import { ModelMessage, generateText } from 'ai';
import { openai } from '@ai-sdk/openai';

export async function POST(req: Request) {
  const { messages }: { messages: ModelMessage[] } = await req.json();

  const { response } = await generateText({
    model: openai('gpt-4'),
    system: 'You are a helpful assistant.',
    messages,
  });

  return Response.json({ messages: response.messages });
}
```

----------------------------------------

TITLE: AI SDK Core Helper Functions
DESCRIPTION: Utility functions supporting AI SDK Core operations, including tool inference, schema creation, provider management, stream manipulation, and ID generation.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/07-reference/01-ai-sdk-core/index.mdx#_snippet_1

LANGUAGE: APIDOC
CODE:
```
tool(): Type inference helper function for tools.
```

LANGUAGE: APIDOC
CODE:
```
experimental_createMCPClient(): Creates a client for connecting to MCP servers.
```

LANGUAGE: APIDOC
CODE:
```
jsonSchema(): Creates AI SDK compatible JSON schema objects.
```

LANGUAGE: APIDOC
CODE:
```
zodSchema(): Creates AI SDK compatible Zod schema objects.
```

LANGUAGE: APIDOC
CODE:
```
createProviderRegistry(): Creates a registry for using models from multiple providers.
```

LANGUAGE: APIDOC
CODE:
```
cosineSimilarity(): Calculates the cosine similarity between two vectors, e.g. embeddings.
```

LANGUAGE: APIDOC
CODE:
```
simulateReadableStream(): Creates a ReadableStream that emits values with configurable delays.
```

LANGUAGE: APIDOC
CODE:
```
wrapLanguageModel(): Wraps a language model with middleware.
```

LANGUAGE: APIDOC
CODE:
```
extractReasoningMiddleware(): Extracts reasoning from the generated text and exposes it as a `reasoning` property on the result.
```

LANGUAGE: APIDOC
CODE:
```
simulateStreamingMiddleware(): Simulates streaming behavior with responses from non-streaming language models.
```

LANGUAGE: APIDOC
CODE:
```
defaultSettingsMiddleware(): Applies default settings to a language model.
```

LANGUAGE: APIDOC
CODE:
```
smoothStream(): Smooths text streaming output.
```

LANGUAGE: APIDOC
CODE:
```
generateId(): Helper function for generating unique IDs
```

LANGUAGE: APIDOC
CODE:
```
createIdGenerator(): Creates an ID generator
```

----------------------------------------

TITLE: Generate Image with Reference Images (Luma)
DESCRIPTION: This example demonstrates how to guide image generation using up to four reference images with the `generateImage` function. The `weight` parameter, ranging from 0 to 1, controls the influence of these reference images on the final output.
SOURCE: https://github.com/vercel/ai/blob/v5/content/providers/01-ai-sdk-providers/80-luma.mdx#_snippet_8

LANGUAGE: typescript
CODE:
```
// Example: Generate a salamander with reference
await generateImage({
  model: luma.image('photon-1'),
  prompt: 'A salamander at dusk in a forest pond, in the style of ukiyo-e',
  providerOptions: {
    luma: {
      image_ref: [
        {
          url: 'https://example.com/reference.jpg',
          weight: 0.85
        }
      ]
    }
  }
});
```

----------------------------------------

TITLE: Vercel AI SDK API Response and Stream Structure
DESCRIPTION: Detailed documentation of the return values from the Vercel AI SDK API, including the main response object, optional data, warnings, and the comprehensive stream structure with various event types like text deltas, tool calls, errors, and finish reasons. This snippet outlines the types, descriptions, and nested properties for each component of the API's output.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/07-reference/03-ai-sdk-rsc/01-stream-ui.mdx#_snippet_4

LANGUAGE: APIDOC
CODE:
```
API Returns:
  value: ReactNode
    The user interface based on the stream output.
  response: Response (optional)
    Optional response data.
    Response:
      headers: Record<string, string> (optional)
        Response headers.
  warnings: Warning[] | undefined
    Warnings from the model provider (e.g. unsupported settings).
  stream: AsyncIterable<StreamPart> & ReadableStream<StreamPart>
    A stream with all events, including text deltas, tool calls, tool results, and errors. You can use it as either an AsyncIterable or a ReadableStream. When an error occurs, the stream will throw the error.
    StreamPart (type: 'text-delta'):
      type: 'text-delta'
        The type to identify the object as text delta.
      textDelta: string
        The text delta.
    StreamPart (type: 'tool-call'):
      type: 'tool-call'
        The type to identify the object as tool call.
      toolCallId: string
        The id of the tool call.
      toolName: string
        The name of the tool, which typically would be the name of the function.
      args: object based on zod schema
        Parameters generated by the model to be used by the tool.
    StreamPart (type: 'error'):
      type: 'error'
        The type to identify the object as error.
      error: Error
        Describes the error that may have occurred during execution.
    StreamPart (type: 'finish'):
      type: 'finish'
        The type to identify the object as finish.
      finishReason: 'stop' | 'length' | 'content-filter' | 'tool-calls' | 'error' | 'other' | 'unknown'
        The reason the model finished generating the text.
      usage: TokenUsage
        The token usage of the generated text.
        TokenUsage:
          promptTokens: number
            The total number of tokens in the prompt.
          completionTokens: number
            The total number of tokens in the completion.
          totalTokens: number
            The total number of tokens generated.
```

----------------------------------------

TITLE: wrapLanguageModel API Signature
DESCRIPTION: Detailed API documentation for the `wrapLanguageModel` function, including its parameters and return type. It describes how to configure the language model wrapping with middleware, custom IDs, and provider IDs.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/60-wrap-language-model.mdx#_snippet_2

LANGUAGE: APIDOC
CODE:
```
wrapLanguageModel(
  model: LanguageModelV2,
  middleware: LanguageModelV2Middleware | LanguageModelV2Middleware[],
  modelId?: string,
  providerId?: string
): LanguageModelV2

Parameters:
  model: LanguageModelV2
    description: The original LanguageModelV2 instance to be wrapped.
  middleware: LanguageModelV2Middleware | LanguageModelV2Middleware[]
    description: The middleware to be applied to the language model. When multiple middlewares are provided, the first middleware will transform the input first, and the last middleware will be wrapped directly around the model.
  modelId: string (optional)
    description: Optional custom model ID to override the original model's ID.
  providerId: string (optional)
    description: Optional custom provider ID to override the original model's provider.

Returns:
  LanguageModelV2
    description: A new LanguageModelV2 instance with middleware applied.
```

----------------------------------------

TITLE: Server-side Streamable Value Generation Example
DESCRIPTION: Demonstrates how to create and manage a streamable value on the server using `createStreamableValue`. This example shows updating the stream with multiple values before marking it as complete, making the value available for client-side consumption.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/07-reference/03-ai-sdk-rsc/05-read-streamable-value.mdx#_snippet_1

LANGUAGE: TypeScript
CODE:
```
async function generate() {
  'use server';
  const streamable = createStreamableValue();

  streamable.update(1);
  streamable.update(2);
  streamable.done(3);

  return streamable.value;
}
```

----------------------------------------

TITLE: AI SDK Tool Definition and Message Part API (APIDOC)
DESCRIPTION: Documentation for defining custom tools within the AI SDK, focusing on the `tool` function and its configuration options, as well as the structure of tool-related message parts.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-getting-started/02-nextjs-app-router.mdx#_snippet_12

LANGUAGE: APIDOC
CODE:
```
AI SDK Tool Definition:
  tool(options: object): Tool
    options:
      description: string
        Description: A human-readable description of the tool's purpose, used by the language model to determine when to call the tool.
      inputSchema: ZodSchema
        Description: A Zod schema defining the expected input parameters for the tool. The model will attempt to extract these inputs from the conversation context.
        Type: ZodSchema (e.g., z.object({ location: z.string() }))
      execute: (input: object) => Promise<any>
        Description: An asynchronous function that performs the core logic of the tool. This function runs on the server.
        Parameters:
          input: object - An object containing the validated input parameters, conforming to the `inputSchema`.
        Returns: Promise<any> - The result of the tool's execution. This output will be added to the conversation messages as a 'tool' message.

AI SDK Message Part Types for Tools:
  message.parts: Array<object>
    Description: An array within a message object that can contain various types of content, including tool call results.
    Tool Part Structure:
      type: string
        Value: ""tool-{toolName}"" (e.g., ""tool-weather"")
        Description: Identifies the part as a tool call result, where `{toolName}` is the key used when defining the tool.
      data: object
        Description: The actual data returned by the tool's `execute` function.
```

----------------------------------------

TITLE: Set Environment Variables for Vercel Deployment
DESCRIPTION: Configures essential environment variables for the Slack bot, including Slack API tokens and AI service keys, within Vercel project settings. These variables are crucial for the application's functionality.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/00-guides/03-slackbot.mdx#_snippet_10

LANGUAGE: bash
CODE:
```
SLACK_BOT_TOKEN=your_slack_bot_token
SLACK_SIGNING_SECRET=your_slack_signing_secret
OPENAI_API_KEY=your_openai_api_key
EXA_API_KEY=your_exa_api_key
```

----------------------------------------

TITLE: Convert useChat Messages to ModelMessages for AI Streaming
DESCRIPTION: Demonstrates how to use `convertToModelMessages` to prepare `useChat` messages for AI core functions like `streamText` with an OpenAI model. This example shows a typical API route handler for streaming text responses.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/07-reference/02-ai-sdk-ui/31-convert-to-model-messages.mdx#_snippet_0

LANGUAGE: ts
CODE:
```
import { openai } from '@ai-sdk/openai';
import { convertToModelMessages, streamText } from 'ai';

export async function POST(req: Request) {
  const { messages } = await req.json();

  const result = streamText({
    model: openai('gpt-4o'),
    messages: convertToModelMessages(messages),
  });

  return result.toUIMessageStreamResponse();
}
```

----------------------------------------

TITLE: Tool Result Part Format and Example
DESCRIPTION: Defines the structure and provides an example of a tool result. This part must be sent after the corresponding tool call part for the same tool call ID.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/04-ai-sdk-ui/50-stream-protocol.mdx#_snippet_7

LANGUAGE: APIDOC
CODE:
```
Format: a:{toolCallId:string; result:object}
Example: a:{""toolCallId"":""call-123"",""result"":""tool output""}
```

----------------------------------------

TITLE: useStreamableValue Hook API Reference
DESCRIPTION: Comprehensive API documentation for the `useStreamableValue` React hook, detailing its input parameters and the structure of its return value. It clarifies the types and purpose of the data, error, and pending status returned by the hook.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/07-reference/03-ai-sdk-rsc/11-use-streamable-value.mdx#_snippet_2

LANGUAGE: APIDOC
CODE:
```
useStreamableValue(streamableValue: StreamableValue): [data: any, error: Error | undefined, pending: boolean]

Parameters:
- streamableValue: A streamable value created using `createStreamableValue`.

Returns:
- [data, error, pending]: An array tuple.
  - data: The current value of the stream.
  - error: An `Error` object if an error occurred during the stream, otherwise `undefined`.
  - pending: A boolean indicating if the stream is still pending (true) or has completed (false).
```

----------------------------------------

TITLE: Create Customized xAI Provider Instance
DESCRIPTION: Create a customized xAI provider instance with specific settings like an API key.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/01-xai.mdx#_snippet_2

LANGUAGE: TypeScript
CODE:
```
import { createXai } from '@ai-sdk/xai';

const xai = createXai({
  apiKey: 'your-api-key'
});
```

----------------------------------------

TITLE: Implement GET Handler for Resuming Chat Streams (TypeScript)
DESCRIPTION: This TypeScript code defines a `GET` API handler for `/api/chat` that enables resuming chat streams. It extracts a `chatId` from the request query, loads associated stream IDs, and returns the latest one using `resumable-stream`. The handler includes validation for `chatId` presence and handles cases where no streams are found or the stream is already closed, falling back to an empty stream if necessary.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/04-ai-sdk-ui/03-chatbot-message-persistence.mdx#_snippet_12

LANGUAGE: ts
CODE:
```
import { loadStreams } from '@/util/chat-store';
import { createUIMessageStream, JsonToSseTransformStream } from 'ai';
import { after } from 'next/server';
import { createResumableStreamContext } from 'resumable-stream';

export async function GET(request: Request) {
  const streamContext = createResumableStreamContext({
    waitUntil: after,
  });

  const { searchParams } = new URL(request.url);
  const chatId = searchParams.get('chatId');

  if (!chatId) {
    return new Response('id is required', { status: 400 });
  }

  const streamIds = await loadStreams(chatId);

  if (!streamIds.length) {
    return new Response('No streams found', { status: 404 });
  }

  const recentStreamId = streamIds.at(-1);

  if (!recentStreamId) {
    return new Response('No recent stream found', { status: 404 });
  }

  const emptyDataStream = createUIMessageStream({
    execute: () => {},
  });

  return new Response(
    await streamContext.resumableStream(recentStreamId, () =>
      emptyDataStream.pipeThrough(new JsonToSseTransformStream()),
    ),
  );
}
```

----------------------------------------

TITLE: Integrate Exa for AI-Powered Web Search
DESCRIPTION: This TypeScript snippet demonstrates how to create a web search tool using the Exa API within an AI application. It defines a 'webSearch' tool that queries Exa for up-to-date information, processes the results, and integrates with an AI model (e.g., OpenAI's gpt-4o-mini) for text generation based on the search output. It requires an Exa API key and uses Zod for parameter validation.
SOURCE: https://github.com/vercel/ai/blob/v5/content/cookbook/05-node/56-web-search-agent.mdx#_snippet_3

LANGUAGE: typescript
CODE:
```
import { generateText, tool } from 'ai';
import { openai } from '@ai-sdk/openai';
import { z } from 'zod';
import Exa from 'exa-js';

export const exa = new Exa(process.env.EXA_API_KEY);

export const webSearch = tool({
  description: 'Search the web for up-to-date information',
  parameters: z.object({
    query: z.string().min(1).max(100).describe('The search query'),
  }),
  execute: async ({ query }) => {
    const { results } = await exa.searchAndContents(query, {
      livecrawl: 'always',
      numResults: 3,
    });
    return results.map(result => ({
      title: result.title,
      url: result.url,
      content: result.text.slice(0, 1000), // take just the first 1000 characters
      publishedDate: result.publishedDate,
    }));
  },
});

const { text } = await generateText({
  model: openai('gpt-4o-mini'), // can be any model that supports tools
  prompt: 'What happened in San Francisco last week?',
  tools: {
    webSearch,
  },
  maxSteps: 2,
});
```

----------------------------------------

TITLE: Vercel AI SDK Step/Response Object API Schema Definition
DESCRIPTION: This snippet provides a detailed API specification for a key data structure used in the Vercel AI SDK. It describes the various fields, their data types, whether they are optional, and their purpose, including nested objects for sources, files, usage, and metadata.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/02-stream-text.mdx#_snippet_20

LANGUAGE: APIDOC
CODE:
```
VercelAISDKStepOutput Object:
  sources: Array<Source> - (Inferred) A list of sources used by the model.
    Source Object:
      sourceType: 'url' (string) - A URL source. This is return by web search RAG models.
      id: string - The ID of the source.
      url: string - The URL of the source.
      title: string (optional) - The title of the source.
      providerMetadata: SharedV2ProviderMetadata (optional) - Additional provider metadata for the source.

  files: Array<GeneratedFile> - Files that were generated in this step.
    GeneratedFile Object:
      base64: string - File as a base64 encoded string.
      uint8Array: Uint8Array - File as a Uint8Array.
      mediaType: string - The IANA media type of the file.

  toolCalls: array - A list of tool calls made by the model.
  toolResults: array - A list of tool results returned as responses to earlier tool calls.
  finishReason: 'stop' | 'length' | 'content-filter' | 'tool-calls' | 'error' | 'other' | 'unknown' (string) - The reason the model finished generating the text.

  usage: LanguageModelUsage - The token usage of the generated text.
    LanguageModelUsage Object:
      promptTokens: number - The total number of
```

----------------------------------------

TITLE: `useChat` Hook Properties (APIDOC)
DESCRIPTION: API documentation for key properties returned by the `@ai-sdk/react` `useChat` hook, detailing their types, possible values, and purpose for building interactive chat UIs.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/02-chatbot.mdx#_snippet_5

LANGUAGE: APIDOC
CODE:
```
useChat Hook Properties:

status: string
  Description: The current state of the chat message processing.
  Possible Values:
    - ""submitted"": The message has been sent to the API and we're awaiting the start of the response stream.
    - ""streaming"": The response is actively streaming in from the API, receiving chunks of data.
    - ""ready"": The full response has been received and processed; a new user message can be submitted.
    - ""error"": An error occurred during the API request, preventing successful completion.

error: object | null
  Description: The error object thrown during the fetch request. It is null if no error occurred.
  Usage: Can be used to display an error message, disable UI elements, or show a retry button.
```

----------------------------------------

TITLE: Install AI SDK and OpenAI Provider for Next.js
DESCRIPTION: This command installs the necessary AI SDK packages, including the core SDK, OpenAI provider, and React bindings, using pnpm. These dependencies are required to build AI-powered applications with Next.js.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/00-guides/22-gpt-4-5.mdx#_snippet_3

LANGUAGE: bash
CODE:
```
pnpm install ai@beta @ai-sdk/openai@beta @ai-sdk/react@beta
```

----------------------------------------

TITLE: Configure API Request Options for useObject Hook
DESCRIPTION: Shows how to customize the API endpoint, add custom headers, and set credentials for the fetch request made by the `useObject` hook using the `api`, `headers`, and `credentials` options.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/04-ai-sdk-ui/08-object-generation.mdx#_snippet_7

LANGUAGE: tsx
CODE:
```
const { submit, object } = useObject({
  api: '/api/use-object',
  headers: {
    'X-Custom-Header': 'CustomValue',
  },
  credentials: 'include',
  schema: yourSchema,
});
```

----------------------------------------

TITLE: Configure Web Search with xAI Grok Model
DESCRIPTION: Example demonstrating how to configure web search with specific sources, country, allowed websites, and safe search settings.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/01-xai.mdx#_snippet_10

LANGUAGE: TypeScript
CODE:
```
const result = await generateText({
  model: xai('grok-3-latest'),
  prompt: 'Best ski resorts in Switzerland',
  providerOptions: {
    xai: {
      searchParameters: {
        mode: 'on',
        sources: [
          {
            type: 'web',
            country: 'CH', // ISO alpha-2 country code
            allowedWebsites: ['ski.com', 'snow-forecast.com'],
            safeSearch: true
          }
        ]
      }
    }
  }
});
```

----------------------------------------

TITLE: Create Server-Side Chat API Endpoint for Streaming with AI SDK
DESCRIPTION: This Next.js API route (`/api/chat`) handles incoming POST requests containing chat messages. It uses the `@ai-sdk/openai` and `ai` libraries to stream text responses from an OpenAI model (gpt-4o) based on the conversation history, returning the stream as UI messages.
SOURCE: https://github.com/vercel/ai/blob/v5/content/cookbook/01-next/21-stream-text-with-chat-prompt.mdx#_snippet_1

LANGUAGE: typescript
CODE:
```
import { streamText, UIMessage } from 'ai';
import { openai } from '@ai-sdk/openai';

export async function POST(req: Request) {
  const { messages }: { messages: UIMessage[] } = await req.json();

  const result = streamText({
    model: openai('gpt-4o'),
    system: 'You are a helpful assistant.',
    messages,
  });

  return result.toUIMessageStreamResponse();
}
```

----------------------------------------

TITLE: Vercel AI SDK API Configuration Parameters
DESCRIPTION: This section details optional parameters that can be provided to configure the behavior of API calls within the Vercel AI SDK. These parameters allow for custom fetch implementations, modification of request bodies, and throttling of updates for improved control and performance.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/07-reference/02-ai-sdk-ui/01-use-chat.mdx#_snippet_2

LANGUAGE: APIDOC
CODE:
```
API Configuration Options:
  - name: fetch
    type: Function
    isOptional: true
    description: A custom fetch function to be used for the API call. Defaults to the global fetch function.
  - name: experimental_prepareRequestBody
    type: (options: { messages: UIMessage[]; requestData?: JSONValue; requestBody?: object, chatId: string }) => unknown
    isOptional: true
    description: Experimental. When a function is provided, it will be used to prepare the request body for the chat API. This can be useful for customizing the request body based on the messages and data in the chat.
  - name: experimental_throttle
    type: number
    isOptional: true
    description: React only. Custom throttle wait time in milliseconds for the message and data updates. When specified, updates will be throttled using this interval. Defaults to undefined (no throttling).
```

----------------------------------------

TITLE: AI SDK RSC Core Functions API Reference
DESCRIPTION: Detailed descriptions of key functions provided by AI SDK RSC, categorized by their role in building Generative UI applications and managing streamable values between server and client.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/05-ai-sdk-rsc/01-overview.mdx#_snippet_0

LANGUAGE: APIDOC
CODE:
```
streamUI:
  description: Calls a model and allows it to respond with React Server Components.
useUIState:
  description: Returns the current UI state and a function to update the UI State (like React's `useState`). UI State is the visual representation of the AI state.
useAIState:
  description: Returns the current AI state and a function to update the AI State (like React's `useState`). The AI state is intended to contain context and information shared with the AI model, such as system messages, function responses, and other relevant data.
useActions:
  description: Provides access to your Server Actions from the client. This is particularly useful for building interfaces that require user interactions with the server.
createAI:
  description: Creates a client-server context provider that can be used to wrap parts of your application tree to easily manage both UI and AI states of your application.
createStreamableValue:
  description: Creates a stream that sends values from the server to the client. The value can be any serializable data.
readStreamableValue:
  description: Reads a streamable value from the client that was originally created using `createStreamableValue`.
createStreamableUI:
  description: Creates a stream that sends UI from the server to the client.
useStreamableValue:
  description: Accepts a streamable value created using `createStreamableValue` and returns the current value, error, and pending state.
```

----------------------------------------

TITLE: Tool Call Delta Part Format and Example
DESCRIPTION: Defines the structure and provides an example of a delta update for a streaming tool call, typically used to stream arguments incrementally.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/04-ai-sdk-ui/50-stream-protocol.mdx#_snippet_5

LANGUAGE: APIDOC
CODE:
```
Format: c:{toolCallId:string; argsTextDelta:string}
Example: c:{""toolCallId"":""call-456"",""argsTextDelta"":""partial arg""}
```

----------------------------------------

TITLE: Create Customized Together.ai Provider Instance
DESCRIPTION: Create a custom Together.ai provider instance with specific settings like an API key using `createTogetherAI`.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/24-togetherai.mdx#_snippet_2

LANGUAGE: typescript
CODE:
```
import { createTogetherAI } from '@ai-sdk/togetherai';

const togetherai = createTogetherAI({
  apiKey: process.env.TOGETHER_AI_API_KEY ?? '',
});
```

----------------------------------------

TITLE: Run Chatbot Application Locally
DESCRIPTION: This command starts the Vercel AI SDK chatbot application on a local development server, typically accessible via http://localhost:3000, allowing users to interact with the AI chatbot in their browser.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-getting-started/05-nuxt.mdx#_snippet_12

LANGUAGE: shell
CODE:
```
pnpm run dev
```

----------------------------------------

TITLE: useAIState Hook API Reference
DESCRIPTION: Detailed API documentation for the `useAIState` hook, outlining its signature and the structure of its return value. This hook provides access to the current AI state, which is shared globally.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/07-reference/03-ai-sdk-rsc/08-use-ai-state.mdx#_snippet_1

LANGUAGE: APIDOC
CODE:
```
useAIState(): [AIState]
  Returns:
    [AIState]: A single element array where the first element is the current AI state.
```

----------------------------------------

TITLE: API: X Source Parameters
DESCRIPTION: Defines the parameters available for configuring X (Twitter) search sources within the xai model's search functionality.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/01-xai.mdx#_snippet_14

LANGUAGE: APIDOC
CODE:
```
xHandles: string[] (Array of X handles to search, without @ symbol)
```

----------------------------------------

TITLE: Define a Tool with Type Inference using `tool()`
DESCRIPTION: This example demonstrates how to use the `tool()` helper function from the 'ai' library to define a tool. It showcases how `tool()` infers the types for the `execute` method's parameters based on the `parameters` schema defined using Zod, ensuring type safety for the `location` argument.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/07-reference/01-ai-sdk-core/20-tool.mdx#_snippet_0

LANGUAGE: TypeScript
CODE:
```
import { tool } from 'ai';
import { z } from 'zod';

export const weatherTool = tool({
  description: 'Get the weather in a location',
  parameters: z.object({
    location: z.string().describe('The location to get the weather for'),
  }),
  // location below is inferred to be a string:
  execute: async ({ location }) => ({
    location,
    temperature: 72 + Math.floor(Math.random() * 21) - 10,
  }),
});
```

----------------------------------------

TITLE: Generate Text with Baseten Model
DESCRIPTION: A complete example demonstrating how to use a Baseten model with the `generateText` function from the AI SDK. It initializes the provider and then generates a single text output based on a given prompt.
SOURCE: https://github.com/vercel/ai/blob/v5/content/providers/02-openai-compatible-providers/40-baseten.mdx#_snippet_3

LANGUAGE: typescript
CODE:
```
import { createOpenAICompatible } from '@ai-sdk/openai-compatible';
import { generateText } from 'ai';

const BASETEN_MODEL_ID = '<model-id>'; // e.g. 5q3z8xcw
const BASETEN_MODEL_URL = `https://model-${BASETEN_MODEL_ID}.api.baseten.co/environments/production/sync/v1`;

const baseten = createOpenAICompatible({
  name: 'baseten',
  baseURL: BASETEN_MODEL_URL,
  headers: {
    Authorization: `Bearer ${process.env.BASETEN_API_KEY ?? ''}`,
  },
});

const { text } = await generateText({
  model: baseten('llama'),
  prompt: 'Tell me about yourself in one sentence',
});

console.log(text);
```

----------------------------------------

TITLE: Text Editor Tool Parameters (APIDOC)
DESCRIPTION: API documentation for the parameters of the Anthropic Text Editor Tool's `execute` method, outlining various commands and their associated arguments for text manipulation.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/16-google-vertex.mdx#_snippet_43

LANGUAGE: APIDOC
CODE:
```
Text Editor Tool Parameters:
- command ('view' | 'create' | 'str_replace' | 'insert' | 'undo_edit'): The command to run.
- path (string): Absolute path to file or directory, e.g. /repo/file.py or /repo.
- file_text (string, optional): Required for create command, with the content of the file to be created.
- insert_line (number, optional): Required for insert command. The line number after which to insert the new string.
- new_str (string, optional): New string for str_replace or insert commands.
- old_str (string, optional): Required for str_replace command, containing the string to replace.
- view_range (number[], optional): Optional for view command to specify line range to show.
```

----------------------------------------

TITLE: Implement Sequential AI Generations with AI SDK
DESCRIPTION: This example demonstrates how to create a sequence of dependent AI generations using the `@ai-sdk/openai` and `ai` libraries. It shows a workflow where blog post ideas are generated, the best idea is selected, and then a detailed outline is created, with each step's output feeding into the next.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/06-advanced/09-sequential-generations.mdx#_snippet_0

LANGUAGE: typescript
CODE:
```
import { openai } from '@ai-sdk/openai';
import { generateText } from 'ai';

async function sequentialActions() {
  // Generate blog post ideas
  const ideasGeneration = await generateText({
    model: openai('gpt-4o'),
    prompt: 'Generate 10 ideas for a blog post about making spaghetti.',
  });

  console.log('Generated Ideas:\n', ideasGeneration);

  // Pick the best idea
  const bestIdeaGeneration = await generateText({
    model: openai('gpt-4o'),
    prompt: `Here are some blog post ideas about making spaghetti:
${ideasGeneration}

Pick the best idea from the list above and explain why it's the best.`,
  });

  console.log('\nBest Idea:\n', bestIdeaGeneration);

  // Generate an outline
  const outlineGeneration = await generateText({
    model: openai('gpt-4o'),
    prompt: `We've chosen the following blog post idea about making spaghetti:
${bestIdeaGeneration}

Create a detailed outline for a blog post based on this idea.`,
  });

  console.log('\nBlog Post Outline:\n', outlineGeneration);
}

sequentialActions().catch(console.error);
```

----------------------------------------

TITLE: Process File Inputs with AI SDK Google Vertex Provider
DESCRIPTION: This example illustrates how to send file inputs, such as PDF documents, to a Gemini model using the AI SDK's Google Vertex provider. It demonstrates structuring the `messages` array to include both text and file parts, specifying the file path and media type. The AI SDK can automatically download URLs, but `gs://` URLs require the Google Cloud Storage API.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/16-google-vertex.mdx#_snippet_13

LANGUAGE: ts
CODE:
```
import { vertex } from '@ai-sdk/google-vertex';
import { generateText } from 'ai';

const { text } = await generateText({
  model: vertex('gemini-1.5-pro'),
  messages: [
    {
      role: 'user',
      content: [
        {
          type: 'text',
          text: 'What is an embedding model according to this document?',
        },
        {
          type: 'file',
          data: fs.readFileSync('./data/ai.pdf'),
          mediaType: 'application/pdf',
        }
      ]
    }
  ]
});
```

----------------------------------------

TITLE: Generate Text with DeepSeek Chat Model
DESCRIPTION: Example of using the `deepseek` provider with `generateText` to create text content, specifying a model like 'deepseek-chat'.
SOURCE: https://github.com/vercel/ai/blob/v5/content/providers/01-ai-sdk-providers/30-deepseek.mdx#_snippet_4

LANGUAGE: ts
CODE:
```
import { deepseek } from '@ai-sdk/deepseek';
import { generateText } from 'ai';

const { text } = await generateText({
  model: deepseek('deepseek-chat'),
  prompt: 'Write a vegetarian lasagna recipe for 4 people.'
});
```

----------------------------------------

TITLE: Migrate Stream Protocol to Server-Sent Events in AI SDK
DESCRIPTION: This example illustrates the transition from a proprietary data stream protocol to Server-Sent Events (SSE) in AI SDK 5.0, showing the updated API for creating and writing to streams.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/08-migration-guides/26-migration-guide-5-0.mdx#_snippet_56

LANGUAGE: tsx
CODE:
```
import { createDataStream, formatDataStreamPart } from 'ai';

const dataStream = createDataStream({
  execute: writer => {
    writer.writeData('initialized call');
    writer.write(formatDataStreamPart('text', 'Hello'));
    writer.writeSource({
      type: 'source',
      sourceType: 'url',
      id: 'source-1',
      url: 'https://example.com',
      title: 'Example Source',
    });
  },
});
```

LANGUAGE: tsx
CODE:
```
import { createUIMessageStream } from 'ai';

const stream = createUIMessageStream({
  execute: ({ writer }) => {
    writer.write({ type: 'data', value: ['initialized call'] });
    writer.write({ type: 'text', value: 'Hello' });
    writer.write({
      type: 'source-url',
      value: {
        type: 'source',
        id: 'source-1',
        url: 'https://example.com',
        title: 'Example Source',
      },
    });
  },
});
```

----------------------------------------

TITLE: Generate AI Responses with Tool Calls using Vercel AI SDK and Mem0
DESCRIPTION: Demonstrates how to use the Vercel AI SDK's `generateText` function with a Mem0-powered model and custom tools. It illustrates defining a 'weather' tool with Zod for parameter validation and executing it based on a user prompt, showcasing dynamic interaction capabilities.
SOURCE: https://github.com/vercel/ai/blob/v5/content/providers/03-community-providers/70-mem0.mdx#_snippet_8

LANGUAGE: typescript
CODE:
```
import { generateText } from 'ai';
import { createMem0 } from '@mem0/vercel-ai-provider';
import { z } from 'zod';

const mem0 = createMem0({
  provider: 'anthropic',
  apiKey: 'anthropic-api-key',
  mem0Config: {
    // Global User ID
    user_id: 'borat',
  },
});

const prompt = 'What the temperature in the city that I live in?';

const result = await generateText({
  model: mem0('claude-3-5-sonnet-20240620'),
  tools: {
    weather: tool({
      description: 'Get the weather in a location',
      parameters: z.object({
        location: z.string().describe('The location to get the weather for'),
      }),
      execute: async ({ location }) => ({
        location,
        temperature: 72 + Math.floor(Math.random() * 21) - 10,
      }),
    }),
  },
  prompt: prompt,
});

console.log(result);
```

----------------------------------------

TITLE: Configure OpenAI-Compatible Client for FriendliAI Serverless
DESCRIPTION: Explains how to configure `@ai-sdk/openai` to work with FriendliAI's serverless endpoints. It sets the `baseURL` to the FriendliAI serverless API and uses an API key for authentication.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/08-friendliai.mdx#_snippet_12

LANGUAGE: typescript
CODE:
```
import { createOpenAI } from '@ai-sdk/openai';

const friendli = createOpenAI({
  baseURL: 'https://api.friendli.ai/serverless/v1',
  apiKey: process.env.FRIENDLI_TOKEN,
});
```

----------------------------------------

TITLE: Using a Custom AI SDK Provider
DESCRIPTION: This snippet demonstrates how to import and use a custom AI SDK provider (e.g., '@company-name/example') with the `generateText` function. It shows how to pass a model instance from the custom provider and a prompt to generate text.
SOURCE: https://github.com/vercel/ai/blob/v5/content/providers/02-openai-compatible-providers/01-custom-providers.mdx#_snippet_7

LANGUAGE: ts
CODE:
```
import { example } from '@company-name/example';
import { generateText } from 'ai';

const { text } = await generateText({
  model: example('example/chat-model-1'),
  prompt: 'Hello, how are you?',
});
```

----------------------------------------

TITLE: Client-side Reading of Streamable Value Example
DESCRIPTION: Illustrates how to use `readStreamableValue` on the client to process an asynchronous stream received from the server. The example iterates over the stream's deltas and appends them to a state variable, demonstrating real-time UI updates.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/07-reference/03-ai-sdk-rsc/05-read-streamable-value.mdx#_snippet_2

LANGUAGE: TypeScript
CODE:
```
import { readStreamableValue } from '@ai-sdk/rsc';

export default function Page() {
  const [generation, setGeneration] = useState('');

  return (
    <div>
      <button
        onClick={async () => {
          const stream = await generate();

          for await (const delta of readStreamableValue(stream)) {
            setGeneration(generation => generation + delta);
          }
        }}
      >
        Generate
      </button>
    </div>
  );
}
```

----------------------------------------

TITLE: Importing simulateStreamingMiddleware
DESCRIPTION: Demonstrates how to import and initialize the `simulateStreamingMiddleware` function from the 'ai' library for use in an application.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/07-reference/01-ai-sdk-core/67-simulate-streaming-middleware.mdx#_snippet_0

LANGUAGE: ts
CODE:
```
import { simulateStreamingMiddleware } from 'ai';

const middleware = simulateStreamingMiddleware();
```

----------------------------------------

TITLE: Next.js Client Component for Chat UI with useChat Hook
DESCRIPTION: This React component (app/page.tsx) demonstrates building a chat interface using the useChat hook from @ai-sdk/react. It manages chat messages, user input, and submission, displaying real-time AI responses and user queries.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/00-guides/22-gpt-4-5.mdx#_snippet_5

LANGUAGE: typescript
CODE:
```
'use client';

import { useChat } from '@ai-sdk/react';

export default function Page() {
  const { messages, input, handleInputChange, handleSubmit, error } = useChat();

  return (
    <>
      {messages.map(message => (
        <div key={message.id}>
          {message.role === 'user' ? 'User: ' : 'AI: '}
          {message.content}
        </div>
      ))}
      <form onSubmit={handleSubmit}>
        <input name=""prompt"" value={input} onChange={handleInputChange} />
        <button type=""submit"">Submit</button>
      </form>
    </>
  );
}
```

----------------------------------------

TITLE: Define AI Prompt for SQL Query Explanation
DESCRIPTION: This prompt guides an AI model (Postgres expert) to explain SQL queries by breaking them into logical sections. It includes the database schema for 'unicorns' table and provides an example of how to structure the explanation, emphasizing unique sections.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/00-guides/04-natural-language-postgres.mdx#_snippet_10

LANGUAGE: txt
CODE:
```
You are a SQL (postgres) expert. Your job is to explain to the user write a SQL query you wrote to retrieve the data they asked for. The table schema is as follows:
unicorns (
  id SERIAL PRIMARY KEY,
  company VARCHAR(255) NOT NULL UNIQUE,
  valuation DECIMAL(10, 2) NOT NULL,
  date_joined DATE,
  country VARCHAR(255) NOT NULL,
  city VARCHAR(255) NOT NULL,
  industry VARCHAR(255) NOT NULL,
  select_investors TEXT NOT NULL
);

When you explain you must take a section of the query, and then explain it. Each ""section"" should be unique. So in a query like: ""SELECT * FROM unicorns limit 20"", the sections could be ""SELECT *"", ""FROM UNICORNS"", ""LIMIT 20"".
If a section doesn't have any explanation, include it, but leave the explanation empty.
```

----------------------------------------

TITLE: Configure Example Navigation Links in JSX
DESCRIPTION: This JSX snippet defines the data structure used to populate navigation links for various Vercel AI SDK examples. It utilizes the `ExampleLinks` component, passing an array of objects where each object specifies a `title` and `link` to a specific demonstration, categorizing them by functionality and framework (Next.js or Node.js).
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/07-reference/01-ai-sdk-core/01-generate-text.mdx#_snippet_14

LANGUAGE: JSX
CODE:
```
<ExampleLinks
  examples={[
    {
      title: 'Learn to generate text using a language model in Next.js',
      link: '/examples/next-app/basics/generating-text',
    },
    {
      title:
        'Learn to generate a chat completion using a language model in Next.js',
      link: '/examples/next-app/basics/generating-text',
    },
    {
      title: 'Learn to call tools using a language model in Next.js',
      link: '/examples/next-app/tools/call-tool',
    },
    {
      title:
        'Learn to render a React component as a tool call using a language model in Next.js',
      link: '/examples/next-app/tools/render-interface-during-tool-call',
    },
    {
      title: 'Learn to generate text using a language model in Node.js',
      link: '/examples/node/generating-text/generate-text',
    },
    {
      title:
        'Learn to generate chat completions using a language model in Node.js',
      link: '/examples/node/generating-text/generate-text-with-chat-prompt',
    },
  ]}
/>
```

----------------------------------------

TITLE: Server-Side Text Generation API Route in Next.js
DESCRIPTION: This Next.js API route (`/api/completion`) handles incoming POST requests for text generation. It extracts the prompt from the request body, utilizes the `generateText` function from the `ai` module with an OpenAI model (gpt-4) to generate a response, and then returns the generated text as a JSON object. It's configured as a helpful assistant.
SOURCE: https://github.com/vercel/ai/blob/v5/content/cookbook/01-next/10-generate-text.mdx#_snippet_1

LANGUAGE: typescript
CODE:
```
import { generateText } from 'ai';
import { openai } from '@ai-sdk/openai';

export async function POST(req: Request) {
  const { prompt }: { prompt: string } = await req.json();

  const { text } = await generateText({
    model: openai('gpt-4'),
    system: 'You are a helpful assistant.',
    prompt,
  });

  return Response.json({ text });
}
```

----------------------------------------

TITLE: Customize Google Vertex Anthropic Provider for Edge Runtime
DESCRIPTION: This example shows how to customize the Google Vertex Anthropic provider for Edge Runtimes using `createVertexAnthropic`. It allows specifying optional `project` and `location` settings, providing more control over the API calls within edge environments.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/16-google-vertex.mdx#_snippet_32

LANGUAGE: TypeScript
CODE:
```
import { createVertexAnthropic } from '@ai-sdk/google-vertex/anthropic/edge';

const vertexAnthropic = createVertexAnthropic({
  project: 'my-project', // optional
  location: 'us-central1', // optional
});
```

----------------------------------------

TITLE: Initial Meal Logging Interaction
DESCRIPTION: This example demonstrates a user's request to log a meal, showing how the language model translates the request into a `log_meal` tool call with specific parameters and provides a confirmation message.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/06-advanced/09-multistep-interfaces.mdx#_snippet_0

LANGUAGE: txt
CODE:
```
User: Log a chicken shawarma for lunch.
Tool: log_meal(""chicken shawarma"", ""250g"", ""12:00 PM"")
Model: Chicken shawarma has been logged for lunch.
```

----------------------------------------

TITLE: Bootstrap Next.js AI Chat Application
DESCRIPTION: Commands to initialize a new Next.js project using the `create-next-app` utility, specifically cloning the `next-langchain` example from the Vercel AI repository. This sets up the foundational structure for the AI chat application and can be executed with npm, Yarn, or pnpm.
SOURCE: https://github.com/vercel/ai/blob/main/examples/next-langchain/README.md#_snippet_0

LANGUAGE: bash
CODE:
```
npx create-next-app --example https://github.com/vercel/ai/tree/main/examples/next-langchain next-langchain-app
```

LANGUAGE: bash
CODE:
```
yarn create next-app --example https://github.com/vercel/ai/tree/main/examples/next-langchain next-langchain-app
```

LANGUAGE: bash
CODE:
```
pnpm create next-app --example https://github.com/vercel/ai/tree/main/examples/next-langchain next-langchain-app
```

----------------------------------------

TITLE: Migrate AI SDK provider options to Responses API
DESCRIPTION: Illustrates the migration of provider-specific options from the Completions API to the Responses API. It shows how options like `parallelToolCalls` are now specified within the `providerOptions` object for the Responses API.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/00-guides/19-openai-responses.mdx#_snippet_7

LANGUAGE: ts
CODE:
```
import { generateText } from 'ai';
import { openai } from '@ai-sdk/openai';

// Completions API
const { text } = await generateText({
  model: openai('gpt-4o'),
  prompt: 'Explain the concept of quantum entanglement.',
  providerOptions: {
    openai: {
      parallelToolCalls: false,
    },
  },
});

// Responses API
const { text } = await generateText({
  model: openai.responses('gpt-4o'),
  prompt: 'Explain the concept of quantum entanglement.',
  providerOptions: {
    openai: {
      parallelToolCalls: false,
    },
  },
});
```

----------------------------------------

TITLE: Vercel AI Model Input Parameters, Experimental Output, and onStepFinish Callback
DESCRIPTION: Defines the structure for various input parameters used in Vercel AI model interactions, including system prompts, messages, tool handling, and error reporting. It also details experimental output settings and the comprehensive `onStepFinish` callback for monitoring generation steps.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/07-reference/01-ai-sdk-core/01-generate-text.mdx#_snippet_8

LANGUAGE: APIDOC
CODE:
```
// API Configuration Properties

// Input/Context Parameters:
- name: 'system'
  type: 'string | undefined'
  description: 'The system prompt.'

- name: 'messages'
  type: 'ModelMessage[]'
  description: 'The messages in the current generation step.'

- name: 'toolCall'
  type: 'LanguageModelV2ToolCall'
  description: 'The tool call that failed to parse.'

- name: 'tools'
  type: 'TOOLS'
  description: 'The tools that are available.'

- name: 'parameterSchema'
  type: '(options: { toolName: string }) => JSONSchema7'
  description: 'A function that returns the JSON Schema for a tool.'

- name: 'error'
  type: 'NoSuchToolError | InvalidToolArgumentsError'
  description: 'The error that occurred while parsing the tool call.'

// Experimental Output Configuration:
- name: 'experimental_output'
  type: 'Output'
  isOptional: true
  description: 'Experimental setting for generating structured outputs.'
  properties:
    - type: 'Output'
      parameters:
        - name: 'Output.text()'
          type: 'Output'
          description: 'Forward text output.'
        - name: 'Output.object()'
          type: 'Output'
          description: 'Generate a JSON object of type OBJECT.'
          properties:
            - type: 'Options'
              parameters:
                - name: 'schema'
                  type: 'Schema<OBJECT>'
                  description: 'The schema of the JSON object to generate.'

// Step Finish Callback:
- name: 'onStepFinish'
  type: '(result: OnStepFinishResult) => Promise<void> | void'
  isOptional: true
  description: 'Callback that is called when a step is finished.'
  properties:
    - type: 'OnStepFinishResult'
      parameters:
        - name: 'stepType'
          type: '""initial"" | ""continue"" | ""tool-result""'
          description: 'The type of step. The first step is always an ""initial"" step, and subsequent steps are either ""continue"" steps or ""tool-result"" steps.'
        - name: 'finishReason'
          type: '""stop"" | ""length"" | ""content-filter"" | ""tool-calls"" | ""error"" | ""other"" | ""unknown""'
          description: 'The reason the model finished generating the text for the step.'
        - name: 'usage'
          type: 'TokenUsage'
          description: 'The token usage of the step.'
          properties:
            - type: 'TokenUsage'
              parameters:
                - name: 'promptTokens'
                  type: 'number'
                  description: 'The total number of tokens in the prompt.'
                - name: 'completionTokens'
                  type: 'number'
                  description: 'The total number of tokens in the completion.'
                - name: 'totalTokens'
                  type: 'number'
                  description: 'The total number of tokens generated.'
        - name: 'text'
          type: 'string'
          description: 'The full text that has been generated.'
        - name: 'toolCalls'
          type: 'ToolCall[]'
          description: 'The tool calls that have been executed.'
        - name: 'toolResults'
          type: 'ToolResult[]'
          description: 'The tool results that have been generated.'
        - name: 'warnings'
          type: 'Warning[] | undefined'
          description: 'Warnings from the model provider (e.g. unsupported settings).'
        - name: 'response'
          type: 'Response'
          isOptional: true
          description: 'Response metadata.'
          properties:
            - type: 'Response'
              parameters:
                // ... (further parameters for Response are truncated in the input)
```

----------------------------------------

TITLE: Configure OpenAI-Compatible Client for FriendliAI Dedicated Endpoints
DESCRIPTION: Shows how to configure `@ai-sdk/openai` to work with FriendliAI's dedicated endpoints. It sets the `baseURL` to the FriendliAI dedicated API and uses an API key for authentication.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/08-friendliai.mdx#_snippet_13

LANGUAGE: typescript
CODE:
```
import { createOpenAI } from '@ai-sdk/openai';

const friendli = createOpenAI({
  baseURL: 'https://api.friendli.ai/dedicated/v1',
  apiKey: process.env.FRIENDLI_TOKEN,
});
```

----------------------------------------

TITLE: Create Fireworks Completion Model Instance
DESCRIPTION: Shows how to create a model instance specifically for the Fireworks completions API using the `.completionModel()` factory method.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/26-fireworks.mdx#_snippet_7

LANGUAGE: TypeScript
CODE:
```
const model = fireworks.completionModel(
  'accounts/fireworks/models/firefunction-v1',
);
```

----------------------------------------

TITLE: jsonSchema Function API Signature
DESCRIPTION: Detailed API documentation for the `jsonSchema` helper function, outlining its parameters (`schema` and an optional `options` object with a nested `validate` function) and its return type. This section clarifies the expected inputs and the structure of the returned JSON schema object.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/07-reference/01-ai-sdk-core/25-json-schema.mdx#_snippet_2

LANGUAGE: APIDOC
CODE:
```
jsonSchema(schema: JSONSchema7, options?: SchemaOptions): JSON schema object compatible with AI SDK
  schema: The JSON schema definition.
  options: Additional options for the JSON schema.
    validate?: (value: unknown) => { success: true; value: OBJECT } | { success: false; error: Error };
      A function that validates the value against the JSON schema. If the value is valid, the function should return an object with a `success` property set to `true` and a `value` property set to the validated value. If the value is invalid, the function should return an object with a `success` property set to `false` and an `error` property set to the error.
```

----------------------------------------

TITLE: Configure Letta API Key
DESCRIPTION: Sets up the `LETTA_API_KEY` in a `.env.local` file for authenticating with the Letta Cloud API. This key is essential for accessing Letta's services.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/71-letta.mdx#_snippet_2

LANGUAGE: text
CODE:
```
LETTA_API_KEY=<your api key>
```

----------------------------------------

TITLE: AI SDK `UIMessage` and `ModelMessage` Type Definitions
DESCRIPTION: API documentation explaining the distinct purposes and structures of `UIMessage` and `ModelMessage` types within the AI SDK. `UIMessage` is designed for UI display with metadata, while `ModelMessage` is a stripped-down version expected by language models.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-getting-started/05-nuxt.mdx#_snippet_11

LANGUAGE: APIDOC
CODE:
```
UIMessage[]: Designed for application UI, includes entire message history and metadata like timestamps.
ModelMessage[]: Expected by models, does not include UI-specific metadata (e.g., timestamps, sender info). Conversion from UIMessage[] to ModelMessage[] is done via `convertToModelMessages`.
```

----------------------------------------

TITLE: Stream Text with Requesty and AI SDK
DESCRIPTION: Illustrates how to use Requesty with the AI SDK's `streamText` function for real-time text generation. The example iterates over the `textStream` to log chunks as they arrive.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/5-requesty.mdx#_snippet_5

LANGUAGE: javascript
CODE:
```
import { requesty } from '@requesty/ai-sdk';
import { streamText } from 'ai';

const result = streamText({
  model: requesty('anthropic/claude-3.5-sonnet'),
  prompt: 'Write a short story about AI.',
});

for await (const chunk of result.textStream) {
  console.log(chunk);
}
```

----------------------------------------

TITLE: StreamingTextResponse API Reference
DESCRIPTION: Detailed API documentation for the StreamingTextResponse class, including its constructor parameters and return type. This class is deprecated in AI SDK 4.0, with a recommendation to use `streamText.toDataStreamResponse()` instead.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/07-reference/04-stream-helpers/02-streaming-text-response.mdx#_snippet_1

LANGUAGE: APIDOC
CODE:
```
StreamingTextResponse:
  __init__(stream: ReadableStream, init?: ResponseInit, data?: StreamData)
    stream: ReadableStream
      description: The stream of content which represents the HTTP response.
    init?: ResponseInit
      description: It can be used to customize the properties of the HTTP response. It is an object that corresponds to the ResponseInit object used in the Response constructor.
      properties:
        status?: number
          description: The status code for the response. StreamingTextResponse will overwrite this value with 200.
        statusText?: string
          description: The status message associated with the status code.
        headers?: HeadersInit
          description: Any headers you want to add to your response. StreamingTextResponse will add 'Content-Type': 'text/plain; charset=utf-8' to these headers.
    data?: StreamData
      description: StreamData object that you are using to generate additional data for the response.
  Returns: Response
    description: An instance of Response with the provided ReadableStream as the body, the status set to 200, and the Content-Type header set to 'text/plain; charset=utf-8'. Additional headers and properties can be added using the init parameter.
```

----------------------------------------

TITLE: Migration Guide for @ai-sdk/amazon-bedrock 2.x API Changes
DESCRIPTION: Details the breaking changes introduced in version 2.x of `@ai-sdk/amazon-bedrock`, specifically the removal of the `bedrockOptions` object. It advises direct usage of `region`, `accessKeyId`, `secretAccessKey`, and `sessionToken` and notes the importance of explicit `undefined` assignment for unused parameters in serverless environments.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/08-amazon-bedrock.mdx#_snippet_31

LANGUAGE: APIDOC
CODE:
```
Version: 2.x
Change: Removed dependency on @aws-sdk/client-bedrock-runtime.
API Impact:
  - `bedrockOptions` provider setting: Removed.
  - Replacement: Use `region`, `accessKeyId`, `secretAccessKey`, `sessionToken` directly.
  - Note: Explicitly set unused parameters (e.g., `sessionToken`) to `undefined` to avoid conflicts with environment variables in serverless environments.
```

----------------------------------------

TITLE: Send PDF Files to OpenAI Chat API
DESCRIPTION: Illustrates how to include PDF files in messages sent to the OpenAI Chat API. It demonstrates using the `type: 'file'` content block, providing file `data` (e.g., from `fs.readFileSync`), `mediaType`, and an optional `filename`.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/03-openai.mdx#_snippet_16

LANGUAGE: ts
CODE:
```
const result = await generateText({
  model: openai('gpt-4o'),
  messages: [
    {
      role: 'user',
      content: [
        {
          type: 'text',
          text: 'What is an embedding model?',
        },
        {
          type: 'file',
          data: fs.readFileSync('./data/ai.pdf'),
          mediaType: 'application/pdf',
          filename: 'ai.pdf', // optional
        },
      ],
    },
  ],
});
```

----------------------------------------

TITLE: Generate Text with DeepSeek Model
DESCRIPTION: This example demonstrates how to use the DeepSeek provider with the AI SDK's `generateText` function. It initializes a DeepSeek chat model and sends a prompt to generate text.
SOURCE: https://github.com/vercel/ai/blob/main/packages/deepseek/README.md#_snippet_2

LANGUAGE: ts
CODE:
```
import { deepseek } from '@ai-sdk/deepseek';
import { generateText } from 'ai';

const { text } = await generateText({
  model: deepseek('deepseek-chat'),
  prompt: 'Write a JavaScript function that sorts a list:',
});
```

----------------------------------------

TITLE: Generate Text with Claude 4 Sonnet using AI SDK
DESCRIPTION: This snippet demonstrates a basic text generation call to Claude 4 Sonnet using the AI SDK. It imports the `anthropic` provider and `generateText` function, then calls the model with a prompt and logs the generated text.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/00-guides/18-claude-4.mdx#_snippet_0

LANGUAGE: ts
CODE:
```
import { anthropic } from '@ai-sdk/anthropic';
import { generateText } from 'ai';

const { text, reasoning, reasoningDetails } = await generateText({
  model: anthropic('claude-4-sonnet-20250514'),
  prompt: 'How will quantum computing impact cryptography by 2050?',
});
console.log(text);
```

----------------------------------------

TITLE: Slack Event Handler API Route (TypeScript)
DESCRIPTION: This TypeScript function defines the POST endpoint for handling incoming Slack events. It verifies requests, processes different event types like url_verification, app_mention, assistant_thread_started, and message, and dispatches them to appropriate handlers. The waitUntil function is used for asynchronous processing.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/00-guides/03-slackbot.mdx#_snippet_2

LANGUAGE: typescript
CODE:
```
import type { SlackEvent } from '@slack/web-api';
import {
  assistantThreadMessage,
  handleNewAssistantMessage,
} from '../lib/handle-messages';
import { waitUntil } from '@vercel/functions';
import { handleNewAppMention } from '../lib/handle-app-mention';
import { verifyRequest, getBotId } from '../lib/slack-utils';

export async function POST(request: Request) {
  const rawBody = await request.text();
  const payload = JSON.parse(rawBody);
  const requestType = payload.type as 'url_verification' | 'event_callback';

  // See https://api.slack.com/events/url_verification
  if (requestType === 'url_verification') {
    return new Response(payload.challenge, { status: 200 });
  }

  await verifyRequest({ requestType, request, rawBody });

  try {
    const botUserId = await getBotId();

    const event = payload.event as SlackEvent;

    if (event.type === 'app_mention') {
      waitUntil(handleNewAppMention(event, botUserId));
    }

    if (event.type === 'assistant_thread_started') {
      waitUntil(assistantThreadMessage(event));
    }

    if (
      event.type === 'message' &&
      !event.subtype &&
      event.channel_type === 'im' &&
      !event.bot_id &&
      !event.bot_profile &&
      event.bot_id !== botUserId
    ) {
      waitUntil(handleNewAssistantMessage(event, botUserId));
    }

    return new Response('Success!', { status: 200 });
  } catch (error) {
    console.error('Error generating response', error);
    return new Response('Error generating response', { status: 500 });
  }
}
```

----------------------------------------

TITLE: Install Dependencies and Build Project
DESCRIPTION: Installs project dependencies using pnpm and then builds the project. These steps are necessary to prepare the AI SDK repository for execution.
SOURCE: https://github.com/vercel/ai/blob/main/examples/mcp/README.md#_snippet_1

LANGUAGE: sh
CODE:
```
pnpm install
pnpm build
```

----------------------------------------

TITLE: Luma Provider Instance Configuration Options
DESCRIPTION: Detailed documentation of the optional settings available when creating a custom Luma provider instance, including `baseURL`, `apiKey`, `headers`, and `fetch`.
SOURCE: https://github.com/vercel/ai/blob/v5/content/providers/01-ai-sdk-providers/80-luma.mdx#_snippet_3

LANGUAGE: APIDOC
CODE:
```
createLuma(options?: object): LumaProvider
  options:
    apiKey: string (optional)
      Description: API key that is being sent using the `Authorization` header. Defaults to the `LUMA_API_KEY` environment variable.
    baseURL: string (optional)
      Description: Use a different URL prefix for API calls, e.g. to use proxy servers. The default prefix is `https://api.lumalabs.ai`.
    headers: Record<string, string> (optional)
      Description: Custom headers to include in the requests.
    fetch: (input: RequestInfo, init?: RequestInit) => Promise<Response> (optional)
      Description: Custom fetch implementation. Can be used as a middleware to intercept requests or to provide a custom fetch implementation for testing.
```

----------------------------------------

TITLE: Next.js API Route for AI Tool Integration
DESCRIPTION: This TypeScript code defines a Next.js API route (`/api/chat`) that integrates an AI tool. It uses `@ai-sdk/openai` to stream text, converts UI messages, and includes a `tools` array for tool invocation. The `streamText` function processes incoming messages and returns a UI message stream response, enabling the AI to use defined tools.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/04-generative-user-interfaces.mdx#_snippet_3

LANGUAGE: TypeScript
CODE:
```
import { openai } from '@ai-sdk/openai';
import { streamText, convertToModelMessages, UIMessage } from 'ai';
import { tools } from '@/ai/tools';

export async function POST(request: Request) {
  const { messages }: { messages: UIMessage[] } = await request.json();

  const result = streamText({
    model: openai('gpt-4o'),
    system: 'You are a friendly assistant!',
    messages: convertToModelMessages(messages),
    maxSteps: 5,
    tools,
  });

  return result.toUIMessageStreamResponse();
}
```

----------------------------------------

TITLE: Implement Chat UI with useChat Hook in Next.js
DESCRIPTION: This TSX client component (`app/page.tsx`) demonstrates how to build an interactive chat interface using the `useChat` hook from `@ai-sdk/react`. It manages chat messages, input, and submission, connecting to a backend AI endpoint to display real-time AI responses.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/00-guides/24-o3.mdx#_snippet_6

LANGUAGE: tsx
CODE:
```
'use client';

import { useChat } from '@ai-sdk/react';

export default function Page() {
  const { messages, input, handleInputChange, handleSubmit, error } = useChat();

  return (
    <>
      {messages.map(message => (
        <div key={message.id}>
          {message.role === 'user' ? 'User: ' : 'AI: '}
          {message.content}
        </div>
      ))}
      <form onSubmit={handleSubmit}>
        <input name=""prompt"" value={input} onChange={handleInputChange} />
        <button type=""submit"">Submit</button>
      </form>
    </>
  );
}
```

----------------------------------------

TITLE: Use DeepSeek Reasoner Model
DESCRIPTION: Demonstrates how to use the `deepseek-reasoner` model for tasks requiring reasoning, capturing both the generated text and the reasoning output.
SOURCE: https://github.com/vercel/ai/blob/v5/content/providers/01-ai-sdk-providers/30-deepseek.mdx#_snippet_5

LANGUAGE: ts
CODE:
```
import { deepseek } from '@ai-sdk/deepseek';
import { generateText } from 'ai';

const { text, reasoning } = await generateText({
  model: deepseek('deepseek-reasoner'),
  prompt: 'How many people will live in the world in 2040?'
});

console.log(reasoning);
console.log(text);
```

----------------------------------------

TITLE: Vercel AI SDK Core API Options and Callbacks
DESCRIPTION: Defines the structure for configuration options and callback functions within the Vercel AI SDK, including details for provider-specific settings, error handling, and response finalization. It outlines the types and descriptions for parameters, nested objects like token usage, and response metadata.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/04-stream-object.mdx#_snippet_9

LANGUAGE: APIDOC
CODE:
```
- providerOptions: Record<string,Record<string,JSONValue>> | undefined (Optional)\n  Description: Provider-specific options. The outer key is the provider name. The inner values are the metadata. Details depend on the provider.\n\n- onError: (event: OnErrorResult) => Promise<void> |void (Optional)\n  Description: Callback that is called when an error occurs during streaming. You can use it to log errors.\n  Structure of OnErrorResult:\n    - error: unknown\n      Description: The error that occurred.\n\n- onFinish: (result: OnFinishResult) => void (Optional)\n  Description: Callback that is called when the LLM response has finished.\n  Structure of OnFinishResult:\n    - usage: LanguageModelUsage\n      Description: The token usage of the generated text.\n      Structure of LanguageModelUsage:\n        - inputTokens: number | undefined\n          Description: The number of input (prompt) tokens used.\n\n        - outputTokens: number | undefined\n          Description: The number of output (completion) tokens used.\n\n        - totalTokens: number | undefined\n          Description: The total number of tokens as reported by the provider. This number might be different from the sum of inputTokens and outputTokens and e.g. include reasoning tokens or other overhead.\n\n        - reasoningTokens: number | undefined (Optional)\n          Description: The number of reasoning tokens used.\n\n        - cachedInputTokens: number | undefined (Optional)\n          Description: The number of cached input tokens.\n\n    - providerMetadata: ProviderMetadata | undefined\n      Description: Optional metadata from the provider. The outer key is the provider name. The inner values are the metadata. Details depend on the provider.\n\n    - object: T | undefined\n      Description: The generated object (typed according to the schema). Can be undefined if the final object does not match the schema.\n\n    - error: unknown | undefined\n      Description: Optional error object. This is e.g. a TypeValidationError when the final object does not match the schema.\n\n    - warnings: CallWarning[] | undefined\n      Description: Warnings from the model provider (e.g. unsupported settings).\n\n    - response: Response (Optional)\n      Description: Response metadata.\n      Structure of Response:\n        - id: string\n          Description: The response identifier. The AI SDK uses the ID from the provider response when available, and generates an ID otherwise.\n\n        - model: string\n          Description: The model that was used to generate the response. The AI SDK uses the response model from the provider response when available, and the model from the function call otherwise.\n\n        - timestamp: Date
```

----------------------------------------

TITLE: Call Server Action to Stream UI Components on Client
DESCRIPTION: This client-side React component calls the `streamComponent` Server Action to dynamically fetch and display a React component streamed from the server. It uses `useState` to manage the displayed component, demonstrating how to integrate AI SDK RSC with a client-side UI.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/00-guides/21-llama-3_1.mdx#_snippet_9

LANGUAGE: tsx
CODE:
```
'use client';

import { useState } from 'react';
import { streamComponent } from './actions';

export default function Page() {
  const [component, setComponent] = useState<React.ReactNode>();

  return (
    <div>
      <form
        onSubmit={async e => {
          e.preventDefault();
          setComponent(await streamComponent());
        }}
      >
        <button>Stream Component</button>
      </form>
      <div>{component}</div>
    </div>
  );
}
```

----------------------------------------

TITLE: API: RSS Source Parameters
DESCRIPTION: Defines the parameters available for configuring RSS feed search sources within the xai model's search functionality.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/01-xai.mdx#_snippet_18

LANGUAGE: APIDOC
CODE:
```
links: string[] (Array of RSS feed URLs, max 1 currently supported)
```

----------------------------------------

TITLE: Stream Text Response with Llama 3.1 using DeepInfra and AI SDK
DESCRIPTION: This snippet demonstrates how to stream the response from the Llama 3.1 model using the AI SDK's `streamText` function. It uses the DeepInfra provider and returns a `textStream` object, allowing for real-time processing of the generated text.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/00-guides/21-llama-3_1.mdx#_snippet_2

LANGUAGE: TypeScript
CODE:
```
import { streamText } from 'ai';
import { deepinfra } from '@ai-sdk/deepinfra';

const { textStream } = streamText({
  model: deepinfra('meta-llama/Meta-Llama-3.1-405B-Instruct'),
  prompt: 'What is love?',
});
```

----------------------------------------

TITLE: Vercel AI SDK Output API Methods
DESCRIPTION: Defines the `Output` type and its associated methods for generating different forms of AI output. Includes methods for forwarding raw text and generating structured JSON objects based on a provided schema.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/02-stream-text.mdx#_snippet_13

LANGUAGE: APIDOC
CODE:
```
Type: Output
  Method: text()
    Description: Forward text output.
  Method: object()
    Description: Generate a JSON object of type OBJECT.
    Parameters:
      schema: Schema<OBJECT>
        Description: The schema of the JSON object to generate.
```

----------------------------------------

TITLE: Implement Server-side Multi-Step AI Calls with Tool Execution
DESCRIPTION: Demonstrates how to use `streamText` on the server-side to handle multi-step AI calls, including defining and executing a tool (`getWeatherInformation`) with an `inputSchema` and `execute` function. It shows how to integrate with `@ai-sdk/openai` and `zod` for schema validation within an `app/api/chat/route.ts` file.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/03-chatbot-tool-usage.mdx#_snippet_6

LANGUAGE: TypeScript
CODE:
```
import { openai } from '@ai-sdk/openai';
import { convertToModelMessages, streamText, UIMessage, stepCountIs } from 'ai';
import { z } from 'zod';

export async function POST(req: Request) {
  const { messages }: { messages: UIMessage[] } = await req.json();

  const result = streamText({
    model: openai('gpt-4o'),
    messages: convertToModelMessages(messages),
    tools: {
      getWeatherInformation: {
        description: 'show the weather in a given city to the user',
        inputSchema: z.object({ city: z.string() }),
        // tool has execute function:
        execute: async ({}: { city: string }) => {
          const weatherOptions = ['sunny', 'cloudy', 'rainy', 'snowy', 'windy'];
          return weatherOptions[
            Math.floor(Math.random() * weatherOptions.length)
          ];
        },
      },
    },
    stopWhen: stepCountIs(5),
  });

  return result.toUIMessageStreamResponse();
}
```

----------------------------------------

TITLE: Server-Side UI Streaming Workflow with AI SDK RSC
DESCRIPTION: This describes the simplified four-step process for rendering and streaming user interfaces using the AI SDK RSC. It highlights how React components are rendered on the server within tool calls and directly streamed to the client, streamlining the UI rendering pipeline.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/06-advanced/07-rendering-ui-with-language-models.mdx#_snippet_7

LANGUAGE: APIDOC
CODE:
```
1. The user prompts the language model.
2. The language model generates a response that includes a tool call.
3. The tool call renders a React component along with relevant props that represent the user interface.
4. The response is streamed to the client and rendered directly.
```

----------------------------------------

TITLE: Build and Deploy Nuxt AI Chat to Vercel
DESCRIPTION: Commands to build the Nuxt application and deploy it to Vercel's Edge Network. This example is configured to use the `vercel-edge` Nitro preset.
SOURCE: https://github.com/vercel/ai/blob/main/examples/nuxt-openai/README.md#_snippet_1

LANGUAGE: bash
CODE:
```
pnpm run build
vercel deploy
```

----------------------------------------

TITLE: Vercel AI SDK Core API Definitions
DESCRIPTION: Defines core interfaces and functions for stream handling, tool call management, and step-by-step processing within the Vercel AI SDK.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/02-stream-text.mdx#_snippet_6

LANGUAGE: APIDOC
CODE:
```
Type: PrepareStepFunction<TOOLS>
  parameters:
    - name: options
      type: object
      description: The options for the step.
      properties:
        Type: PrepareStepOptions
          parameters:
            - name: steps
              type: Array<StepResult<TOOLS>>
              description: The steps that have been executed so far.
            - name: stepNumber
              type: number
              description: The number of the step that is being executed.
            - name: model
              type: LanguageModel
              description: The model that is being used.

Function: experimental_repairToolCall
  Type: (options: ToolCallRepairOptions) => Promise<LanguageModelV2ToolCall | null>
  Optional: true
  Description: A function that attempts to repair a tool call that failed to parse. Return either a repaired tool call or null if the tool call cannot be repaired.
  Parameters (ToolCallRepairOptions):
    - name: system
      type: string | undefined
      description: The system prompt.
    - name: messages
      type: ModelMessage[]
      description: The messages in the current generation step.
    - name: toolCall
      type: LanguageModelV2ToolCall
      description: The tool call that failed to parse.
    - name: tools
      type: TOOLS
      description: The tools that are available.
    - name: parameterSchema
      type: (options: { toolName: string }) => JSONSchema7
      description: A function that returns the JSON Schema for a tool.
    - name: error
      type: NoSuchToolError | InvalidToolArgumentsError
      description: The error that occurred while parsing the tool call.

Function: onChunk
  Type: (event: OnChunkResult) => Promise<void> |void
  Optional: true
  Description: Callback that is called for each chunk of the stream. The stream processing will pause until the callback promise is resolved.
  Parameters (OnChunkResult):
    - name: chunk
      type: TextStreamPart
      description: The chunk of the stream.
      properties:
        Type: TextStreamPart (type: 'text')
          parameters:
            - name: type
              type: ""'text'""
              description: The type to identify the object as text delta.
            - name: text
              type: string
              description: The text delta.
        Type: TextStreamPart (type: 'reasoning')
          parameters:
            - name: type
              type: ""'reasoning'""
              description: The type to identify the object as reasoning.
            - name: text
              type: string
              description: The reasoning text delta.
        Type: TextStreamPart (type: 'source')
          parameters:
            - name: type
              type: ""'source'""
              description: The type to identify the object as source.
            - name: source
              type: Source
              description: The source.
        Type: TextStreamPart (type: 'tool-call')
          (content truncated)
```

----------------------------------------

TITLE: Add OpenAI API Key to .env File
DESCRIPTION: This snippet shows the format for adding your OpenAI API key to the `.env` file. Replace `xxxxxxxxx` with your actual key to authenticate your application with the OpenAI service.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-getting-started/06-nodejs.mdx#_snippet_3

LANGUAGE: dotenv
CODE:
```
OPENAI_API_KEY=xxxxxxxxx
```

----------------------------------------

TITLE: Example Usage of createDataStreamResponse
DESCRIPTION: Demonstrates how to use `createDataStreamResponse` to create a streaming `Response` object. This example illustrates setting HTTP status and headers, and utilizing the `execute` function to write various types of data (plain data, message annotations) and merge other streams. It also shows how to provide a custom error handler.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/07-reference/02-ai-sdk-ui/41-create-data-stream-response.mdx#_snippet_1

LANGUAGE: tsx
CODE:
```
const response = createDataStreamResponse({
  status: 200,
  statusText: 'OK',
  headers: {
    'Custom-Header': 'value',
  },
  async execute(dataStream) {
    // Write data
    dataStream.writeData({ value: 'Hello' });

    // Write annotation
    dataStream.writeMessageAnnotation({ type: 'status', value: 'processing' });

    // Merge another stream
    const otherStream = getAnotherStream();
    dataStream.merge(otherStream);
  },
  onError: error => `Custom error: ${error.message}`,
});
```

----------------------------------------

TITLE: Update Route Handler to Process AI Tool Calls
DESCRIPTION: This code updates an API route handler to integrate AI tool processing. It uses `createDataStreamResponse` and `processToolCalls` to manage tool execution, including handling tools that require human confirmation (like `getWeatherInformation` in this example) and streaming text responses from an OpenAI model.
SOURCE: https://github.com/vercel/ai/blob/v5/content/cookbook/01-next/75-human-in-the-loop.mdx#_snippet_7

LANGUAGE: typescript
CODE:
```
import { openai } from '@ai-sdk/openai';
import { createDataStreamResponse, Message, streamText } from 'ai';
import { processToolCalls } from './utils';
import { tools } from './tools';

// Allow streaming responses up to 30 seconds
export const maxDuration = 30;

export async function POST(req: Request) {
  const { messages }: { messages: Message[] } = await req.json();

  return createDataStreamResponse({
    execute: async dataStream => {
      // Utility function to handle tools that require human confirmation
      // Checks for confirmation in last message and then runs associated tool
      const processedMessages = await processToolCalls(
        {
          messages,
          dataStream,
          tools,
        },
        {
          // type-safe object for tools without an execute function
          getWeatherInformation: async ({ city }) => {
            const conditions = ['sunny', 'cloudy', 'rainy', 'snowy'];
            return `The weather in ${city} is ${
              conditions[Math.floor(Math.random() * conditions.length)]
            }.`;
          },
        },
      );

      const result = streamText({
        model: openai('gpt-4o'),
        messages: processedMessages,
        tools,
      });

      result.mergeIntoDataStream(dataStream);
    },
  });
}
```

----------------------------------------

TITLE: streamObject API Reference
DESCRIPTION: Defines the signature and parameters for the `streamObject` function, which facilitates streaming structured data from a language model based on various output configurations and schemas.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/04-stream-object.mdx#_snippet_5

LANGUAGE: APIDOC
CODE:
```
streamObject(options: object): { partialObjectStream: AsyncIterable<object>, elementStream?: AsyncIterable<object> }

options properties:
  model: object
    The language model instance to use (e.g., from @ai-sdk/openai).
  schema?: ZodSchema
    The Zod schema defining the structure of the expected output object or array items. Required for 'object' and 'array' output types.
  prompt: string
    The input prompt for the language model.
  output?: 'object' | 'array' | 'no-schema' | 'enum'
    Specifies the desired output format. Defaults to 'object'.
  enum?: string[]
    An array of possible string values when output is 'enum'. Required for 'enum' output type.
```

----------------------------------------

TITLE: API: experimental_output Setting
DESCRIPTION: Experimental setting for generating structured outputs. Further details on its properties are not fully provided in the snippet.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/02-stream-text.mdx#_snippet_12

LANGUAGE: APIDOC
CODE:
```
Name: experimental_output
  Type: Output
  Optional: true
  Description: Experimental setting for generating structured outputs.
  Properties: []
```

----------------------------------------

TITLE: Generate Text with Azure OpenAI Model
DESCRIPTION: A practical example demonstrating the use of the `generateText` function with an Azure OpenAI model to produce human-like text based on a provided prompt, such as generating a recipe.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/04-azure.mdx#_snippet_6

LANGUAGE: TypeScript
CODE:
```
import { azure } from '@ai-sdk/azure';
import { generateText } from 'ai';

const { text } = await generateText({
  model: azure('your-deployment-name'),
  prompt: 'Write a vegetarian lasagna recipe for 4 people.',
});
```

----------------------------------------

TITLE: AI SDK MCP Client Initialization API
DESCRIPTION: API documentation for `experimental_createMCPClient` function, used to initialize a Model Context Protocol (MCP) client. It supports various transport mechanisms including SSE (Server-Sent Events) for remote communication and Stdio for local inter-process communication, allowing for flexible integration with MCP servers.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/15-tools-and-tool-calling.mdx#_snippet_26

LANGUAGE: APIDOC
CODE:
```
experimental_createMCPClient(options: object): Promise<MCPClient>
  options:
    transport: object (required)
      type: 'sse' | 'stdio' | 'custom' (required)
      // SSE Transport options:
      url: string (required, for 'sse')
      headers: object (optional, for 'sse')
        Authorization: string (optional)
      // Stdio Transport options:
      command: string (required, for 'stdio')
      args: string[] (optional, for 'stdio')
      // Custom Transport options:
      // Implement MCPTransport interface
```

----------------------------------------

TITLE: Server-side PDF Analysis API Route with AI SDK
DESCRIPTION: This Next.js API route (`POST /api/analyze`) receives a PDF file from a form submission, extracts its `ArrayBuffer`, and uses the AI SDK's `generateObject` function with Anthropic's Claude 3.5 Sonnet model to summarize the PDF. It defines a Zod schema for structured output, expecting a 'summary' field, and returns the generated summary as a response. Dependencies include `ai`, `@ai-sdk/anthropic`, and `zod`.
SOURCE: https://github.com/vercel/ai/blob/v5/content/cookbook/01-next/31-generate-object-with-file-prompt.mdx#_snippet_1

LANGUAGE: typescript
CODE:
```
import { generateObject } from 'ai';
import { anthropic } from '@ai-sdk/anthropic';
import { z } from 'zod';

export async function POST(request: Request) {
  const formData = await request.formData();
  const file = formData.get('pdf') as File;

  const result = await generateObject({
    model: anthropic('claude-3-5-sonnet-latest'),
    messages: [
      {
        role: 'user',
        content: [
          {
            type: 'text',
            text: 'Analyze the following PDF and generate a summary.',
          },
          {
            type: 'file',
            data: await file.arrayBuffer(),
            mediaType: 'application/pdf',
          },
        ],
      },
    ],
    schema: z.object({
      summary: z.string().describe('A 50 word summary of the PDF.'),
    }),
  });

  return new Response(result.object.summary);
}
```

----------------------------------------

TITLE: Generate Structured JSON Data with AI SDK
DESCRIPTION: This snippet demonstrates how to use `generateObject` from AI SDK Core with DeepInfra and Zod to generate type-safe JSON objects conforming to a specified schema. It shows how to define a recipe schema and generate a lasagna recipe, ensuring the model output adheres to the defined structure.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/00-guides/21-llama-3_1.mdx#_snippet_3

LANGUAGE: TypeScript
CODE:
```
import { generateObject } from 'ai';
import { deepinfra } from '@ai-sdk/deepinfra';
import { z } from 'zod';

const { object } = await generateObject({
  model: deepinfra('meta-llama/Meta-Llama-3.1-70B-Instruct'),
  schema: z.object({
    recipe: z.object({
      name: z.string(),
      ingredients: z.array(z.object({ name: z.string(), amount: z.string() })),
      steps: z.array(z.string()),
    }),
  }),
  prompt: 'Generate a lasagna recipe.',
});
```

----------------------------------------

TITLE: Simulating AI SDK Data Stream Protocol Responses in Next.js
DESCRIPTION: Provides an example of how to simulate AI SDK Data Stream Protocol responses using `simulateReadableStream` within a Next.js API route. This is useful for testing, debugging, or demonstration purposes, allowing control over chunk delays and the exact stream content, including event types and metadata.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/03-ai-sdk-core/55-testing.mdx#_snippet_4

LANGUAGE: ts
CODE:
```
import { simulateReadableStream } from 'ai';

export async function POST(req: Request) {
  return new Response(
    simulateReadableStream({
      initialDelayInMs: 1000, // Delay before the first chunk
      chunkDelayInMs: 300, // Delay between chunks
      chunks: [
        `0:""This""\n`,
        `0:"" is an""\n`,
        `0:""example.""\n`,
        `e:{""finishReason"":""stop"",""usage"":{""promptTokens"":20,""completionTokens"":50},""isContinued"":false}\n`,
        `d:{""finishReason"":""stop"",""usage"":{""promptTokens"":20,""completionTokens"":50}}\n`
      ]
    }).pipeThrough(new TextEncoderStream()),
    {
      status: 200,
      headers: {
        'X-Vercel-AI-Data-Stream': 'v2',
        'Content-Type': 'text/plain; charset=utf-8'
      }
    }
  );
}
```

----------------------------------------

TITLE: OpenAI Logprobs API Details
DESCRIPTION: Explains that log probabilities are available for OpenAI completion/chat models and can be accessed through the `providerMetadata` object after a generation call, with configuration options in `providerOptions`.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/03-openai.mdx#_snippet_15

LANGUAGE: APIDOC
CODE:
```
OpenAI Logprobs:
  Availability: For completion/chat models
  Access: Via providerMetadata.openai.logprobs
  Configuration: Set logprobs: true (or a number) in providerOptions.openai
```

----------------------------------------

TITLE: Install Project Dependencies and Configure Environment
DESCRIPTION: This snippet outlines the initial setup steps for the project. It includes commands to install all required dependencies using pnpm and to set up the local environment variables by copying the example file. Users are instructed to then populate the .env.local file with their specific configuration values.
SOURCE: https://github.com/vercel/ai/blob/v5/examples/next-openai-kasada-bot-protection/README.md#_snippet_0

LANGUAGE: sh
CODE:
```
pnpm i
cp .env.local.example .env.local # and fill in the required values
```

----------------------------------------

TITLE: Control Reasoning Effort for OpenAI o1 Model
DESCRIPTION: This snippet demonstrates how to control the reasoning effort of the OpenAI 'o1' model using the `reasoningEffort` parameter. By setting `providerOptions.openai.reasoningEffort` to 'low', 'medium', or 'high', developers can adjust the model's internal computation time for faster or more refined responses. This parameter is specific to the 'o1' model.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/00-guides/23-o1.mdx#_snippet_2

LANGUAGE: ts
CODE:
```
import { generateText } from 'ai';
import { openai } from '@ai-sdk/openai';

// Reduce reasoning effort for faster responses
const { text } = await generateText({
  model: openai('o1'),
  prompt: 'Explain quantum entanglement briefly.',
  providerOptions: {
    openai: { reasoningEffort: 'low' },
  },
});
```

----------------------------------------

TITLE: Typical OpenTelemetry (OTEL) Setup for AI SDK Observability
DESCRIPTION: This example outlines a more complex OpenTelemetry (OTEL) setup for observability, contrasting it with Helicone's proxy approach. It involves installing multiple packages, configuring an exporter, setting up an SDK with instrumentations and resources, starting the SDK, enabling telemetry on requests, and finally shutting down the SDK to flush traces.
SOURCE: https://github.com/vercel/ai/blob/v5/content/providers/05-observability/helicone.mdx#_snippet_2

LANGUAGE: javascript
CODE:
```
// Install multiple packages
// @vercel/otel, @opentelemetry/sdk-node, @opentelemetry/auto-instrumentations-node, etc.

// Create exporter
const exporter = new OtherProviderExporter({ 
  projectApiKey: process.env.API_KEY 
});

// Setup SDK
const sdk = new NodeSDK({
  traceExporter: exporter,
  instrumentations: [getNodeAutoInstrumentations()],
  resource: new Resource({...}),
});

// Start SDK
sdk.start();

// Enable telemetry on each request
const response = await generateText({
  model: openai(""gpt-4o-mini""),
  prompt: ""Hello world"",
  experimental_telemetry: { isEnabled: true }
});

// Shutdown SDK to flush traces
await sdk.shutdown();
```

----------------------------------------

TITLE: Server-side AI Chat API Route with Tool Definitions
DESCRIPTION: This server-side API route (`POST`) handles chat requests using the `@ai-sdk/openai` library. It streams text responses from an OpenAI model (`gpt-4.1`) and defines various AI tools. These tools include a server-side `getWeatherInformation` tool that returns mock data, and client-side tools like `askForConfirmation` and `getLocation` for user interaction and data retrieval, demonstrating how to integrate AI capabilities with custom functions.
SOURCE: https://github.com/vercel/ai/blob/v5/content/cookbook/01-next/90-render-visual-interface-in-chat.mdx#_snippet_3

LANGUAGE: tsx
CODE:
```
import { openai } from '@ai-sdk/openai';
import { streamText } from 'ai';
import { z } from 'zod';

export default async function POST(request: Request) {
  const { messages } = await request.json();

  const result = streamText({
    model: openai('gpt-4.1'),
    messages,
    tools: {
      // server-side tool with execute function:
      getWeatherInformation: {
        description: 'show the weather in a given city to the user',
        parameters: z.object({ city: z.string() }),
        execute: async ({}: { city: string }) => {
          return {
            value: 24,
            unit: 'celsius',
            weeklyForecast: [
              { day: 'Monday', value: 24 },
              { day: 'Tuesday', value: 25 },
              { day: 'Wednesday', value: 26 },
              { day: 'Thursday', value: 27 },
              { day: 'Friday', value: 28 },
              { day: 'Saturday', value: 29 },
              { day: 'Sunday', value: 30 }
            ]
          };
        }
      },
      // client-side tool that starts user interaction:
      askForConfirmation: {
        description: 'Ask the user for confirmation.',
        parameters: z.object({
          message: z.string().describe('The message to ask for confirmation.')
        })
      },
      // client-side tool that is automatically executed on the client:
      getLocation: {
        description:
          'Get the user location. Always ask for confirmation before using this tool.',
        parameters: z.object({})
      }
    }
  });

  return result.toUIMessageStreamResponse();
}
```

----------------------------------------

TITLE: createProviderRegistry API Return Type
DESCRIPTION: Documentation for the object returned by `createProviderRegistry`, detailing its methods for accessing different types of AI models (language, text embedding, and image models) by their `providerId:modelId` format.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/07-reference/01-ai-sdk-core/40-provider-registry.mdx#_snippet_7

LANGUAGE: APIDOC
CODE:
```
Returns: Provider instance
  description: The createProviderRegistry function returns a Provider instance. It has the following methods:
  methods:
    languageModel: (id: string) => LanguageModel
      description: A function that returns a language model by its id (format: providerId:modelId)
    textEmbeddingModel: (id: string) => EmbeddingModel<string>
      description: A function that returns a text embedding model by its id (format: providerId:modelId)
    imageModel: (id: string) => ImageModel
      description: A function that returns an image model by its id (format: providerId:modelId)
```

----------------------------------------

TITLE: Install AI SDK and Anthropic Provider for Next.js
DESCRIPTION: This shell command installs the core AI SDK library (`ai`) and the specific Anthropic provider (`@ai-sdk/anthropic`). These packages are essential for setting up an AI-powered application that interacts with Anthropic's large language models, such as Claude 3.7 Sonnet, within a Next.js environment.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/00-guides/20-sonnet-3-7.mdx#_snippet_3

LANGUAGE: Shell
CODE:
```
npm install ai @ai-sdk/anthropic
```

----------------------------------------

TITLE: Server-side Next.js API Route for AI Object Generation
DESCRIPTION: This Next.js API route (`app/api/completion/route.ts`) implements the backend logic for generating structured data. It utilizes the `generateObject` function from the AI SDK, integrates with an OpenAI model, and enforces a specific JSON schema using Zod for the output, returning it as a JSON response.
SOURCE: https://github.com/vercel/ai/blob/v5/content/cookbook/01-next/30-generate-object.mdx#_snippet_1

LANGUAGE: typescript
CODE:
```
import { generateObject } from 'ai';
import { openai } from '@ai-sdk/openai';
import { z } from 'zod';

export async function POST(req: Request) {
  const { prompt }: { prompt: string } = await req.json();

  const result = await generateObject({
    model: openai('gpt-4'),
    system: 'You generate three notifications for a messages app.',
    prompt,
    schema: z.object({
      notifications: z.array(
        z.object({
          name: z.string().describe('Name of a fictional person.'),
          message: z.string().describe('Do not use emojis or links.'),
          minutesAgo: z.number(),
        }),
      ),
    }),
  });

  return result.toJsonResponse();
}
```

----------------------------------------

TITLE: API Signature for experimental_createMCPClient
DESCRIPTION: Detailed API documentation for the `experimental_createMCPClient` function, outlining its `config` parameter. This includes nested configurations for message transport (custom `MCPTransport` or `McpSSEServerConfig` for SSE), optional client naming, and an error handler for uncaught errors.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/07-reference/01-ai-sdk-core/21-create-mcp-client.mdx#_snippet_1

LANGUAGE: APIDOC
CODE:
```
experimental_createMCPClient(config: MCPClientConfig)

MCPClientConfig:
  config: Configuration for the MCP client.
    transport: TransportConfig = MCPTransport | McpSSEServerConfig
      Configuration for the message transport layer.
      MCPTransport:
        A client transport instance, used explicitly for stdio or custom transports
        start: () => Promise<void>
          A method that starts the transport
        send: (message: JSONRPCMessage) => Promise<void>
          A method that sends a message through the transport
        close: () => Promise<void>
          A method that closes the transport
        onclose: () => void
          A method that is called when the transport is closed
        onerror: (error: Error) => void
          A method that is called when the transport encounters an error
        onmessage: (message: JSONRPCMessage) => void
          A method that is called when the transport receives a message
      McpSSEServerConfig:
        type: ""'sse'""
          Use Server-Sent Events for communication
        url: string
          URL of the MCP server
        headers?: Record<string, string>
          Additional HTTP headers to be sent with requests.
    name?: string
      Client name. Defaults to ""ai-sdk-mcp-client""
    onUncaughtError?: (error: unknown) => void
      Handler for uncaught errors
```

----------------------------------------

TITLE: OpenAI Speech Provider Options API Reference
DESCRIPTION: Details the available `providerOptions` for OpenAI speech models, including `instructions`, `response_format`, and `speed`, along with their types, descriptions, and default values.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/03-openai.mdx#_snippet_50

LANGUAGE: APIDOC
CODE:
```
instructions: string
  Control the voice of your generated audio with additional instructions e.g. ""Speak in a slow and steady tone"".
  Does not work with tts-1 or tts-1-hd.
  Optional.
response_format: string
  The format to audio in.
  Supported formats are mp3, opus, aac, flac, wav, and pcm.
  Defaults to mp3.
  Optional.
speed: number
  The speed of the generated audio.
  Select a value from 0.25 to 4.0.
  Defaults to 1.0.
  Optional.
```

----------------------------------------

TITLE: Finish Step Part Format and Example
DESCRIPTION: Defines the structure and provides an example of the part indicating the completion of a processing step. It includes metadata such as the finish reason, token usage for that step, and whether the step's text will be continued.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/04-ai-sdk-ui/50-stream-protocol.mdx#_snippet_9

LANGUAGE: APIDOC
CODE:
```
Format: e:{finishReason:'stop' | 'length' | 'content-filter' | 'tool-calls' | 'error' | 'other' | 'unknown';usage:{promptTokens:number; completionTokens:number;},isContinued:boolean}
Example: e:{""finishReason"":""stop"",""usage"":{""promptTokens"":10,""completionTokens"":20},""isContinued"":false}
```

----------------------------------------

TITLE: Generate Text with PDF Input using Azure OpenAI Responses API
DESCRIPTION: This snippet demonstrates how to use the Azure OpenAI Responses API to process PDF files. It shows how to include a PDF file as part of the message content using the `file` type, allowing the model to access and respond to questions about its contents.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/04-azure.mdx#_snippet_16

LANGUAGE: TypeScript
CODE:
```
const result = await generateText({
  model: azure.responses('your-deployment-name'),
  messages: [
    {
      role: 'user',
      content: [
        {
          type: 'text',
          text: 'What is an embedding model?',
        },
        {
          type: 'file',
          data: fs.readFileSync('./data/ai.pdf'),
          mediaType: 'application/pdf',
          filename: 'ai.pdf', // optional
        },
      ],
    },
  ],
});
```

----------------------------------------

TITLE: API Route for Chatbot Tool Integration
DESCRIPTION: This TypeScript code defines an API route (`/api/chat`) that processes chat messages and integrates various tools using the `@ai-sdk/openai` and `ai` libraries. It demonstrates how to define server-side tools (e.g., `getWeatherInformation`), client-side interactive tools (e.g., `askForConfirmation`), and client-side automatic tools (e.g., `getLocation`) with input schemas validated by `zod`. The `streamText` function is utilized to handle message processing and tool definitions.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/03-chatbot-tool-usage.mdx#_snippet_0

LANGUAGE: TypeScript
CODE:
```
import { openai } from '@ai-sdk/openai';
import { convertToModelMessages, streamText, UIMessage } from 'ai';
import { z } from 'zod';

// Allow streaming responses up to 30 seconds
export const maxDuration = 30;

export async function POST(req: Request) {
  const { messages }: { messages: UIMessage[] } = await req.json();

  const result = streamText({
    model: openai('gpt-4o'),
    messages: convertToModelMessages(messages),
    tools: {
      // server-side tool with execute function:
      getWeatherInformation: {
        description: 'show the weather in a given city to the user',
        inputSchema: z.object({ city: z.string() }),
        execute: async ({}: { city: string }) => {
          const weatherOptions = ['sunny', 'cloudy', 'rainy', 'snowy', 'windy'];
          return weatherOptions[
            Math.floor(Math.random() * weatherOptions.length)
          ];
        },
      },
      // client-side tool that starts user interaction:
      askForConfirmation: {
        description: 'Ask the user for confirmation.',
        inputSchema: z.object({
          message: z.string().describe('The message to ask for confirmation.'),
        }),
      },
      // client-side tool that is automatically executed on the client:
      getLocation: {
        description:
          'Get the user location. Always ask for confirmation before using this tool.',
        inputSchema: z.object({}),
      },
    },
  });

  return result.toUIMessageStreamResponse();
}
```

----------------------------------------

TITLE: AIStream API Signature
DESCRIPTION: This section details the API signature for the `AIStream` function, outlining its required parameters: the `Response` object from a fetch call, an optional `customParser` function for stream events, and a `callbacks` object to handle various stages of the stream processing.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/07-reference/04-stream-helpers/01-ai-stream.mdx#_snippet_1

LANGUAGE: APIDOC
CODE:
```
AIStream(response: Response, customParser: (AIStreamParser) => void, callbacks: AIStreamCallbacksAndOptions)
  response: Response
    Description: This is the response object returned by fetch. It's used as the source of the readable stream.
  customParser: (AIStreamParser) => void
    Description: This is a function that is used to parse the events in the stream. It should return a function that receives a stringified chunk from the LLM and extracts the message content. The function is expected to return nothing (void) or a string.
    Type Definition: AIStreamParser
      (data: string) => string | void
  callbacks: AIStreamCallbacksAndOptions
    Properties:
      onStart: () => Promise<void>
        Description: An optional function that is called at the start of the stream processing.
      onCompletion: (completion: string) => Promise<void>
        Description: An optional function that is called for every completion. It's passed the completion as a string.
      onFinal: (completion: string) => Promise<void>
        Description: An optional function that is called once when the stream is closed with the final completion message.
      onToken: (token: string) => Promise<void>
        Description: An optional function that is called for each token in the stream. It's passed the token as a string.
```

----------------------------------------

TITLE: Tool Object API Reference
DESCRIPTION: Defines the structure and properties of the 'Tool' object, including its core definition, execution function, and result conversion methods. This object is central to defining how language models interact with external functionalities and process their outputs.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/07-reference/01-ai-sdk-core/20-tool.mdx#_snippet_2

LANGUAGE: APIDOC
CODE:
```
Tool:
  description: Information about the purpose of the tool including details on how and when it can be used by the model. (string, optional)
  parameters: The schema of the input that the tool expects. The language model will use this to generate the input. It is also used to validate the output of the language model. Use descriptions to make the input understandable for the language model. You can either pass in a Zod schema or a JSON schema (using the `jsonSchema` function). (Zod Schema | JSON Schema)
  execute: An async function that is called with the arguments from the tool call and produces a result. If not provided, the tool will not be executed automatically. (async (parameters: T, options: ToolExecutionOptions) => RESULT, optional)
    ToolExecutionOptions:
      toolCallId: The ID of the tool call. You can use it e.g. when sending tool-call related information with stream data. (string)
      messages: Messages that were sent to the language model to initiate the response that contained the tool call. The messages do not include the system prompt nor the assistant response that contained the tool call. (ModelMessage[])
      abortSignal: An optional abort signal that indicates that the overall operation should be aborted. (AbortSignal)
  experimental_toToolResultContent: An optional function that converts the result of the tool call to a content object that can be used in LLM messages. ((result: RESULT) => TextToolResultContent | ImageToolResultContent, optional)
    TextToolResultContent:
      type: The type of the tool result content. ('text')
      text: The content of the message. (string)
    ImageToolResultContent:
      type: The type of the tool result content. ('image')
      data: The base64 encoded png image. (string)
      mediaType: The IANA media type of the image. (string, optional)

Returns: The tool that was passed in.
```

----------------------------------------

TITLE: Create API Route for Chat Message Streaming
DESCRIPTION: This API route processes incoming chat messages from the client. It uses `@ai-sdk/openai` to interact with an OpenAI model (gpt-4o), converts UI messages to model-compatible format, and streams the AI's text responses back to the client as a UI message stream. The `maxSteps` parameter limits the number of turns in the conversation.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/04-generative-user-interfaces.mdx#_snippet_1

LANGUAGE: ts
CODE:
```
import { openai } from '@ai-sdk/openai';
import { streamText, convertToModelMessages, UIMessage } from 'ai';

export async function POST(request: Request) {
  const { messages }: { messages: UIMessage[] } = await request.json();

  const result = streamText({
    model: openai('gpt-4o'),
    system: 'You are a friendly assistant!',
    messages: convertToModelMessages(messages),
    maxSteps: 5,
  });

  return result.toUIMessageStreamResponse();
}
```

----------------------------------------

TITLE: Generate Text with Letta Cloud
DESCRIPTION: Demonstrates how to use the `lettaCloud` instance to generate text using a specified agent ID and prompt, connecting to the Letta Cloud API. This is suitable for applications deployed to production.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/71-letta.mdx#_snippet_3

LANGUAGE: ts
CODE:
```
import { lettaCloud } from '@letta-ai/vercel-ai-sdk-provider';
import { generateText } from 'ai';

const { text } = await generateText({
  model: lettaCloud('your-agent-id'),
  prompt: 'Write a vegetarian lasagna recipe for 4 people.',
});
```

----------------------------------------

TITLE: API Reference for zodSchema Function
DESCRIPTION: Detailed API documentation for the `zodSchema` function, outlining its parameters, their types, descriptions, and the structure of the returned Schema object. It specifies the `zodSchema` input, the optional `options` object (including `useReferences`), and the compatible AI SDK Schema output.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/07-reference/01-ai-sdk-core/26-zod-schema.mdx#_snippet_2

LANGUAGE: APIDOC
CODE:
```
zodSchema(zodSchema: z.Schema, options?: object) -> Schema object

Parameters:
  zodSchema: z.Schema
    Description: The Zod schema definition.
  options: object (optional)
    Description: Additional options for the schema conversion.
    Properties:
      useReferences: boolean (optional)
        Description: Enables support for references in the schema. This is required for recursive schemas, e.g. with `z.lazy`. However, not all language models and providers support such references. Defaults to `false`.

Returns:
  A Schema object that is compatible with the AI SDK, containing both the JSON schema representation and validation functionality.
```

----------------------------------------

TITLE: Eager Stream Production and Consumption Example
DESCRIPTION: This JavaScript example demonstrates an eager approach to stream production. It includes an async generator for integers, a `sleep` utility, an eager `createStream` function using `for await`, and a `run` function to consume the stream. The setup highlights how a fast producer can overwhelm a slower consumer, leading to backpressure issues.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/06-advanced/03-backpressure.mdx#_snippet_0

LANGUAGE: jsx
CODE:
```
// A generator that will yield positive integers
async function* integers() {
  let i = 1;
  while (true) {
    console.log(`yielding ${i}`);
    yield i++;

    await sleep(100);
  }
}
function sleep(ms) {
  return new Promise(resolve => setTimeout(resolve, ms));
}

// Wraps a generator into a ReadableStream
function createStream(iterator) {
  return new ReadableStream({
    async start(controller) {
      for await (const v of iterator) {
        controller.enqueue(v);
      }
      controller.close();
    },
  });
}

// Collect data from stream
async function run() {
  // Set up a stream of integers
  const stream = createStream(integers());

  // Read values from our stream
  const reader = stream.getReader();
  for (let i = 0; i < 10_000; i++) {
    // we know our stream is infinite, so there's no need to check `done`.
    const { value } = await reader.read();
    console.log(`read ${value}`);

    await sleep(1_000);
  }
}
run();
```

----------------------------------------

TITLE: AI SDK Provider Feature Comparison Table
DESCRIPTION: This table outlines the capabilities of various AI models from different providers within the AI SDK, specifically detailing support for Image Input, Object Generation, Tool Usage, and Tool Streaming. It serves as a quick reference for developers to understand model compatibility with different AI SDK features.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/index.mdx#_snippet_0

LANGUAGE: APIDOC
CODE:
```
Provider                                                                 | Model                                               | Image Input         | Object Generation   | Tool Usage          | Tool Streaming      
------------------------------------------------------------------------ | --------------------------------------------------- | ------------------- | ------------------- | ------------------- | ------------------- 
xAI Grok                                                                 | `grok-3`                                            | No                  | Yes                 | Yes                 | Yes                 
xAI Grok                                                                 | `grok-3-fast`                                       | No                  | Yes                 | Yes                 | Yes                 
xAI Grok                                                                 | `grok-3-mini`                                       | No                  | Yes                 | Yes                 | Yes                 
xAI Grok                                                                 | `grok-3-mini-fast`                                  | No                  | Yes                 | Yes                 | Yes                 
xAI Grok                                                                 | `grok-2-1212`                                       | No                  | Yes                 | Yes                 | Yes                 
xAI Grok                                                                 | `grok-2-vision-1212`                                | Yes                 | Yes                 | Yes                 | Yes                 
xAI Grok                                                                 | `grok-beta`                                         | No                  | Yes                 | Yes                 | Yes                 
xAI Grok                                                                 | `grok-vision-beta`                                  | Yes                 | No                  | No                  | No                  
Vercel                                                                   | `v0-1.0-md`                                         | Yes                 | Yes                 | Yes                 | Yes                 
OpenAI                                                                   | `gpt-4.1`                                           | Yes                 | Yes                 | Yes                 | Yes                 
OpenAI                                                                   | `gpt-4.1-mini`                                      | Yes                 | Yes                 | Yes                 | Yes                 
OpenAI                                                                   | `gpt-4.1-nano`                                      | Yes                 | Yes                 | Yes                 | Yes                 
OpenAI                                                                   | `gpt-4o`                                            | Yes                 | Yes                 | Yes                 | Yes                 
OpenAI                                                                   | `gpt-4o-mini`                                       | Yes                 | Yes                 | Yes                 | Yes                 
OpenAI                                                                   | `gpt-4.1`                                           | Yes                 | Yes                 | Yes                 | Yes                 
OpenAI                                                                   | `gpt-4`                                             | No                  | Yes                 | Yes                 | Yes                 
OpenAI                                                                   | `o1`                                                | Yes                 | No                  | Yes                 | Yes                 
OpenAI                                                                   | `o1-mini`                                           | Yes                 | No                  | Yes                 | Yes                 
OpenAI                                                                   | `o1-preview`                                        | No                  | No                  | No                  | No                  
Anthropic                                                                | `claude-3-7-sonnet-20250219`                        | Yes                 | Yes                 | Yes                 | Yes                 
```

----------------------------------------

TITLE: Access Underlying Letta Client Functions
DESCRIPTION: Demonstrates how to access the underlying Letta client functions (e.g., `lettaCloud.client.agents.list()`) for more direct API interactions. This leverages the extended functionality from the `@letta-ai/letta-client` package.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/71-letta.mdx#_snippet_6

LANGUAGE: ts
CODE:
```
import { lettaCloud } from '@letta-ai/vercel-ai-sdk-provider';

lettaCloud.agents.list();
```

----------------------------------------

TITLE: AWSBedrockStream API Signature
DESCRIPTION: Detailed API documentation for the `AWSBedrockStream` function, outlining its parameters and return type. This function is designed to transform outputs from the AWS Bedrock API into a ReadableStream.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/07-reference/04-stream-helpers/09-aws-bedrock-stream.mdx#_snippet_1

LANGUAGE: APIDOC
CODE:
```
AWSBedrockStream(response: AWSBedrockResponse, callbacks?: AIStreamCallbacksAndOptions) -> ReadableStream

Parameters:
  response: AWSBedrockResponse
    The response object returned from AWS Bedrock.
    Properties of AWSBedrockResponse:
      body?: AsyncIterable<{ chunk?: { bytes?: Uint8Array } }>
        An optional async iterable of objects containing optional binary data chunks.

  callbacks?: AIStreamCallbacksAndOptions
    An object containing callback functions to handle the start, each token, and completion of the AI response. In the absence of this parameter, default behavior is implemented.
    Properties of AIStreamCallbacksAndOptions:
      onStart?: () => Promise<void>
        An optional function that is called at the start of the stream processing.
      onCompletion?: (completion: string) => Promise<void>
        An optional function that is called for every completion. It's passed the completion as a string.
      onFinal?: (completion: string) => Promise<void>
        An optional function that is called once when the stream is closed with the final completion message.
      onToken?: (token: string) => Promise<void>
        An optional function that is called for each token in the stream. It's passed the token as a string.

Returns:
  ReadableStream
```

----------------------------------------

TITLE: Define Server Action for Streaming UI Components with AI SDK RSC
DESCRIPTION: This Next.js Server Action demonstrates how to use `streamUI` from `@ai-sdk/rsc` to generate and stream React components based on AI model output. It includes tool definitions (like `getWeather`) that can yield loading states and return dynamic UI, showcasing generative UI capabilities.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/00-guides/21-llama-3_1.mdx#_snippet_8

LANGUAGE: tsx
CODE:
```
'use server';

import { streamUI } from '@ai-sdk/rsc';
import { deepinfra } from '@ai-sdk/deepinfra';
import { z } from 'zod';

export async function streamComponent() {
  const result = await streamUI({
    model: deepinfra('meta-llama/Meta-Llama-3.1-70B-Instruct'),
    prompt: 'Get the weather for San Francisco',
    text: ({ content }) => <div>{content}</div>,
    tools: {
      getWeather: {
        description: 'Get the weather for a location',
        parameters: z.object({ location: z.string() }),
        generate: async function* ({ location }) {
          yield <div>loading...</div>;
          const weather = '25c'; // await getWeather(location);
          return (
            <div>
              the weather in {location} is {weather}.
            </div>
          );
        },
      },
    },
  });
  return result.value;
}
```

----------------------------------------

TITLE: experimental_useObject Hook API Return Values
DESCRIPTION: Documentation of the values returned by the `experimental_useObject` hook. This includes functions to submit input and stop requests, as well as state variables for the generated object, error status, and loading state.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/07-reference/02-ai-sdk-ui/03-use-object.mdx#_snippet_3

LANGUAGE: APIDOC
CODE:
```
experimental_useObject Returns:
  submit: (input: INPUT) => void
    Calls the API with the provided input as JSON body.
  object: DeepPartial<RESULT> | undefined
    The current value for the generated object. Updated as the API streams JSON chunks.
  error: Error | unknown
    The error object if the API call fails.
  isLoading: boolean
    Boolean flag indicating whether a request is currently in progress.
  stop: () => void
    Function to abort the current API request.
```

----------------------------------------

TITLE: Vercel AI SDK Core API Parameters
DESCRIPTION: This section details the input parameters for core functions within the Vercel AI SDK, such as those used for interacting with language models. It covers basic parameters like model selection and prompts, as well as complex message array structures with various roles and content types, including nested parts for text, images, and files.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/07-reference/01-ai-sdk-core/01-generate-text.mdx#_snippet_2

LANGUAGE: APIDOC
CODE:
```
Parameters:
  - model (LanguageModel): The language model to use. Example: openai('gpt-4o')
  - system (string): The system prompt to use that specifies the behavior of the model.
  - prompt (string): The input prompt to generate the text from.
  - messages (Array<SystemModelMessage | UserModelMessage | AssistantModelMessage | ToolModelMessage> | Array<UIMessage>): A list of messages that represent a conversation. Automatically converts UI messages from the useChat hook.
    Message Types:
      - SystemModelMessage:
        - role ('system'): The role for the system message.
        - content (string): The content of the message.
      - UserModelMessage:
        - role ('user'): The role for the user message.
        - content (string | Array<TextPart | ImagePart | FilePart>): The content of the message.
          Content Parts:
            - TextPart:
              - type ('text'): The type of the message part.
              - text (string): The text content of the message part.
            - ImagePart:
              - type ('image'): The type of the message part.
              - image (string | Uint8Array | Buffer | ArrayBuffer | URL): The image content of the message part. String are either base64 encoded content, base64 data URLs, or http(s) URLs.
              - mediaType (string, Optional): The IANA media type of the image.
            - FilePart:
              - type ('file'): The type of the message part.
              - data (string | Uint8Array | Buffer | ArrayBuffer | URL): The file content of the message part. String are either base64 encoded content, base64 data URLs, or http(s) URLs.
              - mediaType (string): The IANA media type of the file.
      - AssistantModelMessage:
        - role ('assistant'): The role for the assistant message.
        - content (string | Array<TextPart | FilePart | ReasoningPart | ToolCallPart>): The content of the message.
          Content Parts:
            - TextPart:
              - type ('text'): The type of the message part.
              - text (string): The text content of the message part.
            - ReasoningPart:
              - parameters:
                - type ('reasoning'): The type of the message part.
                - content (string): The content of the message.
```

----------------------------------------

TITLE: Enable Extended Thinking for Claude 3.7 Sonnet with AI SDK
DESCRIPTION: This snippet demonstrates how to enable Claude 3.7 Sonnet's extended thinking capability using the AI SDK. It configures `providerOptions` to set `thinking` to 'enabled' and specifies a `budgetTokens` for the reasoning process. The output includes the generated text, detailed reasoning, and redacted reasoning.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/00-guides/20-sonnet-3-7.mdx#_snippet_2

LANGUAGE: ts
CODE:
```
import { anthropic, AnthropicProviderOptions } from '@ai-sdk/anthropic';
import { generateText } from 'ai';

const { text, reasoning, reasoningDetails } = await generateText({
  model: anthropic('claude-3-7-sonnet-20250219'),
  prompt: 'How many people will live in the world in 2040?',
  providerOptions: {
    anthropic: {
      thinking: { type: 'enabled', budgetTokens: 12000 },
    } satisfies AnthropicProviderOptions,
  },
});

console.log(reasoning); // reasoning text
console.log(reasoningDetails); // reasoning details including redacted reasoning
console.log(text); // text response
```

----------------------------------------

TITLE: API: News Source Parameters
DESCRIPTION: Defines the parameters available for configuring news search sources within the xai model's search functionality.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/01-xai.mdx#_snippet_16

LANGUAGE: APIDOC
CODE:
```
country: string (ISO alpha-2 country code)
excludedWebsites: string[] (Max 5 excluded websites)
safeSearch: boolean (Enable safe search, default: true)
```

----------------------------------------

TITLE: Server-side API Route for Streaming AI Text
DESCRIPTION: This TypeScript snippet defines a Next.js API route that handles incoming prompts, uses `ai` and `@ai-sdk/openai` to stream text completions from an OpenAI model, and returns the result as a UI message stream. It also sets a `maxDuration` for the response to allow longer streaming.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/04-ai-sdk-ui/05-completion.mdx#_snippet_1

LANGUAGE: ts
CODE:
```
import { streamText } from 'ai';
import { openai } from '@ai-sdk/openai';

// Allow streaming responses up to 30 seconds
export const maxDuration = 30;

export async function POST(req: Request) {
  const { prompt }: { prompt: string } = await req.json();

  const result = streamText({
    model: openai('gpt-3.5-turbo'),
    prompt,
  });

  return result.toUIMessageStreamResponse();
}
```

----------------------------------------

TITLE: OpenAIStream API Signature
DESCRIPTION: Defines the parameters for the `OpenAIStream` function, including the `response` object and an optional `callbacks` object with various lifecycle functions.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/07-reference/04-stream-helpers/07-openai-stream.mdx#_snippet_1

LANGUAGE: APIDOC
CODE:
```
OpenAIStream(response: Response, callbacks?: AIStreamCallbacksAndOptions)

Parameters:
- response: Response
    The response object returned by a call made by the Provider SDK.
- callbacks?: AIStreamCallbacksAndOptions
    An object containing callback functions to handle the start, each token, and completion of the AI response. In the absence of this parameter, default behavior is implemented.

AIStreamCallbacksAndOptions:
- onStart?: () => Promise<void>
    An optional function that is called at the start of the stream processing.
- onCompletion?: (completion: string) => Promise<void>
    An optional function that is called for every completion. It's passed the completion as a string.
- onFinal?: (completion: string) => Promise<void>
    An optional function that is called once when the stream is closed with the final completion message.
- onToken?: (token: string) => Promise<void>
    An optional function that is called for each token in the stream. It's passed the token as a string.
```

----------------------------------------

TITLE: Import InkeepStream in React/TypeScript
DESCRIPTION: Example of importing the InkeepStream helper function from the 'ai' package, typically used in React or TypeScript applications.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/07-reference/04-stream-helpers/19-inkeep-stream.mdx#_snippet_0

LANGUAGE: TypeScript
CODE:
```
import { InkeepStream } from ""ai""
```

----------------------------------------

TITLE: Generate Text with Together.ai Model using AI SDK
DESCRIPTION: This example shows how to use the `togetherai` provider with the AI SDK's `generateText` function. It demonstrates configuring a specific Together.ai model (e.g., 'meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo') and generating text based on a given prompt.
SOURCE: https://github.com/vercel/ai/blob/main/packages/togetherai/README.md#_snippet_2

LANGUAGE: ts
CODE:
```
import { togetherai } from '@ai-sdk/togetherai';
import { generateText } from 'ai';

const { text } = await generateText({
  model: togetherai('meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo'),
  prompt: 'Write a Python function that sorts a list:',
});
```

----------------------------------------

TITLE: Define Temperature Conversion Tool in AI SDK API Route
DESCRIPTION: This TypeScript code snippet updates the `server/api/chat.ts` file to include a new `convertFahrenheitToCelsius` tool. This tool, defined using `ai-sdk`'s `tool` function, takes a temperature in Fahrenheit as input and returns its Celsius equivalent, enabling the AI model to perform unit conversions.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-getting-started/05-nuxt.mdx#_snippet_16

LANGUAGE: typescript
CODE:
```
import { streamText, UIMessage, convertToModelMessages, tool } from 'ai';
import { createOpenAI } from '@ai-sdk/openai';
import { z } from 'zod';

export default defineLazyEventHandler(async () => {
  const apiKey = useRuntimeConfig().openaiApiKey;
  if (!apiKey) throw new Error('Missing OpenAI API key');
  const openai = createOpenAI({
    apiKey: apiKey,
  });

  return defineEventHandler(async (event: any) => {
    const { messages }: { messages: UIMessage[] } = await readBody(event);

    const result = streamText({
      model: openai('gpt-4o'),
      messages: convertToModelMessages(messages),
      tools: {
        weather: tool({
          description: 'Get the weather in a location (fahrenheit)',
          inputSchema: z.object({
            location: z
              .string()
              .describe('The location to get the weather for'),
          }),
          execute: async ({ location }) => {
            const temperature = Math.round(Math.random() * (90 - 32) + 32);
            return {
              location,
              temperature,
            };
          },
        }),
        convertFahrenheitToCelsius: tool({
          description: 'Convert a temperature in fahrenheit to celsius',
          inputSchema: z.object({
            temperature: z
              .number()
              .describe('The temperature in fahrenheit to convert'),
          }),
          execute: async ({ temperature }) => {
            const celsius = Math.round((temperature - 32) * (5 / 9));
            return {
              celsius,
            };
          },
        }),
      },
    });

    return result.toUIMessageStreamResponse();
  });
});
```

----------------------------------------

TITLE: Trace AI SDK Generation and Run Patronus Evaluation in a Single Span
DESCRIPTION: Provides an example of integrating AI SDK text generation with Patronus AI evaluation within a single OpenTelemetry trace. It demonstrates how to generate text, then call the Patronus evaluation API, linking both operations using the current trace and span IDs for comprehensive observability and automated scoring of LLM outputs.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/05-observability/patronus.mdx#_snippet_3

LANGUAGE: ts
CODE:
```
import { trace } from '@opentelemetry/api';
import { generateText } from 'ai';
import { openai } from '@ai-sdk/openai';

export async function POST(req: Request) {
  const body = await req.json();
  const tracer = trace.getTracer('next-app');

  return await tracer.startActiveSpan('chat-evaluate', async span => {
    try {
      /* 1️⃣ generate answer */
      const answer = await generateText({
        model: openai('gpt-4o'),
        prompt: body.prompt,
        experimental_telemetry: { isEnabled: true, functionId: 'chat' },
      });

      /* 2️⃣ run Patronus evaluation inside the same trace */
      await fetch('https://api.patronus.ai/v1/evaluate', {
        method: 'POST',
        headers: {
          'X-API-Key': process.env.PATRONUS_API_KEY!,
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          evaluators: [
            { evaluator: 'lynx', criteria: 'patronus:hallucination' },
          ],
          evaluated_model_input: body.prompt,
          evaluated_model_output: answer.text,
          trace_id: span.spanContext().traceId,
          span_id: span.spanContext().spanId,
        }),
      });

      return new Response(answer.text);
    } finally {
      span.end();
    }
  });
}
```

----------------------------------------

TITLE: smoothStream API Parameters
DESCRIPTION: Detailed documentation for the `smoothStream` function's configuration parameters, including `delayInMs` and `chunking`.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/07-reference/01-ai-sdk-core/80-smooth-stream.mdx#_snippet_4

LANGUAGE: APIDOC
CODE:
```
smoothStream(options?: object):
  options:
    delayInMs: number | null (optional)
      The delay in milliseconds between outputting each chunk. Defaults to 10ms. Set to `null` to disable delays.
    chunking: ""word"" | ""line"" | RegExp | (buffer: string) => string | undefined | null (optional)
      Controls how the text is chunked for streaming. Use ""word"" to stream word by word (default), ""line"" to stream line by line, or provide a custom callback or RegExp pattern for custom chunking.
```

----------------------------------------

TITLE: Install Project Dependencies with pnpm
DESCRIPTION: This command installs all required project dependencies using `pnpm`, ensuring that the application has all necessary packages to run correctly.
SOURCE: https://github.com/vercel/ai/blob/v5/examples/nest/README.md#_snippet_1

LANGUAGE: sh
CODE:
```
pnpm install
```

----------------------------------------

TITLE: Configure OpenAI API Key
DESCRIPTION: Adds the OpenAI API key to the `.env.local` file. This key is essential for authenticating your application with the OpenAI service. Remember to replace 'xxxxxxxxx' with your actual API key.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-getting-started/04-svelte.mdx#_snippet_4

LANGUAGE: dotenv
CODE:
```
OPENAI_API_KEY=xxxxxxxxx
```

----------------------------------------

TITLE: Install Baseten OpenAI Compatible SDK
DESCRIPTION: Instructions to install the `@ai-sdk/openai-compatible` module, which provides compatibility with Baseten's OpenAI API, using various package managers.
SOURCE: https://github.com/vercel/ai/blob/v5/content/providers/02-openai-compatible-providers/40-baseten.mdx#_snippet_0

LANGUAGE: shell
CODE:
```
pnpm add @ai-sdk/openai-compatible
```

LANGUAGE: shell
CODE:
```
npm install @ai-sdk/openai-compatible
```

LANGUAGE: shell
CODE:
```
yarn add @ai-sdk/openai-compatible
```

----------------------------------------

TITLE: LanguageModelV2Middleware API Signature
DESCRIPTION: Defines the interface for LanguageModelV2Middleware, detailing its methods for transforming parameters and wrapping generate/stream operations.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/07-reference/01-ai-sdk-core/65-language-model-v2-middleware.mdx#_snippet_1

LANGUAGE: APIDOC
CODE:
```
interface LanguageModelV2Middleware {
  transformParams(
    params: {
      type: ""generate"" | ""stream"";
      params: LanguageModelV2CallOptions;
    }
  ): Promise<LanguageModelV2CallOptions>;
  // Transforms the parameters before they are passed to the language model.

  wrapGenerate(
    params: {
      doGenerate: DoGenerateFunction;
      params: LanguageModelV2CallOptions;
      model: LanguageModelV2;
    }
  ): Promise<DoGenerateResult>;
  // Wraps the generate operation of the language model.

  wrapStream(
    params: {
      doStream: DoStreamFunction;
      params: LanguageModelV2CallOptions;
      model: LanguageModelV2;
    }
  ): Promise<DoStreamResult>;
  // Wraps the stream operation of the language model.
}
```

----------------------------------------

TITLE: Next.js Backend API Route for AI Chat with Tooling
DESCRIPTION: This Next.js API route handles POST requests for chat interactions, integrating with the AI SDK. It uses `@ai-sdk/openai` to stream text responses and demonstrates how to define and use a custom tool, `getWeatherInformation`, with Zod for parameter validation. The `createDataStreamResponse` manages the server-side data stream, merging AI generation results.
SOURCE: https://github.com/vercel/ai/blob/v5/content/cookbook/01-next/75-human-in-the-loop.mdx#_snippet_1

LANGUAGE: ts
CODE:
```
import { openai } from '@ai-sdk/openai';
import { createDataStreamResponse, streamText, tool } from 'ai';
import { z } from 'zod';

export async function POST(req: Request) {
  const { messages } = await req.json();

  return createDataStreamResponse({
    execute: async dataStream => {
      const result = streamText({
        model: openai('gpt-4o'),
        messages,
        tools: {
          getWeatherInformation: tool({
            description: 'show the weather in a given city to the user',
            parameters: z.object({ city: z.string() }),
            execute: async ({}: { city: string }) => {
              const weatherOptions = ['sunny', 'cloudy', 'rainy', 'snowy'];
              return weatherOptions[
                Math.floor(Math.random() * weatherOptions.length)
              ];
            },
          }),
        },
      });

      result.mergeIntoDataStream(dataStream);
    },
  });
}
```

----------------------------------------

TITLE: Configure OpenAI API Key
DESCRIPTION: Adds the OpenAI API key to the `.env.local` file. This key is essential for authenticating your application with the OpenAI service. Remember to replace `xxxxxxxxx` with your actual API key.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/00-guides/02-multi-modal-chatbot.mdx#_snippet_4

LANGUAGE: dotenv
CODE:
```
OPENAI_API_KEY=xxxxxxxxx
```

----------------------------------------

TITLE: Update AI SDK Stream Protocol to Start/Delta/End Pattern
DESCRIPTION: This example demonstrates the fundamental change in the AI SDK's streaming protocol. It transitions from processing single chunks to a more robust three-phase pattern (start, delta, end) for each content block, providing unique IDs and better control over the streaming lifecycle for improved real-time user experiences.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/08-migration-guides/26-migration-guide-5-0.mdx#_snippet_49

LANGUAGE: tsx
CODE:
```
for await (const chunk of result.fullStream) {
  switch (chunk.type) {
    case 'text-delta': {
      process.stdout.write(chunk.textDelta);
      break;
    }
  }
}
```

LANGUAGE: tsx
CODE:
```
for await (const chunk of result.fullStream) {
  switch (chunk.type) {
    case 'text-start': {
      // New: Initialize a text block with unique ID
      console.log(`Starting text block: ${chunk.id}`);
      break;
    }
    case 'text-delta': {
      // Changed: Now includes ID and uses 'delta' property
      process.stdout.write(chunk.delta); // Changed from 'textDelta'
      break;
    }
    case 'text-end': {
      // New: Finalize the text block
      console.log(`Completed text block: ${chunk.id}`);
      break;
    }
  }
}
```

----------------------------------------

TITLE: Implement Logging Middleware for AI SDK Language Models
DESCRIPTION: This example demonstrates how to create a `LanguageModelV2Middleware` that logs the parameters and generated text for both `doGenerate` and `doStream` calls. For streaming, it uses a `TransformStream` to capture the complete generated text before logging it, showcasing how to inspect and modify stream behavior.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/03-ai-sdk-core/40-middleware.mdx#_snippet_8

LANGUAGE: ts
CODE:
```
import type { LanguageModelV2Middleware, LanguageModelV2StreamPart } from 'ai';

export const yourLogMiddleware: LanguageModelV2Middleware = {
  wrapGenerate: async ({ doGenerate, params }) => {
    console.log('doGenerate called');
    console.log(`params: ${JSON.stringify(params, null, 2)}`);

    const result = await doGenerate();

    console.log('doGenerate finished');
    console.log(`generated text: ${result.text}`);

    return result;
  },

  wrapStream: async ({ doStream, params }) => {
    console.log('doStream called');
    console.log(`params: ${JSON.stringify(params, null, 2)}`);

    const { stream, ...rest } = await doStream();

    let generatedText = '';

    const transformStream = new TransformStream<
      LanguageModelV2StreamPart,
      LanguageModelV2StreamPart
    >({
      transform(chunk, controller) {
        if (chunk.type === 'text') {
          generatedText += chunk.text;
        }

        controller.enqueue(chunk);
      },

      flush() {
        console.log('doStream finished');
        console.log(`generated text: ${generatedText}`);
      },
    });

    return {
      stream: stream.pipeThrough(transformStream),
      ...rest,
    };
  },
};
```

----------------------------------------

TITLE: Generate Text with Groq Provider and AI SDK
DESCRIPTION: An example demonstrating how to use the Groq provider with the AI SDK's generateText function. It illustrates importing necessary modules, specifying a Groq model, and providing a prompt to generate text.
SOURCE: https://github.com/vercel/ai/blob/main/packages/groq/README.md#_snippet_2

LANGUAGE: ts
CODE:
```
import { groq } from '@ai-sdk/groq';
import { generateText } from 'ai';

const { text } = await generateText({
  model: groq('gemma2-9b-it'),
  prompt: 'Write a vegetarian lasagna recipe for 4 people.',
});
```

----------------------------------------

TITLE: Implement Chat UI with AI SDK useChat Hook in Next.js
DESCRIPTION: This React component for a Next.js page (`app/page.tsx`) utilizes the `useChat` hook from `@ai-sdk/react` to manage chat state, user input, and message submission. It renders the chat history, including user and model messages, and provides an input field for new messages, supporting the display of model reasoning tokens.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/00-guides/18-claude-4.mdx#_snippet_4

LANGUAGE: TypeScript
CODE:
```
'use client';

import { useChat } from '@ai-sdk/react';

export default function Page() {
  const { messages, input, handleInputChange, handleSubmit, error } = useChat();

  return (
    <div className=""flex flex-col h-screen max-w-2xl mx-auto p-4"">
      <div className=""flex-1 overflow-y-auto space-y-4 mb-4"">
        {messages.map(message => (
          <div
            key={message.id}
            className={`p-3 rounded-lg ${
              message.role === 'user' ? 'bg-blue-50 ml-auto' : 'bg-gray-50'
            }`}
          >
            <p className=""font-semibold"">
              {message.role === 'user' ? 'You' : 'Claude 4'}
            </p>
            {message.parts.map((part, index) => {
              if (part.type === 'text') {
                return (
                  <div key={index} className=""mt-1"">
                    {part.text}
                  </div>
                );
              }
              if (part.type === 'reasoning') {
                return (
                  <pre
                    key={index}
                    className=""bg-gray-100 p-2 rounded mt-2 text-xs overflow-x-auto""
                  >
                    <details>
                      <summary className=""cursor-pointer"">
                        View reasoning
                      </summary>
                      {part.details.map(detail =>
                        detail.type === 'text' ? detail.text : '<redacted>',
                      )}
                    </details>
                  </pre>
                );
              }
            })}
          </div>
        ))}
      </div>
      <form onSubmit={handleSubmit} className=""flex gap-2"">
        <input
          name=""prompt""
          value={input}
          onChange={handleInputChange}
          className=""flex-1 p-2 border rounded focus:outline-none focus:ring-2 focus:ring-blue-500""
          placeholder=""Ask Claude 4 something...""
        />
        <button
          type=""submit""
          className=""bg-blue-500 text-white px-4 py-2 rounded hover:bg-blue-600""
        >
          Send
        </button>
      </form>
    </div>
  );
}
```

----------------------------------------

TITLE: API Reference: `toolChoice` Parameter Options
DESCRIPTION: Defines the available settings for the `toolChoice` parameter, which controls how the model selects and uses tools during text generation.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/15-tools-and-tool-calling.mdx#_snippet_8

LANGUAGE: APIDOC
CODE:
```
toolChoice: string | object
  - 'auto' (default): the model can choose whether and which tools to call.
  - 'required': the model must call a tool. It can choose which tool to call.
  - 'none': the model must not call tools.
  - { type: 'tool', toolName: string (typed) }: the model must call the specified tool.
```

----------------------------------------

TITLE: Configure Anthropic API Key in Environment Variables
DESCRIPTION: Update your local environment variables to include the Anthropic API key, alongside the OpenAI API key, for authenticating requests to Anthropic models.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/00-guides/02-multi-modal-chatbot.mdx#_snippet_13

LANGUAGE: env
CODE:
```
OPENAI_API_KEY=xxxxxxxxx
ANTHROPIC_API_KEY=xxxxxxxxx
```

----------------------------------------

TITLE: Vercel AI Model/Tool Configuration Parameters
DESCRIPTION: Detailed documentation for parameters used to configure AI models and tools within the Vercel AI SDK, including input schemas, execution options, and various generation settings.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/07-reference/01-ai-sdk-core/01-generate-text.mdx#_snippet_6

LANGUAGE: APIDOC
CODE:
```
Parameter: schema
  Type: ZodSchema | JSONSchema
  Description: The schema of the input that the tool expects. The language model will use this to generate the input. It is also used to validate the output of the language model. Use descriptions to make the input understandable for the language model. You can either pass in a Zod schema or a JSON schema (using the `jsonSchema` function).

Parameter: execute
  Type: async (parameters: T, options: ToolExecutionOptions) => RESULT
  Optional: true
  Description: An async function that is called with the arguments from the tool call and produces a result. If not provided, the tool will not be executed automatically.
  Parameters (ToolExecutionOptions):
    - toolCallId: string - The ID of the tool call. You can use it e.g. when sending tool-call related information with stream data.
    - messages: ModelMessage[] - Messages that were sent to the language model to initiate the response that contained the tool call. The messages do not include the system prompt nor the assistant response that contained the tool call.
    - abortSignal: AbortSignal - An optional abort signal that indicates that the overall operation should be aborted.

Parameter: toolChoice
  Type: ""auto"" | ""none"" | ""required"" | { ""type"": ""tool"", ""toolName"": string }
  Optional: true
  Description: The tool choice setting. It specifies how tools are selected for execution. The default is ""auto"". ""none"" disables tool execution. ""required"" requires tools to be executed. { ""type"": ""tool"", ""toolName"": string } specifies a specific tool to execute.

Parameter: maxOutputTokens
  Type: number
  Optional: true
  Description: Maximum number of tokens to generate.

Parameter: temperature
  Type: number
  Optional: true
  Description: Temperature setting. The value is passed through to the provider. The range depends on the provider and model. It is recommended to set either `temperature` or `topP`, but not both.

Parameter: topP
  Type: number
  Optional: true
  Description: Nucleus sampling. The value is passed through to the provider. The range depends on the provider and model. It is recommended to set either `temperature` or `topP`, but not both.

Parameter: topK
  Type: number
  Optional: true
  Description: Only sample from the top K options for each subsequent token. Used to remove ""long tail"" low probability responses. Recommended for advanced use cases only. You usually only need to use temperature.

Parameter: presencePenalty
  Type: number
  Optional: true
  Description: Presence penalty setting. It affects the likelihood of the model to repeat information that is already in the prompt. The value is passed through to the provider. The range depends on the provider and model.

Parameter: frequencyPenalty
  Type: number
  Optional: true
  Description: Frequency penalty setting. It affects the likelihood of the model to repeatedly use the same words or phrases. The value is passed through to the provider. The range depends on the provider and model.

Parameter: stopSequences
  Type: string[]
  Optional: true
  Description: Sequences that will stop the generation of the text. If the model generates any of these sequences, it will stop generating further text.

Parameter: seed
  Type: number
  Optional: true
  Description: The seed (integer) to use for random sampling. If set and supported by the model, calls will generate deterministic results.

Parameter: maxRetries
  Type: number
  Optional: true
  Description: Maximum number of retries. Set to 0 to disable retries. Default: 2.

Parameter: abortSignal
  Type: AbortSignal
  Optional: true
  Description: An optional abort signal that can be used to cancel the call.

Parameter: headers
  Type: Record<string, string>
  Optional: false
  Description: No description provided.
```

----------------------------------------

TITLE: Agent with Calculator Tool and stopWhen (TypeScript)
DESCRIPTION: This TypeScript example demonstrates how to build an AI agent that solves math problems. It integrates a `calculate` tool using `math.js` for expression evaluation and uses `stopWhen: stepCountIs(10)` to limit the agent's execution steps, ensuring controlled behavior. The agent is prompted to reason step-by-step and provide explanations.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-foundations/06-agents.mdx#_snippet_5

LANGUAGE: ts
CODE:
```
import { openai } from '@ai-sdk/openai';
import { generateText, tool, stepCountIs } from 'ai';
import * as mathjs from 'mathjs';
import { z } from 'zod';

const { text: answer } = await generateText({
  model: openai('gpt-4o-2024-08-06'),
  tools: {
    calculate: tool({
      description:
        'A tool for evaluating mathematical expressions. ' +
        'Example expressions: ' +
        ""'1.2 * (2 + 4.5)', '12.7 cm to inch', 'sin(45 deg) ^ 2'."",
      inputSchema: z.object({ expression: z.string() }),
      execute: async ({ expression }) => mathjs.evaluate(expression),
    }),
  },
  stopWhen: stepCountIs(10),
  system:
    'You are solving math problems. ' +
    'Reason step by step. ' +
    'Use the calculator when necessary. ' +
    'When you give the final answer, ' +
    'provide an explanation for how you arrived at it.',
  prompt:
    'A taxi driver earns $9461 per 1-hour of work. ' +
    'If he works 12 hours a day and in 1 hour ' +
    'he uses 12 liters of petrol with a price  of $134 for 1 liter. ' +
    'How much money does he earn in one day?',
});

console.log(`ANSWER: ${answer}`);
```

----------------------------------------

TITLE: Define Anthropic Computer Tool with AI SDK
DESCRIPTION: This TypeScript snippet defines a `computerTool` using Anthropic's AI SDK provider. It configures display dimensions, an `execute` function to handle actions like screenshots and computer commands, and an `experimental_toToolResultContent` function for formatting tool results for the model. Users must implement `getScreenshot` and `executeComputerAction` for actual computer interactions.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/00-guides/05-computer-use.mdx#_snippet_1

LANGUAGE: typescript
CODE:
```
import { anthropic } from '@ai-sdk/anthropic';
import { getScreenshot, executeComputerAction } from '@/utils/computer-use';

const computerTool = anthropic.tools.computer_20241022({
  displayWidthPx: 1920,
  displayHeightPx: 1080,
  execute: async ({ action, coordinate, text }) => {
    switch (action) {
      case 'screenshot': {
        return {
          type: 'image',
          data: getScreenshot(),
        };
      }
      default: {
        return executeComputerAction(action, coordinate, text);
      }
    }
  },
  experimental_toToolResultContent(result) {
    return typeof result === 'string'
      ? [{ type: 'text', text: result }]
      : [{ type: 'image', data: result.data, mediaType: 'image/png' }];
  },
});
```

----------------------------------------

TITLE: Client-Side UI Rendering Workflow
DESCRIPTION: This outlines the traditional six-step process for rendering user interfaces on the client side in response to language model generations. It details the flow from user prompt to the client's conditional rendering based on JSON objects returned by tool calls.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/06-advanced/07-rendering-ui-with-language-models.mdx#_snippet_6

LANGUAGE: APIDOC
CODE:
```
1. The user prompts the language model.
2. The language model generates a response that includes a tool call.
3. The tool call returns a JSON object that represents the user interface.
4. The response is sent to the client.
5. The client receives the response and checks if the latest message was a tool call.
6. If it was a tool call, the client renders the user interface based on the JSON object returned by the tool call.
```

----------------------------------------

TITLE: Initialize Portkey Provider Instance
DESCRIPTION: Demonstrates how to create a Portkey provider instance using the `createPortkey` function, including configuration for the AI provider, API key, and optional model overrides.
SOURCE: https://github.com/vercel/ai/blob/v5/content/providers/03-community-providers/10-portkey.mdx#_snippet_1

LANGUAGE: typescript
CODE:
```
import { createPortkey } from '@portkey-ai/vercel-provider';

const portkeyConfig = {
  provider: 'openai', //enter provider of choice
  api_key: 'OPENAI_API_KEY', //enter the respective provider's api key
  override_params: {
    model: 'gpt-4', //choose from 250+ LLMs
  },
};

const portkey = createPortkey({
  apiKey: 'YOUR_PORTKEY_API_KEY',
  config: portkeyConfig,
});
```

----------------------------------------

TITLE: Access UI State in Client Components with useUIState
DESCRIPTION: This example illustrates how to read and update the UI state within a Client Component using the `useUIState` hook provided by `@ai-sdk/rsc`. It demonstrates destructuring the hook's return value to get the current messages and a setter function. This allows dynamic rendering and interaction with the UI state on the client side.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/05-ai-sdk-rsc/03-generative-ui-state.mdx#_snippet_3

LANGUAGE: tsx
CODE:
```
'use client';

import { useUIState } from '@ai-sdk/rsc';

export default function Page() {
  const [messages, setMessages] = useUIState();

  return (
    <ul>
      {messages.map(message => (
        <li key={message.id}>{message.display}</li>
      ))}
    </ul>
  );
}
```

----------------------------------------

TITLE: FriendliAI Model Capabilities Overview
DESCRIPTION: Lists the capabilities of various FriendliAI models, including support for image input, object generation, tool usage, and tool streaming.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/08-friendliai.mdx#_snippet_11

LANGUAGE: APIDOC
CODE:
```
Model: deepseek-r1
  Image Input: No
  Object Generation: Yes
  Tool Usage: No
  Tool Streaming: No

Model: meta-llama-3.3-70b-instruct
  Image Input: No
  Object Generation: Yes
  Tool Usage: Yes
  Tool Streaming: Yes

Model: meta-llama-3.1-8b-instruct
  Image Input: No
  Object Generation: Yes
  Tool Usage: Yes
  Tool Streaming: Yes
```

----------------------------------------

TITLE: Generate Text Using Amazon Bedrock Model
DESCRIPTION: Demonstrates a complete example of importing the Bedrock provider and `generateText` function, then using a Bedrock model to generate text based on a given prompt.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/08-amazon-bedrock.mdx#_snippet_9

LANGUAGE: ts
CODE:
```
import { bedrock } from '@ai-sdk/amazon-bedrock';
import { generateText } from 'ai';

const { text } = await generateText({
  model: bedrock('meta.llama3-70b-instruct-v1:0'),
  prompt: 'Write a vegetarian lasagna recipe for 4 people.',
});
```

----------------------------------------

TITLE: OpenAI Provider Instance Configuration Options
DESCRIPTION: Detailed documentation for the optional settings available when initializing an OpenAI provider instance via `createOpenAI`.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/03-openai.mdx#_snippet_3

LANGUAGE: APIDOC
CODE:
```
createOpenAI(options?: object):
  options:
    baseURL: string
      Description: Use a different URL prefix for API calls, e.g. to use proxy servers. Default: https://api.openai.com/v1
    apiKey: string
      Description: API key that is being sent using the Authorization header. Default: OPENAI_API_KEY environment variable.
    name: string
      Description: The provider name. You can set this when using OpenAI compatible providers to change the model provider property. Default: openai
    organization: string
      Description: OpenAI Organization.
    project: string
      Description: OpenAI project.
    headers: Record<string, string>
      Description: Custom headers to include in the requests.
    fetch: (input: RequestInfo, init?: RequestInit) => Promise<Response>
      Description: Custom fetch implementation. Defaults to the global fetch function. Can be used as a middleware or for testing.
```

----------------------------------------

TITLE: Streaming AI-Generated Objects via API Route (AI SDK UI)
DESCRIPTION: This API route handles POST requests to stream AI-generated objects using `streamObject` and `toTextStreamResponse`. It's the recommended, modern approach for streaming structured data with AI SDK UI, replacing server actions for this purpose.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/05-ai-sdk-rsc/10-migrating-to-ui.mdx#_snippet_21

LANGUAGE: ts
CODE:
```
import { streamObject } from 'ai';
import { openai } from '@ai-sdk/openai';
import { notificationSchema } from '@/utils/schemas';

export async function POST(req: Request) {
  const context = await req.json();

  const result = streamObject({
    model: openai('gpt-4.1'),
    schema: notificationSchema,
    prompt:
      `Generate 3 notifications for a messages app in this context:` + context,
  });

  return result.toTextStreamResponse();
}
```

----------------------------------------

TITLE: UI Framework Support: Svelte
DESCRIPTION: Added tool calling support to `useChat` in Svelte.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/08-migration-guides/37-migration-guide-3-3.mdx#_snippet_18

LANGUAGE: APIDOC
CODE:
```
Svelte:
  - useChat(): Tool calling support added
```

----------------------------------------

TITLE: Server-side AI Chat Handler with Message Conversion (Outdated Solution)
DESCRIPTION: This TypeScript code snippet provides an example of a server-side `POST` API endpoint. It processes incoming chat messages, converts them to the `ModelMessage` format using `convertToModelMessages` (a solution for a past compatibility issue), and then streams text responses from an OpenAI model. This particular solution is noted as outdated, as the AI SDK now handles this conversion automatically.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/09-troubleshooting/10-use-chat-tools-no-response.mdx#_snippet_0

LANGUAGE: tsx
CODE:
```
import { openai } from '@ai-sdk/openai';
import { convertToModelMessages, streamText } from 'ai';

export async function POST(req: Request) {
  const { messages } = await req.json();

  const result = streamText({
    model: openai('gpt-4o'),
    messages: convertToModelMessages(messages),
  });

  return result.toUIMessageStreamResponse();
}
```

----------------------------------------

TITLE: Generate Image with DeepInfra AI SDK
DESCRIPTION: This TypeScript example demonstrates how to generate an image using the DeepInfra AI SDK. It imports the `deepinfra` provider and the `experimental_generateImage` function, then calls the function with a specified DeepInfra image model, a text prompt, and an aspect ratio. It highlights the use of the `deepinfra.image()` factory method for model selection.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/11-deepinfra.mdx#_snippet_5

LANGUAGE: typescript
CODE:
```
import { deepinfra } from '@ai-sdk/deepinfra';
import { experimental_generateImage as generateImage } from 'ai';

const { image } = await generateImage({
  model: deepinfra.image('stabilityai/sd3.5'),
  prompt: 'A futuristic cityscape at sunset',
  aspectRatio: '16:9',
});
```

----------------------------------------

TITLE: Encode Weave API Key to Base64 for Authorization
DESCRIPTION: This bash command takes your Weave API key, prefixes it with 'api:', and then base64 encodes the resulting string. The output is essential for constructing the authorization header used in OpenTelemetry configuration.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/05-observability/weave.mdx#_snippet_0

LANGUAGE: bash
CODE:
```
echo -n ""api:<YOUR_API_KEY>"" | base64
```

----------------------------------------

TITLE: OpenAIStream
DESCRIPTION: Transforms the response from OpenAI's language models into a readable stream.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/07-reference/04-stream-helpers/index.mdx#_snippet_3

LANGUAGE: APIDOC
CODE:
```
OpenAIStream:
  Description: Transforms the response from OpenAI's language models into a readable stream.
```

----------------------------------------

TITLE: Bash Tool Parameters (APIDOC)
DESCRIPTION: API documentation for the parameters of the Anthropic Bash Tool's `execute` method, detailing the `command` and `restart` arguments.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/16-google-vertex.mdx#_snippet_41

LANGUAGE: APIDOC
CODE:
```
Bash Tool Parameters:
- command (string): The bash command to run. Required unless the tool is being restarted.
- restart (boolean, optional): Specifying true will restart this tool.
```

----------------------------------------

TITLE: Create Baseten OpenAI Compatible Provider Instance
DESCRIPTION: Demonstrates how to initialize a custom Baseten provider instance using the `createOpenAICompatible` function. This involves setting the model ID, constructing the base URL, and configuring authorization headers with a Baseten API key.
SOURCE: https://github.com/vercel/ai/blob/v5/content/providers/02-openai-compatible-providers/40-baseten.mdx#_snippet_1

LANGUAGE: typescript
CODE:
```
import { createOpenAICompatible } from '@ai-sdk/openai-compatible';

const BASETEN_MODEL_ID = '<model-id>'; // e.g. 5q3z8xcw
const BASETEN_MODEL_URL = `https://model-${BASETEN_MODEL_ID}.api.baseten.co/environments/production/sync/v1`;

const baseten = createOpenAICompatible({
  name: 'baseten',
  baseURL: BASETEN_MODEL_URL,
  headers: {
    Authorization: `Bearer ${process.env.BASETEN_API_KEY ?? ''}`,
  },
});
```

----------------------------------------

TITLE: Implement Multi-Step Agents for Problem Solving with AI SDK
DESCRIPTION: This snippet shows how to create an agent using AI SDK's `maxSteps` parameter to solve complex problems like mathematical equations. It defines a `calculate` tool using `mathjs` for evaluating expressions and demonstrates how the agent can reason through a problem step by step, making multiple tool calls as needed to reach a solution.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/00-guides/21-llama-3_1.mdx#_snippet_5

LANGUAGE: TypeScript
CODE:
```
import { generateText, tool } from 'ai';
import { deepinfra } from '@ai-sdk/deepinfra';
import * as mathjs from 'mathjs';
import { z } from 'zod';

const problem =
  'Calculate the profit for a day if revenue is $5000 and expenses are $3500.';

const { text: answer } = await generateText({
  model: deepinfra('meta-llama/Meta-Llama-3.1-70B-Instruct'),
  system:
    'You are solving math problems. Reason step by step. Use the calculator when necessary.',
  prompt: problem,
  tools: {
    calculate: tool({
      description: 'A tool for evaluating mathematical expressions.',
      parameters: z.object({ expression: z.string() }),
      execute: async ({ expression }) => mathjs.evaluate(expression),
    }),
  },
  maxSteps: 5,
});
```

----------------------------------------

TITLE: Vercel AI Model Configuration Parameters Reference
DESCRIPTION: Detailed documentation for various configuration parameters available for Vercel AI models, including options for HTTP headers, maximum steps, telemetry, provider-specific settings, active tools, stop conditions, and step preparation functions.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/07-reference/01-ai-sdk-core/01-generate-text.mdx#_snippet_7

LANGUAGE: APIDOC
CODE:
```
Parameter: isOptional
  Type: true
  Description: Additional HTTP headers to be sent with the request. Only applicable for HTTP-based providers.

Parameter: maxSteps
  Type: number
  Optional: true
  Description: Maximum number of sequential LLM calls (steps), e.g. when you use tool calls. A maximum number is required to prevent infinite loops in the case of misconfigured tools. By default, it is set to 1.

Parameter: experimental_generateMessageId
  Type: () => string
  Optional: true
  Description: Function used to generate a unique ID for each message. This is an experimental feature.

Parameter: experimental_telemetry
  Type: TelemetrySettings
  Optional: true
  Description: Telemetry configuration. Experimental feature.
  Properties:
    - Type: TelemetrySettings
      Parameters:
        - name: isEnabled
          type: boolean
          optional: true
          description: Enable or disable telemetry. Disabled by default while experimental.
        - name: recordInputs
          type: boolean
          optional: true
          description: Enable or disable input recording. Enabled by default.
        - name: recordOutputs
          type: boolean
          optional: true
          description: Enable or disable output recording. Enabled by default.
        - name: functionId
          type: string
          optional: true
          description: Identifier for this function. Used to group telemetry data by function.
        - name: metadata
          type: Record<string, string | number | boolean | Array<null | undefined | string> | Array<null | undefined | number> | Array<null | undefined | boolean>>
          optional: true
          description: Additional information to include in the telemetry data.

Parameter: providerOptions
  Type: Record<string,Record<string,JSONValue>> | undefined
  Optional: true
  Description: Provider-specific options. The outer key is the provider name. The inner values are the metadata. Details depend on the provider.

Parameter: activeTools
  Type: Array<TOOLNAME> | undefined
  Optional: true
  Description: The tools that are currently active. All tools are active by default.

Parameter: stopWhen
  Type: StopCondition<TOOLS> | Array<StopCondition<TOOLS>>
  Optional: true
  Description: Condition for stopping the generation when there are tool results in the last step. When the condition is an array, any of the conditions can be met to stop the generation. Default: stepCountIs(1).

Parameter: prepareStep
  Type: (options: PrepareStepOptions) => PrepareStepResult<TOOLS> | Promise<PrepareStepResult<TOOLS>>
  Optional: true
  Description: Optional function that you can use to provide different settings for a step.
  Properties:
    - Type: PrepareStepFunction<TOOLS>
      Parameters:
        - name: options
          type: object
          description: The options for the step.
          Properties:
            - Type: PrepareStepOptions
              Parameters:
                - name: steps
                  type: Array<StepResult<TOOLS>>
                  description: The steps that have been executed so far.
                - name: stepNumber
                  type: number
                  description: The number of the step that is being executed.
                - name: model
                  type: LanguageModel
                  description: The model that is being used.

Parameter: experimental_repairToolCall
  Type: (options: ToolCallRepairOptions) => Promise<LanguageModelV2ToolCall | null>
  Optional: true
  Description: A function that attempts to repair a tool call that failed to parse. Return either a repaired tool call or null if the tool call cannot be repaired.
  Properties:
    - Type: ToolCallRepairOptions
      Parameters:
        - No further parameters specified in the provided text.
```

----------------------------------------

TITLE: Implement Server-Side Image Generation API with AI SDK
DESCRIPTION: This TypeScript code defines a Next.js API route (`/api/chat`) that processes incoming chat messages. It utilizes the AI SDK's `streamText` and `experimental_generateImage` functions to create a `generateImage` tool. This tool allows the language model to generate images based on a given prompt, returning the image data (base64) to the client. It also includes logic to redact image data from messages sent back to the model.
SOURCE: https://github.com/vercel/ai/blob/v5/content/cookbook/01-next/12-generate-image-with-chat-prompt.mdx#_snippet_0

LANGUAGE: typescript
CODE:
```
import { openai } from '@ai-sdk/openai';
import { experimental_generateImage, Message, streamText, tool } from 'ai';
import { z } from 'zod';

export async function POST(request: Request) {
  const { messages }: { messages: Message[] } = await request.json();

  // filter through messages and remove base64 image data to avoid sending to the model
  const formattedMessages = messages.map(m => {
    if (m.role === 'assistant' && m.toolInvocations) {
      m.toolInvocations.forEach(ti => {
        if (ti.toolName === 'generateImage' && ti.state === 'result') {
          ti.result.image = `redacted-for-length`;
        }
      });
    }
    return m;
  });

  const result = streamText({
    model: openai('gpt-4o'),
    messages: formattedMessages,
    tools: {
      generateImage: tool({
        description: 'Generate an image',
        parameters: z.object({
          prompt: z.string().describe('The prompt to generate the image from'),
        }),
        execute: async ({ prompt }) => {
          const { image } = await experimental_generateImage({
            model: openai.image('dall-e-3'),
            prompt,
          });
          // in production, save this image to blob storage and return a URL
          return { image: image.base64, prompt };
        },
      }),
    },
  });
  return result.toUIMessageStreamResponse();
}
```

----------------------------------------

TITLE: Configure OpenAI API Key
DESCRIPTION: Adds the OpenAI API key to the .env.local file. This key is required to authenticate your application with the OpenAI service and enable access to their models. Replace 'xxxxxxxxx' with your actual OpenAI API key.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-getting-started/03-nextjs-pages-router.mdx#_snippet_4

LANGUAGE: Environment Variables
CODE:
```
OPENAI_API_KEY=xxxxxxxxx
```

----------------------------------------

TITLE: Vercel AI SDK Function Parameters
DESCRIPTION: Defines the various parameters available for Vercel AI SDK functions, detailing their types, descriptions, and nested structures for complex inputs like messages, including system, user, assistant, and tool messages, and their content parts (text, image, file).
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/03-generate-object.mdx#_snippet_5

LANGUAGE: APIDOC
CODE:
```
model: LanguageModel
  Description: ""The language model to use. Example: openai('gpt-4.1')""
output: 'object' | 'array' | 'enum' | 'no-schema' | undefined
  Description: ""The type of output to generate. Defaults to 'object'.""
mode: 'auto' | 'json' | 'tool'
  Description: ""The mode to use for object generation. Not every model supports all modes. Defaults to 'auto' for 'object' output and to 'json' for 'no-schema' output. Must be 'json' for 'no-schema' output.""
schema: Zod Schema | JSON Schema
  Description: ""The schema that describes the shape of the object to generate. It is sent to the model to generate the object and used to validate the output. You can either pass in a Zod schema or a JSON schema (using the `jsonSchema` function). In 'array' mode, the schema is used to describe an array element. Not available with 'no-schema' or 'enum' output.""
schemaName: string | undefined
  Description: ""Optional name of the output that should be generated. Used by some providers for additional LLM guidance, e.g. via tool or schema name. Not available with 'no-schema' or 'enum' output.""
schemaDescription: string | undefined
  Description: ""Optional description of the output that should be generated. Used by some providers for additional LLM guidance, e.g. via tool or schema name. Not available with 'no-schema' or 'enum' output.""
enum: string[]
  Description: ""List of possible values to generate. Only available with 'enum' output.""
system: string
  Description: ""The system prompt to use that specifies the behavior of the model.""
prompt: string
  Description: ""The input prompt to generate the text from.""
messages: Array<SystemModelMessage | UserModelMessage | AssistantModelMessage | ToolModelMessage> | Array<UIMessage>
  Description: ""A list of messages that represent a conversation. Automatically converts UI messages from the useChat hook.""
  Types:
    SystemModelMessage:
      role: ""'system'""
        Description: ""The role for the system message.""
      content: ""string""
        Description: ""The content of the message.""
    UserModelMessage:
      role: ""'user'""
        Description: ""The role for the user message.""
      content: ""string | Array<TextPart | ImagePart | FilePart>""
        Description: ""The content of the message.""
      Content Parts:
        TextPart:
          type: ""'text'""
            Description: ""The type of the message part.""
          text: ""string""
            Description: ""The text content of the message part.""
        ImagePart:
          type: ""'image'""
            Description: ""The type of the message part.""
          image: ""string | Uint8Array | Buffer | ArrayBuffer | URL""
            Description: ""The image content of the message part. String are either base64 encoded content, base64 data URLs, or http(s) URLs.""
          mediaType: ""string""
            Description: ""The IANA media type of the image. Optional.""
        FilePart:
          type: ""'file'""
            Description: ""The type of the message part.""
```

----------------------------------------

TITLE: Generate Text with Qwen Language Model
DESCRIPTION: Example demonstrating how to use a Qwen language model with the AI SDK's 'generateText' function to produce text based on a prompt.
SOURCE: https://github.com/vercel/ai/blob/v5/content/providers/03-community-providers/02-qwen.mdx#_snippet_5

LANGUAGE: ts
CODE:
```
import { qwen } from 'qwen-ai-provider';
import { generateText } from 'ai';

const { text } = await generateText({
  model: qwen('qwen-plus'),
  prompt: 'Write a vegetarian lasagna recipe for 4 people.',
});
```

----------------------------------------

TITLE: AWSBedrockAnthropicMessagesStream API Signature
DESCRIPTION: Detailed API documentation for the AWSBedrockAnthropicMessagesStream function, including its parameters and return type.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/07-reference/04-stream-helpers/10-aws-bedrock-messages-stream.mdx#_snippet_1

LANGUAGE: APIDOC
CODE:
```
AWSBedrockAnthropicMessagesStream(
  response: AWSBedrockResponse,
  callbacks?: AIStreamCallbacksAndOptions
) => ReadableStream

Parameters:
  response: AWSBedrockResponse
    Description: The response object returned from AWS Bedrock.
    Properties:
      body?: AsyncIterable<{ chunk?: { bytes?: Uint8Array } }>
        Description: An optional async iterable of objects containing optional binary data chunks.

  callbacks?: AIStreamCallbacksAndOptions
    Description: An object containing callback functions to handle the start, each token, and completion of the AI response. In the absence of this parameter, default behavior is implemented.
    Properties:
      onStart?: () => Promise<void>
        Description: An optional function that is called at the start of the stream processing.
      onCompletion?: (completion: string) => Promise<void>
        Description: An optional function that is called for every completion. It's passed the completion as a string.
      onFinal?: (completion: string) => Promise<void>
        Description: An optional function that is called once when the stream is closed with the final completion message.
      onToken?: (token: string) => Promise<void>
        Description: An optional function that is called for each token in the stream. It's passed the token as a string.

Returns:
  ReadableStream
```

----------------------------------------

TITLE: Start Development Server
DESCRIPTION: Starts the Next.js development server, making the application accessible locally.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/00-guides/04-natural-language-postgres.mdx#_snippet_5

LANGUAGE: Bash
CODE:
```
pnpm run dev
```

----------------------------------------

TITLE: Implement Custom AI SDK LanguageModelV2
DESCRIPTION: This TypeScript class, `CustomChatLanguageModel`, implements the `LanguageModelV2` interface from the AI SDK. It provides methods for handling chat generations (`doGenerate`) and streaming (`doStream`) by converting AI SDK prompts to a custom provider's format, making API calls, and then converting the provider's response back to the AI SDK's format. It includes logic for handling messages, tools, and managing API requests and responses.
SOURCE: https://github.com/vercel/ai/blob/v5/content/providers/03-community-providers/01-custom-providers.mdx#_snippet_23

LANGUAGE: typescript
CODE:
```
import { LanguageModelV2, LanguageModelV2CallOptions } from '@ai-sdk/provider';
import { postJsonToApi } from '@ai-sdk/provider-utils';

class CustomChatLanguageModel implements LanguageModelV2 {
  readonly specificationVersion = 'V2';
  readonly provider: string;
  readonly modelId: string;

  constructor(
    modelId: string,
    settings: CustomChatSettings,
    config: CustomChatConfig,
  ) {
    this.provider = config.provider;
    this.modelId = modelId;
    // Initialize with settings and config
  }

  // Convert AI SDK prompt to provider format
  private getArgs(options: LanguageModelV2CallOptions) {
    const warnings: LanguageModelV2CallWarning[] = [];

    // Map messages to provider format
    const messages = this.convertToProviderMessages(options.prompt);

    // Handle tools if provided
    const tools = options.tools
      ? this.prepareTools(options.tools, options.toolChoice)
      : undefined;

    // Build request body
    const body = {
      model: this.modelId,
      messages,
      temperature: options.temperature,
      max_tokens: options.maxOutputTokens,
      stop: options.stopSequences,
      tools,
      // ... other parameters
    };

    return { args: body, warnings };
  }

  async doGenerate(options: LanguageModelV2CallOptions) {
    const { args, warnings } = this.getArgs(options);

    // Make API call
    const response = await postJsonToApi({
      url: `${this.config.baseURL}/chat/completions`,
      headers: this.config.headers(),
      body: args,
      abortSignal: options.abortSignal,
    });

    // Convert provider response to AI SDK format
    const content: LanguageModelV2Content[] = [];

    // Extract text content
    if (response.choices[0].message.content) {
      content.push({
        type: 'text',
        text: response.choices[0].message.content,
      });
    }

    // Extract tool calls
    if (response.choices[0].message.tool_calls) {
      for (const toolCall of response.choices[0].message.tool_calls) {
        content.push({
          type: 'tool-call',
          toolCallType: 'function',
          toolCallId: toolCall.id,
          toolName: toolCall.function.name,
          args: JSON.stringify(toolCall.function.arguments),
        });
      }
    }

    return {
      content,
      finishReason: this.mapFinishReason(response.choices[0].finish_reason),
      usage: {
        inputTokens: response.usage?.prompt_tokens,
        outputTokens: response.usage?.completion_tokens,
        totalTokens: response.usage?.total_tokens,
      },
      request: { body: args },
      response: { body: response },
      warnings,
    };
  }

  async doStream(options: LanguageModelV2CallOptions) {
    const { args, warnings } = this.getArgs(options);

    // Create streaming response
    const response = await fetch(`${this.config.baseURL}/chat/completions`, {
      method: 'POST',
      headers: {
        ...this.config.headers(),
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({ ...args, stream: true }),
      signal: options.abortSignal,
    });

    // Transform stream to AI SDK format
    const stream = response
      .body!.pipeThrough(new TextDecoderStream())
      .pipeThrough(this.createParser())
      .pipeThrough(this.createTransformer(warnings));

    return { stream, warnings };
  }

  // Supported URL patterns for native file handling
  get supportedUrls() {
    return {
      'image/*': [/^https:\/\/example\.com\/images\/.*/],
    };
  }
}
```

----------------------------------------

TITLE: OpenAIResponsesProviderOptions API Reference
DESCRIPTION: Documents the available configuration options for the OpenAI responses provider. These options allow customization of model behavior, including tool call handling, data storage, and user identification.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/03-openai.mdx#_snippet_26

LANGUAGE: APIDOC
CODE:
```
OpenAIResponsesProviderOptions:
  parallelToolCalls: boolean
    Whether to use parallel tool calls. Defaults to `true`.
  store: boolean
    Whether to store the generation. Defaults to `true`.
  metadata: Record<string, string>
    Additional metadata to store with the generation.
  previousResponseId: string
    The ID of the previous response. You can use it to continue a conversation. Defaults to `undefined`.
  instructions: string
    Instructions for the model. They can be used to change the system or developer message when continuing a conversation using the `previousResponseId` option. Defaults to `undefined`.
  user: string
    A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. Defaults to `undefined`.
  reasoningEffort: 'low' | 'medium' | 'high'
    Reasoning effort for reasoning models. Defaults to `medium`. If you use `providerOptions` to set the `reasoningEffort` option, this model setting will be ignored.
  reasoningSummary: 'auto' | 'detailed'
    Controls whether the model returns its reasoning process. Set to `'auto'` for a condensed summary, `'detailed'` for more comprehensive reasoning. Defaults to `undefined` (no reasoning summaries). When enabled, reasoning summaries appear in the stream as events with type `'reasoning'` and in non-streaming responses within the `reasoning` field.
  strictJsonSchema: boolean
    Whether to use strict JSON schema validation. Defaults to `false`.
  serviceTier: 'auto' | 'flex'
    Service tier for the request. Set to 'flex' for 50% cheaper processing
    at the cost of increased latency. Only available for o3 and o4-mini models.
    Defaults to 'auto'.
```

----------------------------------------

TITLE: ElevenLabs Provider Instance Configuration Options
DESCRIPTION: Detailed API documentation for the optional settings available when creating a custom ElevenLabs provider instance, including authentication, custom headers, and fetch implementation overrides.
SOURCE: https://github.com/vercel/ai/blob/v5/content/providers/01-ai-sdk-providers/90-elevenlabs.mdx#_snippet_3

LANGUAGE: APIDOC
CODE:
```
createElevenLabs(options?: object): ElevenLabsProvider
  options:
    apiKey: string
      API key that is being sent using the `Authorization` header.
      It defaults to the `ELEVENLABS_API_KEY` environment variable.
    headers: Record<string,string>
      Custom headers to include in the requests.
    fetch: (input: RequestInfo, init?: RequestInit) => Promise<Response>
      Custom fetch implementation.
      Defaults to the global `fetch` function.
      You can use it as a middleware to intercept requests,
      or to provide a custom fetch implementation for e.g. testing.
```

----------------------------------------

TITLE: Example Output of Basic Text Generation
DESCRIPTION: An example of the text output generated by the basic text generation snippet.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/96-sambanova.mdx#_snippet_6

LANGUAGE: Text
CODE:
```
Hello. Nice to meet you too. Is there something I can help you with or would you like to chat?
```

----------------------------------------

TITLE: Basic Usage of simulateReadableStream (TypeScript)
DESCRIPTION: An example demonstrating the simplest use case of `simulateReadableStream` to create a stream from an array of chunks without any explicit delays.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/07-reference/01-ai-sdk-core/75-simulate-readable-stream.mdx#_snippet_3

LANGUAGE: ts
CODE:
```
const stream = simulateReadableStream({
  chunks: ['Hello', ' ', 'World'],
});
```

----------------------------------------

TITLE: API: TextStreamPart - Tool Call
DESCRIPTION: Defines the structure for an initial tool call part in a streaming response. It includes the tool's identifier, name, and arguments.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/02-stream-text.mdx#_snippet_7

LANGUAGE: APIDOC
CODE:
```
Type: TextStreamPart
  Parameters:
    - name: type
      type: ""'tool-call'""
      description: The type to identify the object as tool call.
    - name: toolCallId
      type: string
      description: The id of the tool call.
    - name: toolName
      type: string
      description: The name of the tool, which typically would be the name of the function.
    - name: args
      type: object based on zod schema
      description: Parameters generated by the model to be used by the tool.
```

----------------------------------------

TITLE: Experimental Partial Output Stream API
DESCRIPTION: Documents the `experimental_partialOutputStream`, an `AsyncIterableStream` for partial outputs based on the `experimental_output` specification. It details the stream's type and purpose.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/02-stream-text.mdx#_snippet_25

LANGUAGE: APIDOC
CODE:
```
experimental_partialOutputStream:
  type: 'AsyncIterableStream<PARTIAL_OUTPUT>'
  description: 'A stream of partial outputs. It uses the `experimental_output` specification. AsyncIterableStream is defined as AsyncIterable<T> & ReadableStream<T>.'
```

----------------------------------------

TITLE: Start Next.js Development Server
DESCRIPTION: Command to start the Next.js development server, typically accessible at http://localhost:3000.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/00-guides/01-rag-chatbot.mdx#_snippet_18

LANGUAGE: shell
CODE:
```
pnpm run dev
```

----------------------------------------

TITLE: Groq Provider Instance Configuration Options
DESCRIPTION: Defines the configurable parameters for the `createGroq` function, including API endpoint, authentication, custom headers, and fetch implementation.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/09-groq.mdx#_snippet_3

LANGUAGE: APIDOC
CODE:
```
createGroq options:\n  baseURL: string\n    Description: Use a different URL prefix for API calls, e.g. to use proxy servers.\n    Default: https://api.groq.com/openai/v1\n  apiKey: string\n    Description: API key that is being sent using the \`Authorization\` header.\n    Default: GROQ_API_KEY environment variable\n  headers: Record<string,string>\n    Description: Custom headers to include in the requests.\n  fetch: (input: RequestInfo, init?: RequestInit) => Promise<Response>\n    Description: Custom \`fetch\` implementation. Defaults to the global \`fetch\` function. Can be used as a middleware or for testing.
```

----------------------------------------

TITLE: Rev.ai Provider Instance Configuration Options
DESCRIPTION: Documentation for the optional settings available when creating a customized Rev.ai provider instance using `createRevai`.
SOURCE: https://github.com/vercel/ai/blob/v5/content/providers/01-ai-sdk-providers/160-revai.mdx#_snippet_3

LANGUAGE: APIDOC
CODE:
```
RevaiProviderOptions:
  apiKey: string
    description: API key that is being sent using the Authorization header. It defaults to the REVAI_API_KEY environment variable.
  headers: Record<string, string>
    description: Custom headers to include in the requests.
  fetch: (input: RequestInfo, init?: RequestInit) => Promise<Response>
    description: Custom fetch implementation. Defaults to the global fetch function. Can be used as a middleware to intercept requests, or to provide a custom fetch implementation for e.g. testing.
```

----------------------------------------

TITLE: Transcribe Audio with AssemblyAI Provider and AI SDK
DESCRIPTION: An example showing how to use the AssemblyAI provider with the AI SDK's experimental transcribe function to transcribe an audio file from a URL. It illustrates model selection and audio input.
SOURCE: https://github.com/vercel/ai/blob/main/packages/assemblyai/README.md#_snippet_2

LANGUAGE: ts
CODE:
```
import { assemblyai } from '@ai-sdk/assemblyai';
import { experimental_transcribe as transcribe } from 'ai';

const { text } = await transcribe({
  model: assemblyai.transcription('best'),
  audio: new URL(
    'https://github.com/vercel/ai/raw/refs/heads/main/examples/ai-core/data/galileo.mp3',
  ),
});
```

----------------------------------------

TITLE: Using simulateStreamingMiddleware with streamText
DESCRIPTION: Illustrates a practical usage of `simulateStreamingMiddleware` by wrapping a non-streaming language model within `wrapLanguageModel` and integrating it with `streamText`. This example shows how to process the resulting simulated stream.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/07-reference/01-ai-sdk-core/67-simulate-streaming-middleware.mdx#_snippet_2

LANGUAGE: ts
CODE:
```
import { streamText } from 'ai';
import { wrapLanguageModel } from 'ai';
import { simulateStreamingMiddleware } from 'ai';

// Example with a non-streaming model
const result = streamText({
  model: wrapLanguageModel({
    model: nonStreamingModel,
    middleware: simulateStreamingStreamingMiddleware(),
  }),
  prompt: 'Your prompt here',
});

// Now you can use the streaming interface
for await (const chunk of result.fullStream) {
  // Process streaming chunks
}
```

----------------------------------------

TITLE: AI SDK UI: Custom Fetch and Request Body
DESCRIPTION: Added support for custom fetch functions and request body customization, offering greater control over API interactions.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/08-migration-guides/37-migration-guide-3-3.mdx#_snippet_5

LANGUAGE: APIDOC
CODE:
```
API Interactions:
  - Custom fetch functions support
  - Request body customization support
```

----------------------------------------

TITLE: OpenAI Transcription Provider Options API Reference
DESCRIPTION: Details the available `providerOptions` for OpenAI transcription models, including `timestampGranularities`, `language`, `prompt`, `temperature`, and `include`, along with their types, descriptions, and default values.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/03-openai.mdx#_snippet_47

LANGUAGE: APIDOC
CODE:
```
timestampGranularities: string[]
  The granularity of the timestamps in the transcription.
  Defaults to ['segment'].
  Possible values are ['word'], ['segment'], and ['word', 'segment'].
  Note: There is no additional latency for segment timestamps, but generating word timestamps incurs additional latency.
language: string
  The language of the input audio. Supplying the input language in ISO-639-1 format (e.g., 'en') will improve accuracy and latency.
  Optional.
prompt: string
  An optional text to guide the model's style or continue a previous audio segment. The prompt should match the audio language.
  Optional.
temperature: number
  The sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. If set to 0, the model will use log probability to automatically increase the temperature until certain thresholds are hit.
  Defaults to 0.
  Optional.
include: string[]
  Additional information to include in the transcription response.
```

----------------------------------------

TITLE: Transcribe Audio with Deepgram and AI SDK
DESCRIPTION: An example demonstrating how to use the Deepgram provider with the AI SDK's `experimental_transcribe` function to transcribe an audio file from a URL. It shows how to specify the Deepgram model and provide the audio source.
SOURCE: https://github.com/vercel/ai/blob/main/packages/deepgram/README.md#_snippet_2

LANGUAGE: ts
CODE:
```
import { deepgram } from '@ai-sdk/deepgram';
import { experimental_transcribe as transcribe } from 'ai';

const { text } = await transcribe({
  model: deepgram.transcription('nova-3'),
  audio: new URL(
    'https://github.com/vercel/ai/raw/refs/heads/main/examples/ai-core/data/galileo.mp3',
  ),
});
```

----------------------------------------

TITLE: Customize Headers for OpenAI Compatible Provider Authentication
DESCRIPTION: Illustrates how to pass custom headers, such as an `Authorization` bearer token, to the `createOpenAICompatible` function for API key authentication.
SOURCE: https://github.com/vercel/ai/blob/main/packages/openai-compatible/README.md#_snippet_3

LANGUAGE: ts
CODE:
```
import { createOpenAICompatible } from '@ai-sdk/openai-compatible';
import { generateText } from 'ai';

const { text } = await generateText({
  model: createOpenAICompatible({
    baseURL: 'https://api.example.com/v1',
    name: 'example',
    headers: {
      Authorization: `Bearer ${process.env.MY_API_KEY}`,
    },
  }).chatModel('meta-llama/Llama-3-70b-chat-hf'),
  prompt: 'Write a vegetarian lasagna recipe for 4 people.',
});
```

----------------------------------------

TITLE: Create Ollama Embedding Model
DESCRIPTION: Create an embedding model instance that calls the Ollama embeddings API using the `.embedding()` factory method and a specified model ID.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/03-ollama.mdx#_snippet_5

LANGUAGE: ts
CODE:
```
const model = ollama.embedding('nomic-embed-text');
```

----------------------------------------

TITLE: Run a Specific AI SDK End-to-End Test File
DESCRIPTION: This command allows developers to execute tests contained within a single end-to-end test file, such as `google.test.ts`. It's useful for focused debugging or verifying changes related to a particular provider.
SOURCE: https://github.com/vercel/ai/blob/v5/examples/ai-core/README.md#_snippet_4

LANGUAGE: sh
CODE:
```
pnpm run test:file src/e2e/google.test.ts
```

----------------------------------------

TITLE: Error Part Format and Example
DESCRIPTION: Defines the structure and provides an example of the error part, which is appended to messages when an error occurs during processing.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/04-ai-sdk-ui/50-stream-protocol.mdx#_snippet_3

LANGUAGE: APIDOC
CODE:
```
Format: 3:string
Example: 3:""error message""
```

----------------------------------------

TITLE: Run Stdio Transport Client
DESCRIPTION: Executes the client for the stdio transport. This client demonstrates communication using standard input/output streams.
SOURCE: https://github.com/vercel/ai/blob/main/examples/mcp/README.md#_snippet_5

LANGUAGE: sh
CODE:
```
pnpm stdio:client
```

----------------------------------------

TITLE: Gladia Provider Instance Configuration Options
DESCRIPTION: Details the optional settings available for customizing the Gladia provider instance, including API key, custom headers, and a custom fetch implementation.
SOURCE: https://github.com/vercel/ai/blob/v5/content/providers/01-ai-sdk-providers/120-gladia.mdx#_snippet_3

LANGUAGE: APIDOC
CODE:
```
GladiaProviderSettings:
  apiKey: string
    description: API key that is being sent using the `Authorization` header. It defaults to the `DEEPGRAM_API_KEY` environment variable.
  headers: Record<string, string>
    description: Custom headers to include in the requests.
  fetch: (input: RequestInfo, init?: RequestInit) => Promise<Response>
    description: Custom [fetch](https://developer.mozilla.org/en-US/docs/Web/API/fetch) implementation. Defaults to the global `fetch` function. You can use it as a middleware to intercept requests, or to provide a custom fetch implementation for e.g. testing.
```

----------------------------------------

TITLE: Stream Text with Baseten Model
DESCRIPTION: An example showcasing how to use a Baseten model with the `streamText` function from the AI SDK. This allows for receiving text output in a streaming fashion, processing chunks as they become available.
SOURCE: https://github.com/vercel/ai/blob/v5/content/providers/02-openai-compatible-providers/40-baseten.mdx#_snippet_4

LANGUAGE: typescript
CODE:
```
import { createOpenAICompatible } from '@ai-sdk/openai-compatible';
import { streamText } from 'ai';

const BASETEN_MODEL_ID = '<model-id>'; // e.g. 5q3z8xcw
const BASETEN_MODEL_URL = `https://model-${BASETEN_MODEL_ID}.api.baseten.co/environments/production/sync/v1`;

const baseten = createOpenAICompatible({
  name: 'baseten',
  baseURL: BASETEN_MODEL_URL,
  headers: {
    Authorization: `Bearer ${process.env.BASETEN_API_KEY ?? ''}`,
  },
});

const result = streamText({
  model: baseten('llama'),
  prompt: 'Tell me about yourself in one sentence',
});

for await (const message of result.textStream) {
  console.log(message);
}
```

----------------------------------------

TITLE: readStreamableValue API Signature
DESCRIPTION: Defines the parameters and return type for the `readStreamableValue` function, detailing the expected input stream and the asynchronous iterator it yields.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/07-reference/03-ai-sdk-rsc/05-read-streamable-value.mdx#_snippet_3

LANGUAGE: APIDOC
CODE:
```
readStreamableValue(stream: StreamableValue): AsyncIterable<any>

Parameters:
  stream: StreamableValue
    description: The streamable value to read from.

Returns:
  An async iterator that contains the values emitted by the streamable value.
```

----------------------------------------

TITLE: Stream Text with FriendliAI Built-in Tools
DESCRIPTION: Demonstrates how to use FriendliAI's built-in tools like web search and calculator with `streamText` from the AI SDK. This allows models to generate more accurate and up-to-date answers by leveraging external capabilities.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/08-friendliai.mdx#_snippet_8

LANGUAGE: typescript
CODE:
```
import { friendli } from '@friendliai/ai-provider';
import { streamText } from 'ai';

const result = streamText({
  model: friendli('meta-llama-3.3-70b-instruct', {
    tools: [{ type: 'web:search' }, { type: 'math:calculator' }],
  }),
  prompt:
    'Find the current USD to CAD exchange rate and calculate how much $5,000 USD would be in Canadian dollars.',
});

for await (const textPart of result.textStream) {
  console.log(textPart);
}
```

----------------------------------------

TITLE: Generate Dynamic API URLs for Expo Applications
DESCRIPTION: This utility function, `generateAPIUrl`, constructs the correct API endpoint URL based on the application's environment (development or production). It leverages `expo-constants` to determine the base URL and ensures that `EXPO_PUBLIC_API_BASE_URL` is set for production deployments, facilitating correct API communication for streaming.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-getting-started/07-expo.mdx#_snippet_7

LANGUAGE: ts
CODE:
```
import Constants from 'expo-constants';

export const generateAPIUrl = (relativePath: string) => {
  const origin = Constants.experienceUrl.replace('exp://', 'http://');

  const path = relativePath.startsWith('/') ? relativePath : `/${relativePath}`;

  if (process.env.NODE_ENV === 'development') {
    return origin.concat(path);
  }

  if (!process.env.EXPO_PUBLIC_API_BASE_URL) {
    throw new Error(
      'EXPO_PUBLIC_API_BASE_URL environment variable is not defined',
    );
  }

  return process.env.EXPO_PUBLIC_API_BASE_URL.concat(path);
};
```

----------------------------------------

TITLE: AI SDK UI: Stream Data Protocol Documentation
DESCRIPTION: Documented the stream data protocol for `useChat` and `useCompletion`, allowing developers to use these functions with any backend. The protocol also enables custom frontends with `streamText`.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/08-migration-guides/37-migration-guide-3-3.mdx#_snippet_4

LANGUAGE: APIDOC
CODE:
```
Stream Data Protocol:
  - Supported Hooks: useChat(), useCompletion()
  - Purpose: Enables custom backends and frontends with streamText()
```

----------------------------------------

TITLE: Configure OpenAI API Key
DESCRIPTION: Sets the OpenAI API key in the '.env.local' file for authentication with the OpenAI service. Replace 'xxxxxxxxx' with your actual key.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-getting-started/07-expo.mdx#_snippet_4

LANGUAGE: Shell
CODE:
```
OPENAI_API_KEY=xxxxxxxxx
```

----------------------------------------

TITLE: Abort Current API Request
DESCRIPTION: Provides a function to programmatically abort an ongoing API request, useful for managing long-running operations or user cancellations.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/07-reference/02-ai-sdk-ui/01-use-chat.mdx#_snippet_10

LANGUAGE: APIDOC
CODE:
```
abort: () => void
  description: Function to abort the current API request.
```

----------------------------------------

TITLE: Generate Text with xAI Grok Provider and AI SDK
DESCRIPTION: An example demonstrating how to use the xAI Grok provider with the AI SDK's 'generateText' function to create a text-based response based on a given prompt.
SOURCE: https://github.com/vercel/ai/blob/main/packages/xai/README.md#_snippet_2

LANGUAGE: ts
CODE:
```
import { xai } from '@ai-sdk/xai';
import { generateText } from 'ai';

const { text } = await generateText({
  model: xai('grok-3-beta'),
  prompt: 'Write a vegetarian lasagna recipe for 4 people.',
});
```

----------------------------------------

TITLE: Handling Agent Step Completion with onStepFinish Callback (TypeScript)
DESCRIPTION: This TypeScript example demonstrates the use of the `onStepFinish` callback in `generateText` to receive notifications when each agent step is completed. This callback provides access to the step's text, tool calls, tool results, finish reason, and usage, enabling real-time logging, chat history saving, or custom logic execution during agent operation.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-foundations/06-agents.mdx#_snippet_8

LANGUAGE: tsx
CODE:
```
import { generateText, stepCountIs } from 'ai';

const result = await generateText({
  model: 'openai/gpt-4.1',
  stopWhen: stepCountIs(10),
  onStepFinish({ text, toolCalls, toolResults, finishReason, usage }) {
    // your own logic, e.g. for saving the chat history or recording usage
  },
  // ...
});
```

----------------------------------------

TITLE: Generate Text with OpenAI Compatible Provider
DESCRIPTION: Demonstrates how to use `createOpenAICompatible` to configure a model with a custom base URL, name, and API key, then generate text using `ai`'s `generateText` function.
SOURCE: https://github.com/vercel/ai/blob/main/packages/openai-compatible/README.md#_snippet_2

LANGUAGE: ts
CODE:
```
import { createOpenAICompatible } from '@ai-sdk/openai-compatible';
import { generateText } from 'ai';

const { text } = await generateText({
  model: createOpenAICompatible({
    baseURL: 'https://api.example.com/v1',
    name: 'example',
    apiKey: process.env.MY_API_KEY,
  }).chatModel('meta-llama/Llama-3-70b-chat-hf'),
  prompt: 'Write a vegetarian lasagna recipe for 4 people.',
});
```

----------------------------------------

TITLE: Together.ai General Model Capabilities Overview
DESCRIPTION: Provides a detailed table outlining the capabilities of various Together.ai language models, indicating support for features such as image input, object generation, tool usage, and tool streaming.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/24-togetherai.mdx#_snippet_7

LANGUAGE: APIDOC
CODE:
```
Model Capabilities:
  meta-llama/Meta-Llama-3.3-70B-Instruct-Turbo:
    Image Input: No
    Object Generation: No
    Tool Usage: No
    Tool Streaming: No
  meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo:
    Image Input: No
    Object Generation: No
    Tool Usage: Yes
    Tool Streaming: Yes
  mistralai/Mixtral-8x22B-Instruct-v0.1:
    Image Input: No
    Object Generation: Yes
    Tool Usage: Yes
    Tool Streaming: Yes
  mistralai/Mistral-7B-Instruct-v0.3:
    Image Input: No
    Object Generation: Yes
    Tool Usage: Yes
    Tool Streaming: Yes
  deepseek-ai/DeepSeek-V3:
    Image Input: No
    Object Generation: No
    Tool Usage: No
    Tool Streaming: No
  google/gemma-2b-it:
    Image Input: No
    Object Generation: No
    Tool Usage: No
    Tool Streaming: No
  Qwen/Qwen2.5-72B-Instruct-Turbo:
    Image Input: No
    Object Generation: No
    Tool Usage: No
    Tool Streaming: No
  databricks/dbrx-instruct:
    Image Input: No
    Object Generation: No
    Tool Usage: No
    Tool Streaming: No
```

----------------------------------------

TITLE: Utilize AI SDK Tool Type Inference Helpers (InferToolInput, InferToolOutput)
DESCRIPTION: This example showcases new utility types like `InferToolInput`, `InferToolOutput`, and `InferUITool` in AI SDK 5.0. These types simplify extracting input and output types directly from tool definitions, enhancing type safety when defining UI message structures.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/01-announcing-ai-sdk-5-beta/index.mdx#_snippet_14

LANGUAGE: tsx
CODE:
```
import { InferToolInput, InferToolOutput, InferUITool } from 'ai';
import { weatherTool } from './weatherTool';

// Infer input and output types from tool definitions
type WeatherInput = InferToolInput<typeof weatherTool>;
type WeatherOutput = InferToolOutput<typeof weatherTool>;
type WeatherUITool = InferUITool<typeof weatherTool>;

// Use in UI message type definitions
type MyUIMessage = UIMessage<
  never, // metadata type
  UIDataTypes, // data parts type
  {
    weather: WeatherUITool;
  }
>;
```

----------------------------------------

TITLE: Set Zhipu AI API Key Environment Variable
DESCRIPTION: Configure your Zhipu AI API key as an environment variable for automatic detection by the provider.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/95-zhipu.mdx#_snippet_1

LANGUAGE: bash
CODE:
```
ZHIPU_API_KEY=<your-api-key>
```

----------------------------------------

TITLE: Implement Enhanced Tool Streaming Callbacks in AI SDK 5.0
DESCRIPTION: This example illustrates the new fine-grained streaming callbacks (`onInputStart`, `onInputDelta`, `onInputAvailable`) available for AI SDK tools. These callbacks enable real-time monitoring and processing of tool input as it streams, offering greater control and visibility during execution.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/01-announcing-ai-sdk-5-beta/index.mdx#_snippet_16

LANGUAGE: tsx
CODE:
```
const weatherTool = tool({
  inputSchema: z.object({ city: z.string() }),
  onInputStart: ({ toolCallId }) => {
    console.log('Tool input streaming started:', toolCallId);
  },
  onInputDelta: ({ inputTextDelta, toolCallId }) => {
    console.log('Tool input delta:', inputTextDelta);
  },
  onInputAvailable: ({ input, toolCallId }) => {
    console.log('Tool input ready:', input);
  },
  execute: async ({ city }) => {
    return `Weather in ${city}: sunny, 72°F`;
  }
});
```

----------------------------------------

TITLE: Configure LangWatch API Key
DESCRIPTION: Methods to provide the LangWatch API key to your application, either via environment variables or directly in client parameters for initialization.
SOURCE: https://github.com/vercel/ai/blob/v5/content/providers/05-observability/langwatch.mdx#_snippet_1

LANGUAGE: bash
CODE:
```
LANGWATCH_API_KEY='your_api_key_here'
```

LANGUAGE: typescript
CODE:
```
import { LangWatch } from 'langwatch';

const langwatch = new LangWatch({
  apiKey: 'your_api_key_here',
});
```

----------------------------------------

TITLE: Initialize Sarvam AI Provider Instance
DESCRIPTION: Demonstrates how to initialize the Sarvam AI provider instance using `createSarvam`. Requires a Sarvam API Key, which should be passed in the headers, preferably as an environment variable for security.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/97-sarvam.mdx#_snippet_1

LANGUAGE: TypeScript
CODE:
```
import { createSarvam } from 'sarvam-ai-provider';

const sarvam = createSarvam({
  headers: {
    'api-subscription-key': 'YOUR_API_KEY',
  },
});
```

----------------------------------------

TITLE: xAI Live Search Parameters
DESCRIPTION: Describes the available parameters for configuring Live Search functionality, including `mode`, `returnCitations`, `fromDate`, `toDate`, `maxSearchResults`, and `sources`.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/01-xai.mdx#_snippet_9

LANGUAGE: APIDOC
CODE:
```
mode: 'auto' | 'on' | 'off'
  Search mode preference:
  - 'auto' (default): Model decides whether to search
  - 'on': Always enables search
  - 'off': Disables search completely
returnCitations: boolean
  Whether to return citations in the response. Defaults to true.
fromDate: string
  Start date for search data in ISO8601 format (YYYY-MM-DD).
toDate: string
  End date for search data in ISO8601 format (YYYY-MM-DD).
maxSearchResults: number
  Maximum number of search results to consider. Defaults to 20, max 50.
sources: Array<SearchSource>
  Data sources to search from. Defaults to [""web"", ""x""] if not specified.
```

----------------------------------------

TITLE: AI SDK Core: Custom Request Headers
DESCRIPTION: Implemented support for sending custom request headers, enabling more tailored API requests.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/08-migration-guides/37-migration-guide-3-3.mdx#_snippet_6

LANGUAGE: APIDOC
CODE:
```
Core API:
  - Custom Request Headers: Supported for tailored API requests
```

----------------------------------------

TITLE: Set FriendliAI API Token Environment Variable
DESCRIPTION: Set the `FRIENDLI_TOKEN` environment variable with your personal access token obtained from the Friendli suite to authenticate with the FriendliAI API.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/08-friendliai.mdx#_snippet_1

LANGUAGE: bash
CODE:
```
export FRIENDLI_TOKEN=""YOUR_FRIENDLI_TOKEN""
```

----------------------------------------

TITLE: Create Custom Luma Provider Instance
DESCRIPTION: Demonstrates how to create a customized Luma provider instance using `createLuma`, allowing configuration of API key, base URL, and custom headers.
SOURCE: https://github.com/vercel/ai/blob/v5/content/providers/01-ai-sdk-providers/80-luma.mdx#_snippet_2

LANGUAGE: ts
CODE:
```
import { createLuma } from '@ai-sdk/luma';

const luma = createLuma({
  apiKey: 'your-api-key', // optional, defaults to LUMA_API_KEY environment variable
  baseURL: 'custom-url', // optional
  headers: {
    /* custom headers */
  }, // optional
});
```

----------------------------------------

TITLE: Generate Text with OpenAI Model
DESCRIPTION: Demonstrates how to use the `generateText` function from the `ai` package with an OpenAI model (`gpt-4o`) to generate text based on a specific prompt. It shows the necessary imports, model initialization, and how to extract and log the generated text.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/07-reference/01-ai-sdk-core/01-generate-text.mdx#_snippet_0

LANGUAGE: TypeScript
CODE:
```
import { openai } from '@ai-sdk/openai';
import { generateText } from 'ai';

const { text } = await generateText({
  model: openai('gpt-4o'),
  prompt: 'Invent a new holiday and describe its traditions.',
});

console.log(text);
```

----------------------------------------

TITLE: cosineSimilarity API Signature
DESCRIPTION: Detailed API documentation for the `cosineSimilarity` function, including its parameters and return value. This function calculates the cosine similarity between two numerical vectors.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/50-cosine-similarity.mdx#_snippet_2

LANGUAGE: APIDOC
CODE:
```
cosineSimilarity(vector1: number[], vector2: number[]):
  vector1: number[] - The first vector to compare
  vector2: number[] - The second vector to compare
Returns: A number between -1 and 1 representing the cosine similarity between the two vectors.
```

----------------------------------------

TITLE: API Reference: `response.messages` Property Type
DESCRIPTION: Describes the `response.messages` property, which contains an array of `ModelMessage` objects representing the assistant and tool messages generated during the process.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/15-tools-and-tool-calling.mdx#_snippet_10

LANGUAGE: APIDOC
CODE:
```
response.messages: ModelMessage[]
```

----------------------------------------

TITLE: Transcribe Audio with Rev.ai and Provider Options
DESCRIPTION: Shows how to use the `experimental_transcribe` function from the `ai` library to transcribe an audio file (`audio.mp3`) using a Rev.ai transcription model. This example includes passing provider-specific options, such as the input language, to enhance transcription performance.
SOURCE: https://github.com/vercel/ai/blob/v5/content/providers/01-ai-sdk-providers/160-revai.mdx#_snippet_5

LANGUAGE: ts
CODE:
```
import { experimental_transcribe as transcribe } from 'ai';
import { revai } from '@ai-sdk/revai';
import { readFile } from 'fs/promises';

const result = await transcribe({
  model: revai.transcription('machine'),
  audio: await readFile('audio.mp3'),
  providerOptions: { revai: { language: 'en' } },
});
```

----------------------------------------

TITLE: getAIState API Signature Reference
DESCRIPTION: Comprehensive API documentation for the `getAIState` function, outlining its parameters and the type of value it returns. This function is used to retrieve the current AI state.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/07-reference/03-ai-sdk-rsc/06-get-ai-state.mdx#_snippet_1

LANGUAGE: APIDOC
CODE:
```
getAIState(): any
  Parameters:
    key: string (optional)
      description: Returns the value of the specified key in the AI state, if it's an object.
  Returns: The AI state.
```

----------------------------------------

TITLE: Cohere Embedding Model Provider Options (APIDOC)
DESCRIPTION: Details the optional provider-specific configurations available for Cohere embedding models. These options control how the input text is processed by the embedding API.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/25-cohere.mdx#_snippet_9

LANGUAGE: APIDOC
CODE:
```
inputType: 'search_document' | 'search_query' | 'classification' | 'clustering'
  Specifies the type of input passed to the model. Default is `search_query`.
  - `search_document`: Used for embeddings stored in a vector database for search use-cases.
  - `search_query`: Used for embeddings of search queries run against a vector DB to find relevant documents.
  - `classification`: Used for embeddings passed through a text classifier.
  - `clustering`: Used for embeddings run through a clustering algorithm.

truncate: 'NONE' | 'START' | 'END'
  Specifies how the API will handle inputs longer than the maximum token length.
  Default is `END`.
  - `NONE`: If selected, when the input exceeds the maximum input token length will return an error.
  - `START`: Will discard the start of the input until the remaining input is exactly the maximum input token length for the model.
  - `END`: Will discard the end of the input until the remaining input is exactly the maximum input token length for the model.
```

----------------------------------------

TITLE: AI SDK Generative UI: streamUI Function Overview
DESCRIPTION: This documentation describes the `streamUI` function's role in creating generative user interfaces. It explains how `streamUI` leverages 'tools' to execute functions based on user input and renders React components directly from the function's output, extending chat interfaces beyond plain text.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/05-ai-sdk-rsc/10-migrating-to-ui.mdx#_snippet_6

LANGUAGE: APIDOC
CODE:
```
streamUI Function:
  - Purpose: Uses 'tools' as a way to execute functions based on user input.
  - Output: Renders React components based on the function output.
  - Goal: Go beyond text in the chat interface by generating UI.
```

----------------------------------------

TITLE: API Signature for streamToResponse
DESCRIPTION: Defines the parameters and their types for the `streamToResponse` function, including details for the stream, response object, optional configuration options (status and headers), and an optional `StreamData` object for additional data forwarding.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/07-reference/04-stream-helpers/05-stream-to-response.mdx#_snippet_2

LANGUAGE: APIDOC
CODE:
```
streamToResponse(stream: ReadableStream, response: ServerResponse, options?: Options, data?: StreamData)

Parameters:
  stream: ReadableStream
    The Web Stream to pipe to the response. It can be the return value of OpenAIStream, HuggingFaceStream, AnthropicStream, or an AIStream instance.
  response: ServerResponse
    The Node.js ServerResponse object to pipe the stream to. This is usually the second argument of a Node.js HTTP request handler.
  options: Options (optional)
    Configure the response.
    Properties of Options:
      status: number (optional)
        The status code to set on the response. Defaults to 200.
      headers: Record<string, string> (optional)
        Additional headers to set on the response. Defaults to { 'Content-Type': 'text/plain; charset=utf-8' }.
  data: StreamData (optional)
    StreamData object for forwarding additional data to the client.
```

----------------------------------------

TITLE: Transcribe Audio using ElevenLabs Provider in AI SDK
DESCRIPTION: An example demonstrating how to use the `elevenlabs` provider with the `experimental_transcribe` function from the AI SDK to transcribe an audio file from a URL. It shows importing necessary modules and calling the transcription function with a specified model and audio source.
SOURCE: https://github.com/vercel/ai/blob/main/packages/elevenlabs/README.md#_snippet_2

LANGUAGE: ts
CODE:
```
import { elevenlabs } from '@ai-sdk/elevenlabs';
import { experimental_transcribe as transcribe } from 'ai';

const { text } = await transcribe({
  model: elevenlabs.transcription('scribe_v1'),
  audio: new URL(
    'https://github.com/vercel/ai/raw/refs/heads/main/examples/ai-core/data/galileo.mp3',
  ),
});
```

----------------------------------------

TITLE: Vercel AI SDK Chat Hook API Definition
DESCRIPTION: Defines the structure and functionality of the Vercel AI SDK's chat hook return object, including its properties and methods for chat management.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/07-reference/02-ai-sdk-ui/01-use-chat.mdx#_snippet_8

LANGUAGE: APIDOC
CODE:
```
interface UseChatHookResult {
  error: Error | undefined;
  // An error object returned by SWR, if any.

  append: (message: Message | CreateMessage, options?: ChatRequestOptions) => Promise<string | undefined>;
  // Function to append a message to the chat, triggering an API call for the AI response.
  // It returns a promise that resolves to full response message content when the API call is successfully finished,
  // or throws an error when the API call fails.
  // Parameters:
  //   options (ChatRequestOptions):
  //     headers?: Record<string, string> | Headers;
  //       Additional headers that should be to be passed to the API endpoint.
  //     body?: object;
  //       Additional body JSON properties that should be sent to the API endpoint.
  //     data?: JSONValue;
  //       Additional data to be sent to the API endpoint.
  //     experimental_attachments?: FileList | Array<Attachment>;
  //       An array of attachments to be sent to the API endpoint.
  //       - FileList: A list of files that have been selected by the user using an <input type='file'> element.
  //                   It's also used for a list of files dropped into web content when using the drag and drop API.
  //       - Attachment: An attachment object that can be used to describe the metadata of the file.
  //         - name?: string;
  //           The name of the attachment, usually the file name.
  //         - contentType?: string;
  //           A string indicating the media type of the file.
  //         - url: string;
  //           The URL of the attachment. It can either be a URL to a hosted file or a Data URL.

  reload: (options?: ChatRequestOptions) => Promise<string | undefined>;
  // Function to reload the last AI chat response for the given chat history.
  // If the last message isn't from the assistant, it will request the API to generate a new response.
  // Parameters:
  //   options (ChatRequestOptions):
  //     headers?: Record<string, string> | Headers;
  //       Additional headers that should be to be passed to the API endpoint.
  //     body?: object;
  //       Additional body JSON properties that should be sent to the API endpoint.
  //     data?: JSONValue;
  //       Additional data to be sent to the API endpoint.

  stop: () => void;
  // Function to stop the ongoing AI response.
}
```

----------------------------------------

TITLE: Text Editor Tool Parameters (APIDOC)
DESCRIPTION: This section details the parameters available for the `textEditorTool`'s `execute` method. It lists each parameter, its type, and a brief description of its purpose and usage, including which commands require specific parameters.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/05-anthropic.mdx#_snippet_12

LANGUAGE: APIDOC
CODE:
```
command: ('view' | 'create' | 'str_replace' | 'insert' | 'undo_edit') - The command to run.
path: string - Absolute path to file or directory, e.g. /repo/file.py or /repo.
file_text: string (optional) - Required for create command, with the content of the file to be created.
insert_line: number (optional) - Required for insert command. The line number after which to insert the new string.
new_str: string (optional) - New string for str_replace or insert commands.
old_str: string (optional) - Required for str_replace command, containing the string to replace.
view_range: number[] (optional) - Optional for view command to specify line range to show.
```

----------------------------------------

TITLE: API Property: request (LanguageModelRequestMetadata)
DESCRIPTION: Accesses request metadata, providing details about the raw HTTP body sent to the provider API. This property returns a Promise that resolves to the LanguageModelRequestMetadata object.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/04-stream-object.mdx#_snippet_12

LANGUAGE: APIDOC
CODE:
```
request: Promise<LanguageModelRequestMetadata>
  Request metadata.
  LanguageModelRequestMetadata:
    body: string
      Raw request HTTP body that was sent to the provider API as a string (JSON should be stringified).
```

----------------------------------------

TITLE: OpenAI Structured Outputs API Details
DESCRIPTION: Details the default behavior of structured outputs, how to disable them, and critical limitations, including specific schema support and the necessity to use `.nullable()` instead of `.nullish()` or `.optional()` for Zod schemas.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/03-openai.mdx#_snippet_13

LANGUAGE: APIDOC
CODE:
```
OpenAI Structured Outputs:
  Default: Enabled
  Disable: Set structuredOutputs: false in providerOptions.openai
  Limitations:
    - Several limitations (refer to OpenAI documentation)
    - Supported schemas are specific
    - Example: Optional schema properties are NOT supported.
    - Zod mapping: Use .nullable() instead of .nullish() or .optional()
```

----------------------------------------

TITLE: AIStream
DESCRIPTION: Create a readable stream for AI responses.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/07-reference/04-stream-helpers/index.mdx#_snippet_0

LANGUAGE: APIDOC
CODE:
```
AIStream:
  Description: Create a readable stream for AI responses.
```

----------------------------------------

TITLE: Set Laminar Project API Key in .env
DESCRIPTION: After signing up or self-hosting Laminar and creating a project, obtain your API key from project settings and set it as an environment variable `LMNR_PROJECT_API_KEY` in your `.env` file.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/05-observability/laminar.mdx#_snippet_1

LANGUAGE: bash
CODE:
```
LMNR_PROJECT_API_KEY=...
```

----------------------------------------

TITLE: Import xAI Grok Provider Instance in TypeScript
DESCRIPTION: Demonstrates how to import the default xAI Grok provider instance into a TypeScript file for use with the AI SDK.
SOURCE: https://github.com/vercel/ai/blob/main/packages/xai/README.md#_snippet_1

LANGUAGE: ts
CODE:
```
import { xai } from '@ai-sdk/xai';
```

----------------------------------------

TITLE: Vercel AI Component Parameters API Reference
DESCRIPTION: This section details the various parameters available for configuring Vercel AI components, including language models, initial UI, system/user prompts, and a comprehensive breakdown of message types (system, user, assistant, tool) with their respective content structures like text, image, and file parts.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/07-reference/03-ai-sdk-rsc/01-stream-ui.mdx#_snippet_1

LANGUAGE: APIDOC
CODE:
```
Parameters:
	- model (LanguageModel): The language model to use. Example: openai(""gpt-4.1"")
	- initial (ReactNode, optional): The initial UI to render.
	- system (string): The system prompt to use that specifies the behavior of the model.
	- prompt (string): The input prompt to generate the text from.
	- messages (Array<CoreSystemMessage | CoreUserMessage | CoreAssistantMessage | CoreToolMessage> | Array<UIMessage>): A list of messages that represent a conversation. Automatically converts UI messages from the useChat hook.
		Properties:
			- CoreSystemMessage:
				Parameters:
					- role ('system'): The role for the system message.
					- content (string): The content of the message.
			- CoreUserMessage:
				Parameters:
					- role ('user'): The role for the user message.
					- content (string | Array<TextPart | ImagePart | FilePart>): The content of the message.
						Properties:
							- TextPart:
								Parameters:
									- type ('text'): The type of the message part.
									- text (string): The text content of the message part.
							- ImagePart:
								Parameters:
									- type ('image'): The type of the message part.
									- image (string | Uint8Array | Buffer | ArrayBuffer | URL): The image content of the message part. String are either base64 encoded content, base64 data URLs, or http(s) URLs.
									- mediaType (string, optional): The IANA media type of the image. Optional.
							- FilePart:
								Parameters:
									- type ('file'): The type of the message part.
									- data (string | Uint8Array | Buffer | ArrayBuffer | URL): The file content of the message part. String are either base64 encoded content, base64 data URLs, or http(s) URLs.
									- mediaType (string): The IANA media type of the file.
			- CoreAssistantMessage:
				Parameters:
					- role ('assistant'): The role for the assistant message.
					- content (string | Array<TextPart | ToolCallPart>): The content of the message.
						Properties:
							- TextPart:
								Parameters:
									- type ('text'): The type of the message part.
									- text (string): The text content of the message part.
```

----------------------------------------

TITLE: OpenAI Provider Metadata API Reference
DESCRIPTION: Documents the specific metadata fields returned by the OpenAI responses provider. This metadata provides insights into the response, such as its ID, cached prompt tokens, and reasoning tokens.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/03-openai.mdx#_snippet_28

LANGUAGE: APIDOC
CODE:
```
OpenAI Provider Metadata:
  responseId: string
    The ID of the response. Can be used to continue a conversation.
  cachedPromptTokens: number
    The number of prompt tokens that were a cache hit.
  reasoningTokens: number
    The number of reasoning tokens that the model generated.
```

----------------------------------------

TITLE: Transcribe Audio with Deepgram Provider Options
DESCRIPTION: Example of transcribing an audio file using the Deepgram provider, demonstrating how to pass additional provider-specific options like `summarize` to enable advanced features during transcription.
SOURCE: https://github.com/vercel/ai/blob/v5/content/providers/01-ai-sdk-providers/110-deepgram.mdx#_snippet_5

LANGUAGE: TypeScript
CODE:
```
import { experimental_transcribe as transcribe } from 'ai';
import { deepgram } from '@ai-sdk/deepgram';
import { readFile } from 'fs/promises';

const result = await transcribe({
  model: deepgram.transcription('nova-3'),
  audio: await readFile('audio.mp3'),
  providerOptions: { deepgram: { summarize: true } },
});
```

----------------------------------------

TITLE: Inflection AI Supported Models and Capabilities
DESCRIPTION: Overview of supported Inflection AI models and their capabilities, including text generation, streaming, image input, object generation, tool usage, and tool streaming.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/93-inflection-ai.mdx#_snippet_3

LANGUAGE: APIDOC
CODE:
```
Models:
  inflection_3_pi:
    description: ""the model powering our Pi experience, including a backstory, emotional intelligence, productivity, and safety. It excels in scenarios such as customer support chatbots.""
    capabilities:
      Text Generation: true
      Streaming: true
      Image Input: false
      Object Generation: false
      Tool Usage: false
      Tool Streaming: false
  inflection_3_productivity:
    description: ""the model optimized for following instructions. It is better for tasks requiring JSON output or precise adherence to provided guidelines.""
    capabilities:
      Text Generation: true
      Streaming: true
      Image Input: false
      Object Generation: false
      Tool Usage: false
      Tool Streaming: false
  inflection_3_with_tools:
    description: ""This model seems to be in preview and it lacks an official description as of the writing of this README in 1.0.0.""
    capabilities:
      Text Generation: true
      Streaming: true
      Image Input: false
      Object Generation: false
      Tool Usage: false
      Tool Streaming: false
```

----------------------------------------

TITLE: Vercel AI SDK Core API Reference
DESCRIPTION: Key functions and interfaces from the `@ai-sdk/core` package used for streaming text responses and managing message types within the Vercel AI SDK.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-getting-started/02-nextjs-app-router.mdx#_snippet_6

LANGUAGE: APIDOC
CODE:
```
Function: streamText(config: object): StreamTextResult
  Description: Streams text responses from a language model based on provided messages and model configuration.
  Parameters:
    config: object
      model: ModelProvider (e.g., openai('gpt-4o')) - The language model to use.
      messages: ModelMessage[] - An array of messages representing the conversation history.
      ...additional settings: Optional settings to customize model behavior.
  Returns: StreamTextResult - An object containing the streamed response.

Interface: StreamTextResult
  Description: The result object returned by the `streamText` function, providing methods to process the streamed output.
  Methods:
    toUIMessageStreamResponse(): Response
      Description: Converts the streamed result into a standard `Response` object suitable for streaming to a client, formatted for UI messages.
```

----------------------------------------

TITLE: Run All AI SDK End-to-End Integration Tests
DESCRIPTION: This command initiates all end-to-end provider integration tests for the AI SDK. These tests are designed for manual execution to smoke-test provider support across various common features.
SOURCE: https://github.com/vercel/ai/blob/v5/examples/ai-core/README.md#_snippet_3

LANGUAGE: sh
CODE:
```
pnpm run test:e2e:all
```

----------------------------------------

TITLE: Generate Text with AI SDK and OpenAI
DESCRIPTION: Demonstrates how to use the `generateText` function from the AI SDK with an OpenAI model (gpt-3.5-turbo) to generate a text response based on a given prompt. It imports necessary modules from 'ai' and '@ai-sdk/openai' and logs the generated result.
SOURCE: https://github.com/vercel/ai/blob/v5/content/cookbook/05-node/10-generate-text.mdx#_snippet_0

LANGUAGE: typescript
CODE:
```
import { generateText } from 'ai';
import { openai } from '@ai-sdk/openai';

const result = await generateText({
  model: openai('gpt-3.5-turbo'),
  prompt: 'Why is the sky blue?',
});

console.log(result);
```

----------------------------------------

TITLE: Stream Text using Portkey Completion Model
DESCRIPTION: An example illustrating how to use the `streamText` function from the AI SDK with a Portkey completion model to stream text responses asynchronously, processing each chunk as it arrives.
SOURCE: https://github.com/vercel/ai/blob/v5/content/providers/03-community-providers/10-portkey.mdx#_snippet_4

LANGUAGE: javascript
CODE:
```
import { createPortkey } from '@portkey-ai/vercel-provider';
import { streamText } from 'ai';

const portkey = createPortkey({
  apiKey: 'YOUR_PORTKEY_API_KEY',
  config: portkeyConfig,
});

const result = streamText({
  model: portkey.completionModel(''),
  prompt: 'Invent a new holiday and describe its traditions.',
});

for await (const chunk of result) {
  console.log(chunk);
}
```

----------------------------------------

TITLE: Handle File Inputs with Google Generative AI and AI SDK
DESCRIPTION: This TypeScript snippet demonstrates how to send file inputs, such as PDF documents, to a Google Generative AI model using the AI SDK. It shows how to read a local file and include it in the 'messages' array with its 'mediaType'.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/15-google-generative-ai.mdx#_snippet_9

LANGUAGE: ts
CODE:
```
import { google } from '@ai-sdk/google';
import { generateText } from 'ai';

const result = await generateText({
  model: google('gemini-1.5-flash'),
  messages: [
    {
      role: 'user',
      content: [
        {
          type: 'text',
          text: 'What is an embedding model according to this document?',
        },
        {
          type: 'file',
          data: fs.readFileSync('./data/ai.pdf'),
          mediaType: 'application/pdf',
        },
      ],
    },
  ],
});
```

----------------------------------------

TITLE: Create .env File for API Key
DESCRIPTION: This command creates an empty `.env` file in the project's root directory, which will be used to store sensitive environment variables like the OpenAI API key.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-getting-started/06-nodejs.mdx#_snippet_2

LANGUAGE: bash
CODE:
```
touch .env
```

----------------------------------------

TITLE: Test Local Endpoint with cURL
DESCRIPTION: Sends a POST request to the running local server endpoint at http://localhost:8080 using cURL. This command verifies that the server is active and responding to requests.
SOURCE: https://github.com/vercel/ai/blob/v5/examples/node-http-server/README.md#_snippet_3

LANGUAGE: sh
CODE:
```
curl -X POST http://localhost:8080
```

----------------------------------------

TITLE: Run SSE Transport Client (Legacy)
DESCRIPTION: Executes the client for the legacy Server-Sent Events (SSE) transport. This client interacts with the SSE server to demonstrate the legacy streaming example.
SOURCE: https://github.com/vercel/ai/blob/main/examples/mcp/README.md#_snippet_7

LANGUAGE: sh
CODE:
```
pnpm sse:client
```

----------------------------------------

TITLE: Create Custom OpenAI Provider Instance with Settings
DESCRIPTION: Demonstrates how to create a new OpenAI provider instance using `createOpenAI`, allowing for custom configurations such as headers or base URLs.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/03-openai.mdx#_snippet_2

LANGUAGE: TypeScript
CODE:
```
import { createOpenAI } from '@ai-sdk/openai';

const openai = createOpenAI({
  // custom settings, e.g.
  headers: {
    'header-name': 'header-value',
  },
});
```

----------------------------------------

TITLE: Vercel AI SDK APICallError Object Structure
DESCRIPTION: This documentation outlines the structure of the `APICallError` object, a custom error type within the Vercel AI SDK. It details properties such as the `url` of the failed API call, the `requestBodyValues` sent, the `statusCode` received, `responseHeaders`, and the underlying `cause` of the error, providing comprehensive context for debugging API failures.
SOURCE: https://github.com/vercel/ai/blob/main/examples/ai-core/data/error-message.txt#_snippet_2

LANGUAGE: APIDOC
CODE:
```
APICallError:
  cause: TypeError (e.g., 'Body is unusable')
  url: string (e.g., 'https://api.anthropic.com/v1/messages')
  requestBodyValues: object
    model: string (e.g., 'claude-3-5-sonnet-20240620')
    top_k: any
    max_tokens: number (e.g., 4096)
    temperature: number (e.g., 0)
    top_p: any
    stop_sequences: any
    system: any
    messages: array (e.g., [ [Object] ])
    tools: any
    tool_choice: any
  statusCode: number (e.g., 400)
  responseHeaders: object
    'cf-cache-status': string (e.g., 'DYNAMIC')
    'cf-ray': string (e.g., '8b39b60ab8734516-TXL')
    connection: string (e.g., 'keep-alive')
    'content-length': string (e.g., '171')
    'content-type': string (e.g., 'application/json')
    date: string (e.g., 'Thu, 15 Aug 2024 14:00:28 GMT')
    'request-id': string (e.g., 'req_01PLrS159iiihG7kS9PFQiqx')
    server: string (e.g., 'cloudflare')
    via: string (e.g., '1.1 google')
    'x-cloud-trace-context': string (e.g., '1371f8e6d358102b79d109db3829d62e')
    'x-robots-tag': string (e.g., 'none')
    'x-should-retry': string (e.g., 'false')
  responseBody: any (e.g., undefined)
  isRetryable: boolean (e.g., false)
  data: any (e.g., undefined)
  [Symbol(vercel.ai.error)]: boolean (e.g., true)
  [Symbol(vercel.ai.error.AI_APICallError)]: boolean (e.g., true)
```

----------------------------------------

TITLE: AI SDK RSC API Reference
DESCRIPTION: Overview of core functions and hooks available in the experimental AI SDK RSC for building AI-powered React applications. These functions facilitate UI streaming, state management, and server-side interactions.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/07-reference/03-ai-sdk-rsc/index.mdx#_snippet_0

LANGUAGE: APIDOC
CODE:
```
AI SDK RSC API Reference:
- streamUI: Use a helper function that streams React Server Components on tool execution.
- createAI: Create a context provider that wraps your application and shares state between the client and language model on the server.
- createStreamableUI: Create a streamable UI component that can be rendered on the server and streamed to the client.
- createStreamableValue: Create a streamable value that can be rendered on the server and streamed to the client.
- getAIState: Read the AI state on the server.
- getMutableAIState: Read and update the AI state on the server.
- useAIState: Get the AI state on the client from the context provider.
- useUIState: Get the UI state on the client from the context provider.
- useActions: Call server actions from the client.
```

----------------------------------------

TITLE: APIDOC: StepStartUIPart Type Definition
DESCRIPTION: Defines the structure for a StepStartUIPart, a simple message component indicating the initiation of a step within a process.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/07-reference/02-ai-sdk-ui/01-use-chat.mdx#_snippet_6

LANGUAGE: APIDOC
CODE:
```
StepStartUIPart Type Definition:
  type: 'StepStartUIPart'
  description: 'A step start part of the message.'
  parameters:
    - name: 'type'
      type: """"""step-start""""""
```

----------------------------------------

TITLE: Generate a Custom ID with createIdGenerator
DESCRIPTION: Demonstrates how to import and use `createIdGenerator` to create a custom ID generator function. This example configures a specific prefix and separator, then generates an example ID using the custom generator.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/07-reference/01-ai-sdk-core/91-create-id-generator.mdx#_snippet_0

LANGUAGE: ts
CODE:
```
import { createIdGenerator } from 'ai';

const generateCustomId = createIdGenerator({
  prefix: 'user',
  separator: '_'
});

const id = generateCustomId(); // Example: ""user_1a2b3c4d5e6f7g8h""
```

----------------------------------------

TITLE: Groq Transcription API Provider Options
DESCRIPTION: Details the available provider-specific options for the Groq transcription API, including parameters for timestamp granularity, language, prompt, and temperature, along with their types, descriptions, and default values.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/09-groq.mdx#_snippet_12

LANGUAGE: APIDOC
CODE:
```
timestampGranularities: string[]
  Description: The granularity of the timestamps in the transcription.
  Defaults: ['segment']
  Possible values: ['word'], ['segment'], and ['word', 'segment']
  Note: There is no additional latency for segment timestamps, but generating word timestamps incurs additional latency.

language: string
  Description: The language of the input audio. Supplying the input language in ISO-639-1 format (e.g. 'en') will improve accuracy and latency.
  Optional.

prompt: string
  Description: An optional text to guide the model's style or continue a previous audio segment. The prompt should match the audio language.
  Optional.

temperature: number
  Description: The sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. If set to 0, the model will use log probability to automatically increase the temperature until certain thresholds are hit.
  Defaults: 0
  Optional.
```

----------------------------------------

TITLE: Install AI SDK Replicate Provider
DESCRIPTION: Instructions to install the Replicate provider for the AI SDK using npm.
SOURCE: https://github.com/vercel/ai/blob/main/packages/replicate/README.md#_snippet_0

LANGUAGE: bash
CODE:
```
npm i @ai-sdk/replicate
```

----------------------------------------

TITLE: Language Model Call Result Metadata Structure
DESCRIPTION: This API documentation describes the structure of metadata associated with language model calls in the AI SDK. It defines fields for token usage (total, reasoning, cached), warnings from the model provider, and detailed request and response information, including HTTP body, headers, and messages.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/07-reference/01-ai-sdk-core/01-generate-text.mdx#_snippet_13

LANGUAGE: APIDOC
CODE:
```
LanguageModelCallResultMetadata:
  tokens: object
    totalTokens: number | undefined
      description: The total number of tokens as reported by the provider. This number might be different from the sum of inputTokens and outputTokens and e.g. include reasoning tokens or other overhead.
    reasoningTokens: number | undefined (optional)
      description: The number of reasoning tokens used.
    cachedInputTokens: number | undefined (optional)
      description: The number of cached input tokens.
  warnings: CallWarning[] | undefined
    description: Warnings from the model provider (e.g. unsupported settings).
  request: LanguageModelRequestMetadata
    description: Additional request information.
    properties:
      body: string
        description: Raw request HTTP body that was sent to the provider API as a string (JSON should be stringified).
  response: LanguageModelResponseMetadata
    description: Additional response information.
    properties:
      id: string
        description: The response identifier. The AI SDK uses the ID from the provider response when available, and generates an ID otherwise.
      modelId: string
        description: The model that was used to generate the response. The AI SDK uses the response model from the provider response when available, and the model from the function call otherwise.
      timestamp: Date
        description: The timestamp of the response. The AI SDK uses the response timestamp from the provider response when available, and creates a timestamp otherwise.
      headers: Record<string, string> (optional)
        description: Optional response headers.
      body: unknown (optional)
        description: Response body (available only for providers that use HTTP requests).
      messages: Array<ResponseMessage>
        description: The response messages that were generated during the call. Response messages can be either assistant messages or tool messages. They contain a generated id.
  providerMetadata: ProviderMetadata | undefined
    description: Additional provider-specific metadata. They are passed through from the provider to the AI SDK and enable provider-specific results that can be fully encapsulated in the provider.
```

----------------------------------------

TITLE: AnthropicStream Function API Signature
DESCRIPTION: This documentation outlines the complete API signature for the AnthropicStream function, detailing its parameters, including the response object from the provider SDK and an optional callbacks object, along with its return type.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/07-reference/04-stream-helpers/08-anthropic-stream.mdx#_snippet_1

LANGUAGE: APIDOC
CODE:
```
AnthropicStream Function Signature:
  Parameters:
    response: Response
      Description: The response object returned by a call made by the Provider SDK.
    callbacks?: AIStreamCallbacksAndOptions
      Description: An object containing callback functions to handle the start, each token, and completion of the AI response. In the absence of this parameter, default behavior is implemented.
      AIStreamCallbacksAndOptions Properties:
        onStart: () => Promise<void>
          Description: An optional function that is called at the start of the stream processing.
        onCompletion: (completion: string) => Promise<void>
          Description: An optional function that is called for every completion. It's passed the completion as a string.
        onFinal: (completion: string) => Promise<void>
          Description: An optional function that is called once when the stream is closed with the final completion message.
        onToken: (token: string) => Promise<void>
          Description: An optional function that is called for each token in the stream. It's passed the token as a string.
  Returns:
    ReadableStream
```

----------------------------------------

TITLE: Stream Text with OpenRouter and AI SDK
DESCRIPTION: Illustrates how to use the `streamText` function from the AI SDK with an OpenRouter chat model to receive a response in a streaming fashion. The example includes provider setup and iterating over the streamed chunks.
SOURCE: https://github.com/vercel/ai/blob/v5/content/providers/03-community-providers/13-openrouter.mdx#_snippet_4

LANGUAGE: javascript
CODE:
```
import { createOpenRouter } from '@openrouter/ai-sdk-provider';
import { streamText } from 'ai';

const openrouter = createOpenRouter({
  apiKey: 'YOUR_OPENROUTER_API_KEY',
});

const result = streamText({
  model: openrouter.chat('meta-llama/llama-3.1-405b-instruct'),
  prompt: 'Write a short story about AI.',
});

for await (const chunk of result) {
  console.log(chunk);
}
```

----------------------------------------

TITLE: Generate Text with OpenAI Reasoning Model
DESCRIPTION: Demonstrates how to use the `generateText` function with an OpenAI reasoning model (e.g., 'o3-mini'). It shows how to configure the `reasoningEffort` via `providerOptions` and access `reasoningTokens` from the `providerMetadata` for usage tracking.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/03-openai.mdx#_snippet_10

LANGUAGE: ts
CODE:
```
import { openai } from '@ai-sdk/openai';
import { generateText } from 'ai';

const { text, usage, providerMetadata } = await generateText({
  model: openai('o3-mini'),
  prompt: 'Invent a new holiday and describe its traditions.',
  providerOptions: {
    openai: {
      reasoningEffort: 'low',
    },
  },
});

console.log(text);
console.log('Usage:', {
  ...usage,
  reasoningTokens: providerMetadata?.openai?.reasoningTokens,
});
```

----------------------------------------

TITLE: Integrate Weather Tool with AI Stream
DESCRIPTION: This TypeScript code demonstrates how to integrate a custom weather tool into an AI application using @ai-sdk/openai. It defines the tool's description, input schema using Zod, and an asynchronous execute function to simulate fetching weather data, allowing the AI model to use it during text streaming.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-getting-started/06-nodejs.mdx#_snippet_6

LANGUAGE: TypeScript
CODE:
```
import { openai } from '@ai-sdk/openai';
import { ModelMessage, streamText, tool } from 'ai';
import 'dotenv/config';
import { z } from 'zod';
import * as readline from 'node:readline/promises';

const terminal = readline.createInterface({
  input: process.stdin,
  output: process.stdout,
});

const messages: ModelMessage[] = [];

async function main() {
  while (true) {
    const userInput = await terminal.question('You: ');

    messages.push({ role: 'user', content: userInput });

    const result = streamText({
      model: openai('gpt-4o'),
      messages,
      tools: {
        weather: tool({
          description: 'Get the weather in a location (fahrenheit)',
          inputSchema: z.object({
            location: z
              .string()
              .describe('The location to get the weather for'),
          }),
          execute: async ({ location }) => {
            const temperature = Math.round(Math.random() * (90 - 32) + 32);
            return {
              location,
              temperature,
            };
          },
        }),
      },
    });

    let fullResponse = '';
    process.stdout.write('\nAssistant: ');
    for await (const delta of result.textStream) {
      fullResponse += delta;
      process.stdout.write(delta);
    }
    process.stdout.write('\n\n');

    messages.push({ role: 'assistant', content: fullResponse });
  }
}

main().catch(console.error);
```

----------------------------------------

TITLE: Create Next.js API Route File Structure
DESCRIPTION: This bash command creates the necessary directory structure (`app/api/chat`) and an empty `route.ts` file within it. This prepares the environment for defining a Next.js API route handler.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/00-guides/01-rag-chatbot.mdx#_snippet_19

LANGUAGE: bash
CODE:
```
mkdir -p app/api/chat && touch app/api/chat/route.ts
```

----------------------------------------

TITLE: Create Custom ElevenLabs Provider Instance
DESCRIPTION: Create a customized ElevenLabs provider instance using `createElevenLabs` to apply specific settings such as a custom fetch implementation or an explicit API key.
SOURCE: https://github.com/vercel/ai/blob/v5/content/providers/01-ai-sdk-providers/90-elevenlabs.mdx#_snippet_2

LANGUAGE: TypeScript
CODE:
```
import { createElevenLabs } from '@ai-sdk/elevenlabs';

const elevenlabs = createElevenLabs({
  // custom settings, e.g.
  fetch: customFetch,
});
```

----------------------------------------

TITLE: ReplicateStream
DESCRIPTION: Transforms the response from Replicate's language models into a readable stream.
SOURCE: https://github.com/vercel/ai/blob/v5/content/docs/07-reference/04-stream-helpers/index.mdx#_snippet_14

LANGUAGE: APIDOC
CODE:
```
ReplicateStream:
  Description: Transforms the response from Replicate's language models into a readable stream.
```

----------------------------------------

TITLE: AI SDK useChat Hook API Reference
DESCRIPTION: Details the `useChat` hook from `@ai-sdk/react`, explaining its purpose, state management, and default API endpoint. It enables streaming chat messages and automatic UI updates.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/00-guides/01-rag-chatbot.mdx#_snippet_17

LANGUAGE: APIDOC
CODE:
```
useChat(options: UseChatOptions): { messages: Message[], input: string, handleInputChange: (e: ChangeEvent<HTMLInputElement>) => void, handleSubmit: (e: FormEvent<HTMLFormElement>) => void }
  Description: Hook to easily create a conversational user interface. Enables streaming of chat messages, manages state for chat input, and updates UI automatically.
  Parameters:
    options: UseChatOptions - Configuration options for the chat hook.
      chatStore: ChatStoreOptions - Configuration for chat storage.
        api: string (default: '/api/chat') - The API endpoint to send chat messages to.
  Returns:
    messages: Message[] - Array of chat messages.
    input: string - Current value of the chat input field.
    handleInputChange: (e: ChangeEvent<HTMLInputElement>) => void - Event handler for input changes.
    handleSubmit: (e: FormEvent<HTMLFormElement>) => void - Event handler for form submission.
```

----------------------------------------

TITLE: Guide AI Behavior with System Messages (TypeScript)
DESCRIPTION: Illustrates how to use system messages to guide the behavior of an AI assistant. System messages are sent to the model before user messages to set the context or persona, influencing its responses.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-foundations/03-prompts.mdx#_snippet_19

LANGUAGE: TypeScript
CODE:
```
const result = await generateText({
  model: 'openai/gpt-4.1',
  messages: [
    { role: 'system', content: 'You help planning travel itineraries.' },
    {
      role: 'user',
      content:
        'I am planning a trip to Berlin for 3 days. Please suggest the best tourist activities for me to do.',
    }
  ]
});
```

----------------------------------------

TITLE: Configure OpenAI API Key
DESCRIPTION: Adds the OpenAI API key to the `.env.local` file. This key is crucial for authenticating your application with the OpenAI service, allowing it to make requests to OpenAI models and utilize their capabilities.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-getting-started/02-nextjs-app-router.mdx#_snippet_4

LANGUAGE: env
CODE:
```
OPENAI_API_KEY=xxxxxxxxx
```","vercel ai",""